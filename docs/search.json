[
  {
    "objectID": "eda/topics.html",
    "href": "eda/topics.html",
    "title": "Topics",
    "section": "",
    "text": "Topics\nThe questions we will address in this work are:\nIdea 1\n\nBusiness goal: Given a textual post, classify the subreddit to which it belongs.\nTechnical approach: Use NLP techniques to construct a feature set from the textual components of a post. These features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features. Use the newly created features to build a multi-class classification model capable of classifying the subreddit to which a post belongs.\n\nIdea 2\n\nBusiness goal: Given a textual post, predict the age and gender of the author of the post.\nTechnical approach: Use NLP techniques to extract the age and gender of the author of a post, if available (as an example: “My brother (24M) and I (23M) went to the store…”). Use NLP techniques to construct a feature set from the textual components of a post. Again, these features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features. Use the newly created features to build two models: a regression model for predicting the age of the author of the post, and a classification model for classifying the gender of the author of the post.\n\nIdea 3\n\nBusiness goal: Use a pre-trained model to build features surrounding NLP for our ML tasks.\nTechnical approach: Identify available pre-trained models in the realm of NLP and what they are trained to extract from textual data. Use a pre-trained model, or models, to perform NLP tasks for downstream tasks. For instance, use a pre-trained model to obtain word embeddings (vector representations of words) that can be used as features in a subsequent ML model.\n\nIdea 4\n\nBusiness goal: Predict the popularity of text-based submissions based on their text content.\nTechnical approach: Using NLP techniques on the text posts of various similarly sized subreddits (by number of subscribers) that typically contain posts with moderate to high word counts, extract the most pertinent features. Apply supervised machine learning models (e.g., regression) to predict the number of upvotes and/or comments a given text post will receive. Identify the most influential/important words and phrases that predict a post’s popularity and calculate accuracy metrics based on actual post popularity.\n\nIdea 5\n\nBusiness goal: Determine/predict “flairs” of Reddit posts in r/AITA based on their text content.\nTechnical approach: Focusing primarily on the subreddit r/AITA and its flairs (Asshole, Not the A-hole, Everyone Sucks, No A-holes here), apply NLP techniques such as tokenization on text-based submissions to extract the important contents of each post. Apply a multi-class classification model trained on labelled r/AITA posts (i.e., text posts with a flair) to predict which flair a given post will receive based on its text content. Present confusion matrices on testing dataset and identify the words/phrases most commonly associated with each flair type.\n\nIdea 6\n\nBusiness goal: Determine which subreddit posts belong to based on their sentiment.\nTechnical approach: Use NLP techniques to clean content of various text based subreddits. Apply sentiment modeling to these posts. Train a classification model on these posts. Assess accuracy of the model using unlabelled posts (i.e., posts where the subreddit is not identified). Present findings in confusion matrices and calculate various accuracy metrics.\n\nIdea 7\n\nBusiness goal: Evaluate the relationship between the number of comments and the score of Reddit posts to establish an ‘interaction_score’ as an aggregate engagement metric.\nTechnical approach: We plan to analyze the relationship between the number of comments and post scores on social media platforms. This involves collecting data grouped by subreddits or similar categories and using pyspark.sql.functions to calculate the correlation between these two metrics. We will develop an interaction_score metric based on our findings, averaging the number of comments and post scores. This new metric aims to provide a unified measure of user engagement across various posts and platforms.\n\nIdea 8\n\nBusiness goal: Assess the impact of not-safe-for-work (NSFW) content on user engagement.\nTechnical approach: To analyze the influence of NSFW content on user engagement, we will utilize a dataset of submissions, focusing on those marked with the over_18 flag. Considering the limited proportion of such posts, we’ll create a balanced dataset by randomly selecting an equivalent number of submissions without the NSFW tag. This approach ensures a fair comparison between NSFW and non-NSFW content. We will employ the interaction_score from Topic 1 as a primary measure of user engagement. Our methodology includes generating a boxplot to visualize the distribution of interaction scores for both NSFW and non-NSFW posts.\n\nIdea 9\n\nBusiness goal: Identify the times of the day when posts typically receive the most engagement.\nTechnical approach: Implement a data analysis process that focuses on understanding the temporal patterns of user engagement on social media posts. Utilize the created_utc column from the dataset to create two new variables: week_of_the_year and hour_of_the_day. Exclude the first two days of 2022 to maintain accurate weekly categorization, as these days are part of week 53 of 2021. Aggregate and analyze the data based on these new variables to reveal patterns in user engagement across different times of the day and weeks of the year. The outcome will be visualized through a comprehensive plot, illustrating the times when posts receive the most engagement, thus guiding content strategies for optimal post timing.\n\nIdea 10\n\nBusiness goal:\nTechnical approach:\n\nIdea 11\n\nBusiness goal:\nTechnical approach:\n\nIdea 12\n\nBusiness goal:\nTechnical approach:"
  },
  {
    "objectID": "eda/exploratory-analysis.html",
    "href": "eda/exploratory-analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "We selected a subset of the data related to subreddits dedicated to storytelling during 2022. Namely, we chose the 12 subreddits AITA, AskMen, AskWomen, TrueOffMyChest, unpopularopinion, tifu, socialskills, antiwork, relationship_advice, explainitlikeim5, OutOfTheLoop, and NoStupidQuestions. The data we acquired has a shape of 3,444,283 x 68 for the submissions table and 76,503,363 x 21 for the comments table."
  },
  {
    "objectID": "eda/exploratory-analysis.html#the-data-subset",
    "href": "eda/exploratory-analysis.html#the-data-subset",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "We selected a subset of the data related to subreddits dedicated to storytelling during 2022. Namely, we chose the 12 subreddits AITA, AskMen, AskWomen, TrueOffMyChest, unpopularopinion, tifu, socialskills, antiwork, relationship_advice, explainitlikeim5, OutOfTheLoop, and NoStupidQuestions. The data we acquired has a shape of 3,444,283 x 68 for the submissions table and 76,503,363 x 21 for the comments table."
  },
  {
    "objectID": "eda/exploratory-analysis.html#cleaning",
    "href": "eda/exploratory-analysis.html#cleaning",
    "title": "Exploratory Analysis",
    "section": "Cleaning",
    "text": "Cleaning\nOur cleaning process includes removing from the comments database both datasets the columns that are not needed for each particular set of analyses. Also, in the comments table, we remove the rows where the body has been either removed or deleted. In the submissions table, we also remove where the selftext has been removed or deleted. The resulting rows are 977,181 for the submissions table and 70,594,314 for the comments table."
  },
  {
    "objectID": "eda/exploratory-analysis.html#dummies",
    "href": "eda/exploratory-analysis.html#dummies",
    "title": "Exploratory Analysis",
    "section": "Dummies",
    "text": "Dummies\nTo gauge the overall engagement in the posts, we used regex to create dummy variables that indicate whether the words ‘fascinating,’ ‘entertaining,’ and ‘boring’ appear on a post. We then aggregated them, with the results shown in the following table:\n\n\n\n\n\ncount\nfascinating\nentertaining\nboring\n\n\n\n\n1\n51,424\n30,870\n99,068\n\n\n0\n76,451,939\n76,472,493\n76,404,295\n\n\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda/exploratory-analysis.html#the-relationship-between-the-number-of-comments-and-the-score-of-reddit-posts",
    "href": "eda/exploratory-analysis.html#the-relationship-between-the-number-of-comments-and-the-score-of-reddit-posts",
    "title": "Exploratory Analysis",
    "section": "The relationship between the number of comments and the score of Reddit posts",
    "text": "The relationship between the number of comments and the score of Reddit posts\nThe number of comments (num_comments) and the score of a post (score), which is the upvotes minus the downvotes the post has received, are ways to gauge engagement with the post. Determining whether these variables are correlated can justify their combination into an aggregate engagement metric. To do this, we group the submissions by subreddit and leverage the corr function from pyspark.sql.functions. The table below shows the results:\n\n\n\n\n\nSubreddit\nCorrelation Coefficient\n\n\n\n\nexplainlikeimfive\n0.88\n\n\nOutOfTheLoop\n0.86\n\n\nAskMen\n0.85\n\n\nunpopularopinion\n0.83\n\n\nantiwork\n0.82\n\n\ntifu\n0.82\n\n\nAmItheAsshole\n0.81\n\n\nTrueOffMyChest\n0.79\n\n\nNoStupidQuestions\n0.78\n\n\nAskWomen\n0.77\n\n\nsocialskills\n0.72\n\n\nrelationship_advice\n0.64\n\n\n\n\n\nWe can also visualize them separately as in the figure below:\n\nWe can see in both the table and figure that they are significantly correlated, which leads to the creation of the interaction_score additional variable. This metric is an equal-weighted average of the number of comments and the score in each post.\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda/exploratory-analysis.html#the-impact-of-not-safe-for-work-nsfw-content-on-user-engagement.",
    "href": "eda/exploratory-analysis.html#the-impact-of-not-safe-for-work-nsfw-content-on-user-engagement.",
    "title": "Exploratory Analysis",
    "section": "The impact of not-safe-for-work (NSFW) content on user engagement.",
    "text": "The impact of not-safe-for-work (NSFW) content on user engagement.\nTo determine if not safe for work post affects user interactions, first, we filter the submissions dataset for where the over_18 flag is true (which in this tiny percentage of them). Then, we randomly sample an equal amount of false cases, and with this, we create a small, balanced dataset with the same amount of posts flagged as NSFW as those that are not.\nWe can create a boxplot with this small dataset to see the distribution. Since we know from Topic 1 that the interaction_score is a good gauge of overall interaction, we can plot that variable as shown below:\n\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda/exploratory-analysis.html#the-times-of-the-day-when-posts-typically-receive-the-most-engagement.",
    "href": "eda/exploratory-analysis.html#the-times-of-the-day-when-posts-typically-receive-the-most-engagement.",
    "title": "Exploratory Analysis",
    "section": "The times of the day when posts typically receive the most engagement.",
    "text": "The times of the day when posts typically receive the most engagement.\nTo determine the times of day when a post typically receives the most engagement, we create two additional variables: ’ week_of_the_yearandhour_of_the_day, both coming from thecreated_utc` column. We remove the first two days of 2022 as these would be considered part of week 53 of 2021. Then, we can group and pivot the count of our new variables, resulting in the plot below:\n\nThis analysis clearly shows from roughly 6:00 AM to 11:00 AM UTC (or 1:00 AM to 6:00 AM Eastern time) is low on activity in the story time subreddits.\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "NLP"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this project, we dive into the intersection of story telling in Reddit and machine learning (ML). Our exploration spans various aspects from classifying posts into specific subreddits to predicting the age and gender of posters based on their language. We’re also examining the influence of NSFW posts on user engagement and the correlation between post score and its comments. The project will utilize advanced Natural Language Processing (NLP) techniques to unearth trends within communities, predict post flairs, and delve into the sentiment analysis of subreddit posts. We also aim to use ML to distill meaningful insights from vast quantities of data.\n\n\n\nImage generated with OpenAI DALL·E 3 with prompt “a group of four friends around a campfire sharing stories in modern digital illustration style”"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ML",
    "section": "",
    "text": "ML"
  },
  {
    "objectID": "secondary/authors.html",
    "href": "secondary/authors.html",
    "title": "Authors",
    "section": "",
    "text": "Authors\n\n\n\nAlex Pattarini\n Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam commodo metus vehicula nisi elementum rhoncus. Aliquam arcu augue, imperdiet nec lorem vitae, ultrices fermentum nibh. Aenean ut tincidunt ligula. Aenean felis quam, auctor vel varius dapibus, lobortis vitae dolor. Praesent vel facilisis elit. Donec non purus sit amet odio pharetra tincidunt id non nulla. amp419@georgetown.edu\n\n\nLandon Carpenter\n Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam commodo metus vehicula nisi elementum rhoncus. Aliquam arcu augue, imperdiet nec lorem vitae, ultrices fermentum nibh. Aenean ut tincidunt ligula. Aenean felis quam, auctor vel varius dapibus, lobortis vitae dolor. Praesent vel facilisis elit. Donec non purus sit amet odio pharetra tincidunt id non nulla. lc1276@georgetown.edu\n\n\n\n\n\n\nMatt Moriarty\n Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam commodo metus vehicula nisi elementum rhoncus. Aliquam arcu augue, imperdiet nec lorem vitae, ultrices fermentum nibh. Aenean ut tincidunt ligula. Aenean felis quam, auctor vel varius dapibus, lobortis vitae dolor. Praesent vel facilisis elit. Donec non purus sit amet odio pharetra tincidunt id non nulla. mdm341@georgetown.edu\n\n\nVictor De Lima\n I am a currently a second year student at the MS in Data Science and Analytics program. I am very interested in how machine learning models work. I want to play a part in how these models can be made better and smarter. I am hoping to lay a strong foundation for this during my time at the DSAN program. Some of my other interests are science in general, physics, technology, traveling, and history. I also love programming. vad49@georgetown.edu"
  },
  {
    "objectID": "secondary/references.html",
    "href": "secondary/references.html",
    "title": "It's Storytime!\n",
    "section": "",
    "text": "References\n\n\nBaumgartner, Jason, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. “The Pushshift Reddit Dataset.” arXiv. https://doi.org/10.48550/arXiv.2001.08435.\n\n\nThe Reddit Archives. 2016. “JSON.” GitHub. https://github.com/reddit-archive/reddit/wiki/JSON."
  },
  {
    "objectID": "secondary/code-and-data.html",
    "href": "secondary/code-and-data.html",
    "title": "Code and Data",
    "section": "",
    "text": "All the code used in this project is available on Github.\n\n\n\nThis project uses the Baumgartner et al. (2020) dataset."
  },
  {
    "objectID": "secondary/code-and-data.html#code",
    "href": "secondary/code-and-data.html#code",
    "title": "Code and Data",
    "section": "",
    "text": "All the code used in this project is available on Github."
  },
  {
    "objectID": "secondary/code-and-data.html#data",
    "href": "secondary/code-and-data.html#data",
    "title": "Code and Data",
    "section": "",
    "text": "This project uses the Baumgartner et al. (2020) dataset."
  }
]