[
  {
    "objectID": "secondary/code-and-data.html",
    "href": "secondary/code-and-data.html",
    "title": "Code and Data",
    "section": "",
    "text": "All the code used in this project is available on Github. For a description of the code folders, click here.\n\n\n\nThis project uses the [1] dataset. The features available are listed below. While not all formal definitions are made publicly available by the dataset authors, those shown below were sourced from [1] and [2].\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nadserver_click_url\n\n\n\n\nadserver_imp_pixel\n\n\n\n\narchived\n\n\n\n\nauthor\nString\nThe account name of the poster, e.g., “example username”\n\n\nauthor_cakeday\n\n\n\n\nauthor_flair_css_class\nString\nThe CSS class of the author’s flair. This field is specific to subreddit\n\n\nauthor_flair_text\nString\nThe text of the author’s flair. This field is specific to subreddit\n\n\nauthor_id\n\n\n\n\nbrand_safe\n\n\n\n\ncontest_mode\n\n\n\n\ncreated_utc\nInteger\nUNIX timestamp referring to the time of the submission’s creation, e.g., 1483228803\n\n\ncrosspost_parent\n\n\n\n\ncrosspost_parent_list\n\n\n\n\ndisable_comments\n\n\n\n\ndistinguished\nString\nFlag to determine whether the submission is distinguished2 by moderators. “null” means not distinguished\n\n\ndomain\nString\nThe domain of the submission, e.g., self.AskReddit\n\n\ndomain_override\n\n\n\n\nedited\nLong\nIndicates whether the submission has been edited. Either a number indicating the UNIX timestamp that the submission was edited at, “false” otherwise.\n\n\nembed_type\n\n\n\n\nembed_url\n\n\n\n\ngilded\nInteger\nThe number of times this submission received Reddit gold, e.g., 0\n\n\nhidden\nBoolean\ntrue if the post is hidden by the logged in user. false if not logged in or not hidden.\n\n\nhide_score\nBoolean\nFlag indicating if the submission’s score is hidden, e.g., false\n\n\nhref_url\n\n\n\n\nid\nString\nThe submission’s identifier, e.g., “5lcgjh”\n\n\nimp_pixel\n\n\n\n\nis_crosspostable\n\n\n\n\nis_reddit_media_domain\n\n\n\n\nis_self\nBoolean\nFlag that indicates whether the submission is a self post, e.g., true\n\n\nis_video\n\n\n\n\nlink_flair_css_class\nString\nthe CSS class of the link’s flair.\n\n\nlink_flair_text\nString\nthe text of the link’s flair.\n\n\nlocked\nBoolean\nFlag indicating whether the submission is currently closed to new comments, e.g., false\n\n\nmedia\nObject\nUsed for streaming video. Detailed information about the video and it’s origins are placed here\n\n\nmedia_embed\nObject\nUsed for streaming video. Technical embed specific information is found here.\n\n\nmobile_ad_url\n\n\n\n\nnum_comments\nInteger\nThe number of comments associated with this submission, e.g., 7\n\n\nnum_crossposts\n\n\n\n\noriginal_link\n\n\n\n\nover_18\nBoolean\nFlag that indicates whether the submission is Not-Safe-For-Work, e.g., false\n\n\nparent_whitelist_status\n\n\n\n\npermalink\nString\nRelative URL of the permanent link that points to this specific submission, e.g., “/r/AskReddit/comments/5lcgj9/what did you think of the ending of rogue one/”\n\n\npinned\n\n\n\n\npost_hint\n\n\n\n\npreview\n\n\n\n\npromoted\n\n\n\n\npromoted_by\n\n\n\n\npromoted_display_name\n\n\n\n\npromoted_url\n\n\n\n\nretrieved_on\nInteger\nUNIX timestamp referring to the time we crawled the submission, e.g., 1483228803\n\n\nscore\nInteger\nThe score that the submission has accumulated. The score is the number of upvotes minus the number of downvotes. E.g., 5 . NB: Reddit fuzzes the real score to prevent spam bots.\n\n\nsecure_media\n\n\n\n\nsecure_media_embed\n\n\n\n\nselftext\nString\nThe text that is associated with the submission\n\n\nspoiler\n\n\n\n\nstickied\nBoolean\nFlag indicating whether the submission is set as sticky in the subreddit, e.g., false\n\n\nsubreddit\nString\nName of the subreddit that the submission is posted. Note that it excludes the prefix /r/. E.g., ’AskReddit’\n\n\nsubreddit_id\nString\nThe identifier of the subreddit, e.g., “t5 2qh1i”\n\n\nsuggested_sort\n\n\n\n\nthird_party_trackers\n\n\n\n\nthird_party_tracking\n\n\n\n\nthird_party_tracking_2\n\n\n\n\nthumbnail\nString\nfull URL to the thumbnail for this link; “self” if this is a self post; “image” if this is a link to an image but has no thumbnail; “default” if a thumbnail is not available\n\n\nthumbnail_height\n\n\n\n\nthumbnail_width\n\n\n\n\ntitle\nString\nThe title that is associated with the submission, e.g., “What did you think of the ending of Rogue One?”\n\n\nurl\nString\nThe URL that the submission is posting. This is the same with the permalink in cases where the submission is a self post. E.g., “https://www.reddit.com/r/AskReddit/\n\n\nwhitelist_status\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nField\nType\nDescription\n\n\n\n\nauthor\nString\nThe account name of the poster, e.g., “example username”\n\n\nauthor_cakeday\n\n\n\n\nauthor_flair_css_class\nString\nThe CSS class of the author’s flair. This field is specific to subreddit\n\n\nauthor_flair_text\nString\nThe text of the author’s flair. This field is specific to subreddit\n\n\nbody\nString\nThe comment’s text, e.g., “This is an example comment”\n\n\ncan_gild\n\n\n\n\ncontroversiality\nInteger\nNumber that indicates whether the comment is controversial, e.g., 0\n\n\ncreated_utc\nInteger\nUNIX timestamp referring to the time of the submission’s creation, e.g., 1483228803\n\n\ndistinguished\nString\nFlag to determine whether the comment is distinguished by the moderators. “null” meansnot distinguished\n\n\nedited\nLong\nFlag indicating if the comment has been edited. Either the UNIX timestamp that the commentwas edited at, or “false”.\n\n\ngilded\nInteger\nThe number of times this comment received Reddit gold, e.g., 0\n\n\nid\nString\nThe comment’s identifier, e.g., “dbumnq8”\n\n\nis_submitter\n\n\n\n\nlink_id\nString\nIdentifier of the submission that this comment is in, e.g., “t3 5l954r”\n\n\nparent_id\nString\nIdentifier of the parent of this comment, might be the identifier of the submission if it is top-levelcomment or the identifier of another comment, e.g., “t1 dbu5bpp”\n\n\npermalink\nString\nRelative URL of the permanent link that points to this specific submission,e.g., “/r/AskReddit/comments/5lcgj9/what did you think of the ending of rogue one/”\n\n\nretrieved_on\nInteger\nUNIX timestamp that refers to the time that we crawled the comment, e.g., 1483228803\n\n\nscore\nInteger\nThe score of the comment. The score is the number of upvotes minus the number ofdownvotes. Note that Reddit fuzzes the real score to prevent spam bots. E.g., 5\n\n\nstickied\nBoolean\nFlag indicating whether the submission is set as sticky in the subreddit, e.g., false\n\n\nsubreddit\nString\nName of the subreddit that the comment is posted. Note that it excludes the prefix /r/. E.g., ’AskReddit’\n\n\nsubreddit_id\nString\nThe identifier of the subreddit where the comment is posted, e.g., “t5 2qh1i”"
  },
  {
    "objectID": "secondary/code-and-data.html#code",
    "href": "secondary/code-and-data.html#code",
    "title": "Code and Data",
    "section": "",
    "text": "All the code used in this project is available on Github. For a description of the code folders, click here."
  },
  {
    "objectID": "secondary/code-and-data.html#data",
    "href": "secondary/code-and-data.html#data",
    "title": "Code and Data",
    "section": "",
    "text": "This project uses the [1] dataset. The features available are listed below. While not all formal definitions are made publicly available by the dataset authors, those shown below were sourced from [1] and [2]."
  },
  {
    "objectID": "secondary/code-and-data.html#submissions-data-card",
    "href": "secondary/code-and-data.html#submissions-data-card",
    "title": "Code and Data",
    "section": "",
    "text": "Field\nType\nDescription\n\n\n\n\nadserver_click_url\n\n\n\n\nadserver_imp_pixel\n\n\n\n\narchived\n\n\n\n\nauthor\nString\nThe account name of the poster, e.g., “example username”\n\n\nauthor_cakeday\n\n\n\n\nauthor_flair_css_class\nString\nThe CSS class of the author’s flair. This field is specific to subreddit\n\n\nauthor_flair_text\nString\nThe text of the author’s flair. This field is specific to subreddit\n\n\nauthor_id\n\n\n\n\nbrand_safe\n\n\n\n\ncontest_mode\n\n\n\n\ncreated_utc\nInteger\nUNIX timestamp referring to the time of the submission’s creation, e.g., 1483228803\n\n\ncrosspost_parent\n\n\n\n\ncrosspost_parent_list\n\n\n\n\ndisable_comments\n\n\n\n\ndistinguished\nString\nFlag to determine whether the submission is distinguished2 by moderators. “null” means not distinguished\n\n\ndomain\nString\nThe domain of the submission, e.g., self.AskReddit\n\n\ndomain_override\n\n\n\n\nedited\nLong\nIndicates whether the submission has been edited. Either a number indicating the UNIX timestamp that the submission was edited at, “false” otherwise.\n\n\nembed_type\n\n\n\n\nembed_url\n\n\n\n\ngilded\nInteger\nThe number of times this submission received Reddit gold, e.g., 0\n\n\nhidden\nBoolean\ntrue if the post is hidden by the logged in user. false if not logged in or not hidden.\n\n\nhide_score\nBoolean\nFlag indicating if the submission’s score is hidden, e.g., false\n\n\nhref_url\n\n\n\n\nid\nString\nThe submission’s identifier, e.g., “5lcgjh”\n\n\nimp_pixel\n\n\n\n\nis_crosspostable\n\n\n\n\nis_reddit_media_domain\n\n\n\n\nis_self\nBoolean\nFlag that indicates whether the submission is a self post, e.g., true\n\n\nis_video\n\n\n\n\nlink_flair_css_class\nString\nthe CSS class of the link’s flair.\n\n\nlink_flair_text\nString\nthe text of the link’s flair.\n\n\nlocked\nBoolean\nFlag indicating whether the submission is currently closed to new comments, e.g., false\n\n\nmedia\nObject\nUsed for streaming video. Detailed information about the video and it’s origins are placed here\n\n\nmedia_embed\nObject\nUsed for streaming video. Technical embed specific information is found here.\n\n\nmobile_ad_url\n\n\n\n\nnum_comments\nInteger\nThe number of comments associated with this submission, e.g., 7\n\n\nnum_crossposts\n\n\n\n\noriginal_link\n\n\n\n\nover_18\nBoolean\nFlag that indicates whether the submission is Not-Safe-For-Work, e.g., false\n\n\nparent_whitelist_status\n\n\n\n\npermalink\nString\nRelative URL of the permanent link that points to this specific submission, e.g., “/r/AskReddit/comments/5lcgj9/what did you think of the ending of rogue one/”\n\n\npinned\n\n\n\n\npost_hint\n\n\n\n\npreview\n\n\n\n\npromoted\n\n\n\n\npromoted_by\n\n\n\n\npromoted_display_name\n\n\n\n\npromoted_url\n\n\n\n\nretrieved_on\nInteger\nUNIX timestamp referring to the time we crawled the submission, e.g., 1483228803\n\n\nscore\nInteger\nThe score that the submission has accumulated. The score is the number of upvotes minus the number of downvotes. E.g., 5 . NB: Reddit fuzzes the real score to prevent spam bots.\n\n\nsecure_media\n\n\n\n\nsecure_media_embed\n\n\n\n\nselftext\nString\nThe text that is associated with the submission\n\n\nspoiler\n\n\n\n\nstickied\nBoolean\nFlag indicating whether the submission is set as sticky in the subreddit, e.g., false\n\n\nsubreddit\nString\nName of the subreddit that the submission is posted. Note that it excludes the prefix /r/. E.g., ’AskReddit’\n\n\nsubreddit_id\nString\nThe identifier of the subreddit, e.g., “t5 2qh1i”\n\n\nsuggested_sort\n\n\n\n\nthird_party_trackers\n\n\n\n\nthird_party_tracking\n\n\n\n\nthird_party_tracking_2\n\n\n\n\nthumbnail\nString\nfull URL to the thumbnail for this link; “self” if this is a self post; “image” if this is a link to an image but has no thumbnail; “default” if a thumbnail is not available\n\n\nthumbnail_height\n\n\n\n\nthumbnail_width\n\n\n\n\ntitle\nString\nThe title that is associated with the submission, e.g., “What did you think of the ending of Rogue One?”\n\n\nurl\nString\nThe URL that the submission is posting. This is the same with the permalink in cases where the submission is a self post. E.g., “https://www.reddit.com/r/AskReddit/\n\n\nwhitelist_status"
  },
  {
    "objectID": "secondary/code-and-data.html#comments-data-card",
    "href": "secondary/code-and-data.html#comments-data-card",
    "title": "Code and Data",
    "section": "",
    "text": "Field\nType\nDescription\n\n\n\n\nauthor\nString\nThe account name of the poster, e.g., “example username”\n\n\nauthor_cakeday\n\n\n\n\nauthor_flair_css_class\nString\nThe CSS class of the author’s flair. This field is specific to subreddit\n\n\nauthor_flair_text\nString\nThe text of the author’s flair. This field is specific to subreddit\n\n\nbody\nString\nThe comment’s text, e.g., “This is an example comment”\n\n\ncan_gild\n\n\n\n\ncontroversiality\nInteger\nNumber that indicates whether the comment is controversial, e.g., 0\n\n\ncreated_utc\nInteger\nUNIX timestamp referring to the time of the submission’s creation, e.g., 1483228803\n\n\ndistinguished\nString\nFlag to determine whether the comment is distinguished by the moderators. “null” meansnot distinguished\n\n\nedited\nLong\nFlag indicating if the comment has been edited. Either the UNIX timestamp that the commentwas edited at, or “false”.\n\n\ngilded\nInteger\nThe number of times this comment received Reddit gold, e.g., 0\n\n\nid\nString\nThe comment’s identifier, e.g., “dbumnq8”\n\n\nis_submitter\n\n\n\n\nlink_id\nString\nIdentifier of the submission that this comment is in, e.g., “t3 5l954r”\n\n\nparent_id\nString\nIdentifier of the parent of this comment, might be the identifier of the submission if it is top-levelcomment or the identifier of another comment, e.g., “t1 dbu5bpp”\n\n\npermalink\nString\nRelative URL of the permanent link that points to this specific submission,e.g., “/r/AskReddit/comments/5lcgj9/what did you think of the ending of rogue one/”\n\n\nretrieved_on\nInteger\nUNIX timestamp that refers to the time that we crawled the comment, e.g., 1483228803\n\n\nscore\nInteger\nThe score of the comment. The score is the number of upvotes minus the number ofdownvotes. Note that Reddit fuzzes the real score to prevent spam bots. E.g., 5\n\n\nstickied\nBoolean\nFlag indicating whether the submission is set as sticky in the subreddit, e.g., false\n\n\nsubreddit\nString\nName of the subreddit that the comment is posted. Note that it excludes the prefix /r/. E.g., ’AskReddit’\n\n\nsubreddit_id\nString\nThe identifier of the subreddit where the comment is posted, e.g., “t5 2qh1i”"
  },
  {
    "objectID": "secondary/authors.html",
    "href": "secondary/authors.html",
    "title": "Authors",
    "section": "",
    "text": "Authors\n\n\n\nAlex Pattarini\n I am a second-year student in Georgetown University’s Data Science and Analytics Accelerated Program after receiving a BA in Government at GU. I am most interested in analyzing geospatial and temporal data. Some of my other interests include sports analytics, history, and programming. amp419@georgetown.edu\n\n\nLandon Carpenter\n I’m currently a second-year student in the Master of Science in the Data Science and Analytics program at Georgetown University. My favorite topics are Natural Language Processing, Computer Vision, and Time-Series Analysis. Outside of my coursework, I enjoy playing soccer and rock climbing. lc1276@georgetown.edu\n\n\n\n\n\n\nMatt Moriarty\n I am currently a second-year student pursuing a Master of Science in Data Science and Analytics at Georgetown University. My main interest involves merging the topics of Time Series and Geographic Information Systems to perform spatio-temporal data analyses. Alongside this, I am very interested in Deep Learning, Data Visualization, and Data Ethics! mdm341@georgetown.edu\n\n\nVictor De Lima\n I am currently a second-year student in the MS in Data Science and Analytics program. I am very interested in how machine learning models work. I want to play a part in how these models can be made better and smarter. I hope to lay a strong foundation for this during my time at the DSAN program. Some of my other interests are science in general, physics, technology, traveling, and history. I also love programming. vad49@georgetown.edu"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "By analyzing a vast collection of storytelling subreddits, we were able to extract rich information that gave us a better understanding of the details and characteristics of posts on Reddit. In doing so, we explored several different avenues related to storytelling on Reddit, providing us with a well-rounded understanding of both textual content and other features of Reddit posts.\nFirst, we sought to take only the textual content of a post and classify the subreddit to which that post belongs. To do so, we used frequency counts of the 500 most common words used across all posts as predictors to a multi-class classification model in order to classify posts as belonging to a single subreddit. Through this analysis, we found that using these frequency counts, based on the textual contents of a post alone, we were able to classify posts as belonging to the correct subreddit with success that outperformed naïve benchmark methods. However, with such a large class imbalance present in the data, we also found that it can be difficult to extract the unique features of posts belonging to subreddits that are less frequently used.\nFurthermore, we performed a deep dive into the subreddit r/AmItheAsshole with a particular focus on the flairs assigned to each post. We focused on four “primary flairs,” which were “Asshole,” “Not the A-hole,” “Everyone Sucks,” and “No A-holes here,” and performed exploratory analysis, natural language processing, sentiment analysis, and applied machine learning models. Through these analyses, we concluded that it is very difficult to accurately predict how Redditors will judge these stories according to these flairs. It is apparent that the overall manner in which Redditors judge one another may be more complex than purely the content of their posts. Additionally we found that the more positive the text of a post is, the more engagement it seemingly receives from other Reddit users, at least for this particular subreddit.\nNext, we prepared a dataset for training a Recurrent Neural Network (RNN) to generate stories. We combined high-scoring Reddit submissions with texts from classic literature like “The Scarlet Letter” and “The Odyssey.” The Reddit data was carefully curated for quality, and we applied natural language processing techniques like tokenization to prepare both sources for analysis. We then built and trained an RNN model using tools like PySpark, PyTorch, and CUDA. We implemented procedures to set the hyperparameters for better model training and avoiding overfitting. Our model showed strong predictive performance, indicated by low loss and perplexity values. Although the generated stories aren’t entirely cohesive yet, the model successfully forms coherent words, demonstrating a grasp of basic linguistic structures and marking a significant step towards more complex story generation.\nFinally, because one of the best ways to gain an understanding of a topic in a subreddit is simply to read the most popular comments, we wanted to reduce that workload as much as possible. To do this we performed performed exploratory data analysis and natural language processing to identify our topics of interest. We then found an open source model that could accurately summarize a comment to roughly a fourth of its previous size. This could greatly reduce the necessary time for someone to gain a strong understanding of the discourse happening related to a topic within a particular subreddit.\n\n\nHaving explored many different avenues regarding storytelling through Reddit, we still feel that there are many opportunities for improving our understanding of these posts even further.\nWith regard to predicting the subreddit to which a post belongs, we would like to explore additional types of multi-class classification models that are capable of ourperforming our existing model. Furthermore, to address the vast class imbalance in the dataset, we wish to employ sampling techniques that can allow us to gather a more representative and balanced dataset without the steep cost of downsampling to the least common class.\nFurther analysis related to this subreddit and how Redditors judge others could explore other types of models, other predictors, and other subreddits that also involve a similar dynamic between posters and responders.\nAlso in future work, we plan to employ more advanced techniques for text generation, mainly focusing on transformer models. These have shown superior performance in generating more coherent and diverse text. While our RNN understood basic grammar structures, it fell short of generating the stories we were expecting. Transformers are better at handling long-range dependencies, making them ideal for complex tasks like story generation. By incorporating transformers, we anticipate a significant enhancement in the model’s ability to create stories that are linguistically correct and are closer to human-like storytelling.\nFor our work in summarization we could explore the option of retraining a summarization model specifically on reddit data for a specific subreddit. Additionally, one of the biggest challenges with this goal was its subjective nature and the need for human generated refrence summaries. Our workaround was to use the GPT-4 api to generate refrence summaries and this method could be used to generate the data to retrain our model as well."
  },
  {
    "objectID": "conclusion.html#future-work",
    "href": "conclusion.html#future-work",
    "title": "Conclusion",
    "section": "",
    "text": "Having explored many different avenues regarding storytelling through Reddit, we still feel that there are many opportunities for improving our understanding of these posts even further.\nWith regard to predicting the subreddit to which a post belongs, we would like to explore additional types of multi-class classification models that are capable of ourperforming our existing model. Furthermore, to address the vast class imbalance in the dataset, we wish to employ sampling techniques that can allow us to gather a more representative and balanced dataset without the steep cost of downsampling to the least common class.\nFurther analysis related to this subreddit and how Redditors judge others could explore other types of models, other predictors, and other subreddits that also involve a similar dynamic between posters and responders.\nAlso in future work, we plan to employ more advanced techniques for text generation, mainly focusing on transformer models. These have shown superior performance in generating more coherent and diverse text. While our RNN understood basic grammar structures, it fell short of generating the stories we were expecting. Transformers are better at handling long-range dependencies, making them ideal for complex tasks like story generation. By incorporating transformers, we anticipate a significant enhancement in the model’s ability to create stories that are linguistically correct and are closer to human-like storytelling.\nFor our work in summarization we could explore the option of retraining a summarization model specifically on reddit data for a specific subreddit. Additionally, one of the biggest challenges with this goal was its subjective nature and the need for human generated refrence summaries. Our workaround was to use the GPT-4 api to generate refrence summaries and this method could be used to generate the data to retrain our model as well."
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "This section outlines the NLP procedures conducted for the project. We performed a comprehensive sentiment analysis on Reddit’s r/AmItheA*hole, revealing a dominant negative sentiment across posts, regardless of their flairs, and a correlation between positive sentiments and higher user engagement. Additional analysis of demographics showed varied sentiment distributions across age and gender. We also extend the analysis to multi-class classification on Reddit, aiming to predict authors’ age and gender using NLP and ML techniques. We found that post sentiments varied minimally across age groups, predominantly featuring negative sentiments, with a significant concentration in the 18-25 age bracket. Gender analysis echoed this trend, with negative sentiments dominating all identified genders, as numerically validated by the gender sentiment distribution table. Furthermore, the data preparation for machine learning involved cleaning and transforming text through CountVectorizer, focusing on the most frequent words.\nAfter identifying “NoStupidQuestions” as the subreddit that we would investigate for topics related to Covid-19 trends we created a plot that should convey its interest over time in this particular subreddit. Separately, the research integrated Reddit submissions with classic literature texts, employing detailed NLP transformations and data filtering. This preparation will facilitate the development of the input data to train a recurrent neural network (RNN) that will generate new story submissions.\nAll sections used johnsnowlabs sparkNLP to perform the text analysis. Please see the links to the .ipynb notebooks in each section for the specific implementation."
  },
  {
    "objectID": "nlp.html#executive-summary",
    "href": "nlp.html#executive-summary",
    "title": "NLP",
    "section": "",
    "text": "This section outlines the NLP procedures conducted for the project. We performed a comprehensive sentiment analysis on Reddit’s r/AmItheA*hole, revealing a dominant negative sentiment across posts, regardless of their flairs, and a correlation between positive sentiments and higher user engagement. Additional analysis of demographics showed varied sentiment distributions across age and gender. We also extend the analysis to multi-class classification on Reddit, aiming to predict authors’ age and gender using NLP and ML techniques. We found that post sentiments varied minimally across age groups, predominantly featuring negative sentiments, with a significant concentration in the 18-25 age bracket. Gender analysis echoed this trend, with negative sentiments dominating all identified genders, as numerically validated by the gender sentiment distribution table. Furthermore, the data preparation for machine learning involved cleaning and transforming text through CountVectorizer, focusing on the most frequent words.\nAfter identifying “NoStupidQuestions” as the subreddit that we would investigate for topics related to Covid-19 trends we created a plot that should convey its interest over time in this particular subreddit. Separately, the research integrated Reddit submissions with classic literature texts, employing detailed NLP transformations and data filtering. This preparation will facilitate the development of the input data to train a recurrent neural network (RNN) that will generate new story submissions.\nAll sections used johnsnowlabs sparkNLP to perform the text analysis. Please see the links to the .ipynb notebooks in each section for the specific implementation."
  },
  {
    "objectID": "nlp.html#analysis-report",
    "href": "nlp.html#analysis-report",
    "title": "NLP",
    "section": "Analysis Report",
    "text": "Analysis Report\n\nFlair Sentiment Model\nIn this section we explored the textual content of the posts of r/AmItheA*hole (r/AITA) with respect to their assigned flairs and subsequently applying a pretrained sentiment model. Using a sparkNLP pipeline, all text posts from r/AITA in 2022 with one of the four primary flairs (A*hole, Not the A*hole, Everybody Sucks, No A*holes Here) attached were processed/cleaned and run through a pretrained sentiment model. The sentiment model most commonly assigned these posts a “negative” sentiment, as shown in Figure 1. This holds true across posts assigned any of the four primary flairs.\n\n\n\nFigure 1: Shows the subreddit sentiment by flair.\n\n\nTo further delve into the sentiments of these posts in r/AITA, we analyzed these sentiment assignments with respect to the engagement a post receives, represented by the number of comments under each posts. In the Figure 2, the mean number of comments per post grouped by flair assignment and sentiment assignment.\n\n\n\nFigure 2: Shows the subreddit engagement by flair.\n\n\nFrom this plot above, we can extract several conclusions. We can see that posts assigned the “A*hole” flair receive the most user engagement (on average) and posts assigned “No A-holes here” receive the least engagement, on average. Additionally, it is apparent that posts assigned a “negative” sentiment receive less user engagement than those with a “positive” sentiment across all four primary flairs. Thus, it is possible there could be a relationship wherein the more “positive” a post’s sentiment/writing is, the more likely it is to receive more engagement (at least in the form of number of comments).\nTable 1 below is a numerical representation of Figure 2 showing the average number of comments per post by flair and sentiment assignment.\n\n\n\n\nTable 1: Displays the r/AITA subreddit sentiment engagements.\n\n\nFlair\nNegative\nNeutral\nPositive\n\n\n\n\nAsshole\n242.2\n225.59\n265.44\n\n\nEveryone Sucks\n104.88\n172.31\n183.61\n\n\nNo A-holes here\n63.15\n67.74\n90.92\n\n\nNot the A-hole\n105.07\n128.07\n148.4\n\n\n\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nExtracting the age and gender of the author of the post\nIn using NLP and ML techniques to predict the age and gender of the author who created a post, it’s important to observe the distribution of ages and genders as they relate to other variables of interest. In particular, it can be interesting to observe these variables as they relate to post sentiment. Note that the age and gender of the author of the post is not provided in the dataset and is instead extracted from the post itself using regex, if available.\nIn Figure 3 we can see a density plot of the age of the author of a post, grouped by the categorized sentiment of the post itself. We find that there doesn’t seem to be much of a difference between the distribution of ages for each sentiment, as each seems to peak around ages between 18 and 25 years old. Furthermore, we can clearly see that the vast majority of posts contain negative sentiment - likely posts written about negative experiences and asking for advice.\n\n\n\nFigure 3: Shows how age is distributed across sentiment\n\n\nWe can visualize similar features regarding the identified gender of the author of a post in Figure 4. Here, we can see the distribution of categorized sentiment of posts, grouped by the identified gender of the author of the post. Again, we find that the distribution of sentiments across genders is relatively similar, with the vast majority of posts containing negative sentiment.\n\n\n\nFigure 4: Shows how gender is distributed across sentiment\n\n\nTable 2 represents the same data as above, but expresses the relative frequencies of categorized post sentiment, grouped by the author’s identified gender. Here, we find numerically that the distribution of sentiments is very similar across genders, with negative sentiments making up almost 95% of instances for all genders.\n\n\n\n\nTable 2: Displays the distribution of sentiment across genders.\n\n\nFemale\nMale\nOther\n\n\n\n\n0.95\n0.95\n0.93\n\n\n0.02\n0.02\n0.02\n\n\n0.03\n0.03\n0.05\n\n\n\n\n\n\n\n\nNLP with CountVectorizer\nIn order to incorporate NLP-based predictors in a machine learning model, we first have to process the content of each reddit post such that it can appropriately be fed into the model. Importantly, textual data cannot be sent directly into a machine learning model - we must represent the text numerically such that it can be interpreted by the model. One way in which we do this is by identifying how many times each word is used in each post.\nIn preparing the data for machine learning models, we first want to clean the textual data before we represent it numerically. In doing so, we opt to perform the following cleaning steps:\n\nRemove all special characters, retaining only alphabetic characters and spaces: This will help us focus only on the words used, rather than any punctuation present.\nConvert the text to lowercase: This will help us standardize word usage by interpreting capitalized and non-capitalized words as the same.\nTokenize the text: This will help us break down posts into their individual word tokens, rather than maintaining one long document.\nRemove “stop words”: This will help us remove extremely common words, such as “a” and “and”, so that we can focus more on the selective vocabulary that each author uses.\nStem and lemmatize words: This will help us take different variations of the same word, such as “run”, “running”, and “ran”, and reduce them into the stem of the word (“run”).\n\nIn performing these cleaning steps, we obtain representations of the textual components of each post that we can begin to represent numerically. To do this, we conduct a few more steps:\n\nCalculate word frequencies: This will help us obtain a numeric representation for each word within each document - the number of time it occurs.\nSubset to the “n” most frequent words across all documents: This will help us filter out very uncommon words, including, but not limited to, misspelled words.\n\nBy obtaining word frequencies and subsetting the vocabulary to a more manageable size, we can retain a large portion of the total words used, while greatly reducing the search space with respect to our vocabulary. For instance, the top 10% most frequent words used might make up 80% of the total words used, with most words being used very infrequently (Zipf’s Law).\nTable 3 is a glimpse at what our transformed dataset looks like as it prepares to be sent into a machine learning model. We obtain “n” columns - one for each of the “n” most frequent words - with values representing the frequency with which they appear in each document. These features can be combined with other features of the data, such as the number of comments associated with a post, to form a feature set suitable for a machine learning model.\n\n\n\n\nTable 3: Displays a preview of the CountVectorized submissions data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubreddit\n‘like’\n‘feel’\n‘want’\n‘know’\n‘time’\n‘tell’\n‘get’\n‘im’\n‘think’\n‘friend’\n\n\n\n\nantiwork\n1\n0\n1\n0\n2\n0\n0\n0\n0\n0\n\n\nunpopularopinion\n2\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\nAmItheAsshole\n2\n2\n2\n0\n1\n0\n1\n4\n4\n0\n\n\nNoStupidQuestions\n1\n1\n0\n1\n0\n1\n0\n0\n3\n0\n\n\nTrueOffMyChest\n0\n0\n3\n1\n0\n1\n0\n0\n0\n0\n\n\nrelationship_advice\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nrelationship_advice\n0\n1\n0\n1\n0\n2\n0\n1\n0\n2\n\n\nNoStupidQuestions\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nrelationship_advice\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nrelationship_advice\n0\n0\n1\n5\n2\n3\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nPreparing Covid-19 data in NoStupidQuestions\nAfter performing NLP on the NoStupidQuestions subreddit to find longer posts related to COVID-19 for later Machine-Learning-related work. We wanted to visualize the interest over time to see if anything stood out or if there were recognizable trends. From Figure 5, we can see a couple of large spikes in interest. The parquet file should capture these spikes and will be a great place to perform our Machine Learning summarization.\n\n\n\nFigure 5: Shows how gender is distributed across sentiment\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nPreparing Reddit and External Data for Training an RNN\nFor this exercise, we focused on preparing text that contains great storytelling to train a Recurrent Neural Network (RNN) that can generate new stories. We use our 12 months of Reddit submissions data described in the EDA section for the analysis. Additionally, we integrate external data containing the text of famous stories from Project Gutenberg books that have stood the test of time, specifically The Scarlet Letter by Nathaniel Hawthorne [1], The Odyssey by Homer [2], Crime and Punishment by Fyodor Dostoyevsky [3], Metamorphosis by Franz Kafka [4], and The Great Gatsby by F. Scott Fitzgerald [5]. While the stories retrieved from Reddit have high scores, adding books from external sources makes sure that out training data will contain story telling that has stood the test of time and trascended generations. This may make the model more likely to output better, more compelling output.\nTo prepare the Reddit data, we extracted only the relevant information from the parquets, such as subreddit, title, selftext, score, and URL, and filtered out deleted or empty submissions. To select the best stories, we used a regular expressions pattern to remove any “Edit:” sections to remove post-edit additions that could skew the analysis. Since stories must be at least a few paragraphs, we removed all posts that didn’t have at least 4,500 characters (around 750 words). Then, we filtered for only stored with score in the top 85th percentile, thereby focusing on submissions that garnered significant user interaction.\nWe then combined the text sources. The data underwent a series of NLP transformations, including custom tokenization and lowercasing, to prepare it for advanced analysis. We constructed a vocabulary and transformed the individual characters into tokens. The resulting frequency of each token is shown in Figure 6. Lastly, we stored the processed data in a structured Parquet format alongside the character-to-index mappings, crucial for the subsequent machine-learning modeling.\n\n\n\nFigure 6: Shows the count of the Top 10 tokens\n\n\nAs an additionally way to visualize the resulting dataset, we also can see the results of the top 10 N-Grams, where \\(N=5\\) in Table 4.\n\n\n\n\nTable 4: displays the top 10 most frequent N-Grams where \\(N=5\\).\n\n\n5-Gram (incl spaces)\nCount\n\n\n\n\ni was\n22,165\n\n\nand i\n17,933\n\n\nin the\n15,264\n\n\nof the\n13,807\n\n\nthat i\n11,631\n\n\nto be\n10,346\n\n\nit was\n10,172\n\n\nto the\n9,418\n\n\ni had\n8,307\n\n\ni dont\n8,212\n\n\n\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\n\n\n\n\nThe external data is available here.\n\n\n\n\n\nData Storage\nOutputs of the NLP cleaning procedures are stored in .parquet the team bucket s3a://project17-bucket-alex/ for ease of use with ML models."
  },
  {
    "objectID": "discussion.html",
    "href": "discussion.html",
    "title": "Feedback Discussion",
    "section": "",
    "text": "The business goals do not clearly articulate the purpose and rationale for undertaking the project.\n\nWe added a section in the introduction regarding the project goals which provides a clearer explanation of how the goals fit together. We also update the text of the individual goals to better reflect their rationale.\n\nAdditionally, the business goals include some technical terms, focusing more on how to address the question rather than fully connecting with what the audience can gain from it. The technical approach, on the other hand, is clearly directed towards a technical audience, employing technical language to explain the implementation.\n\nWe revised the business goals that had this issue and removed the technical terms from them. We made sure that the technical terms are only included in the technical section.\n\n\n\n\n\n\n\nThe data quality checks need to be explicitly mentioned on the website, ensuring basic checks like missing values, outliers, and data distributions are performed. Suggest including a section on data quality checks and potential issues identified.\n\nThis is included in the project, please see the following link.\n\nFor the plot in data cleaning, consider replacing “Validity” with the name of the subreddit for clarity.\n\nThe subreddits are included in the top of the plot. The “Validity” text was removed for better visualization.\n\n\n\n\n\n\n\nA brief discussion about the external dataset and its relevance to project goals would enhance the project plan’s completeness.\n\nWe added a language to the section discussing how books from external sources will improve the RNN model to improve on this point.\n\n\n\n\nWe did not received feedback for this section.\n\n\n\n\nThe inability to view code on the website is a significant issue (We saw that you sent the code and data file separately by email, but it is better to be available on the website). Ensure that all notebooks are accessible to the audience in the later submission, as transparency in the coding process is crucial.\n\nWe requested to the instructional team to make our GitHub repo public. To keep the rendered pages clean, we prefer not filling up the body of the website with code. In each section, we provided links to the code in GitHub which are now accesible publicly.\n\nBe cautious about the professor’s advice on using only 10,000 rows in plotting; it seems that most graphs and tables contain over 10,000 data points.\n\nWe verified that this is not the case in any of the plots."
  },
  {
    "objectID": "discussion.html#project-plans",
    "href": "discussion.html#project-plans",
    "title": "Feedback Discussion",
    "section": "",
    "text": "The business goals do not clearly articulate the purpose and rationale for undertaking the project.\n\nWe added a section in the introduction regarding the project goals which provides a clearer explanation of how the goals fit together. We also update the text of the individual goals to better reflect their rationale.\n\nAdditionally, the business goals include some technical terms, focusing more on how to address the question rather than fully connecting with what the audience can gain from it. The technical approach, on the other hand, is clearly directed towards a technical audience, employing technical language to explain the implementation.\n\nWe revised the business goals that had this issue and removed the technical terms from them. We made sure that the technical terms are only included in the technical section."
  },
  {
    "objectID": "discussion.html#eda-work",
    "href": "discussion.html#eda-work",
    "title": "Feedback Discussion",
    "section": "",
    "text": "The data quality checks need to be explicitly mentioned on the website, ensuring basic checks like missing values, outliers, and data distributions are performed. Suggest including a section on data quality checks and potential issues identified.\n\nThis is included in the project, please see the following link.\n\nFor the plot in data cleaning, consider replacing “Validity” with the name of the subreddit for clarity.\n\nThe subreddits are included in the top of the plot. The “Validity” text was removed for better visualization."
  },
  {
    "objectID": "discussion.html#nlp-work",
    "href": "discussion.html#nlp-work",
    "title": "Feedback Discussion",
    "section": "",
    "text": "A brief discussion about the external dataset and its relevance to project goals would enhance the project plan’s completeness.\n\nWe added a language to the section discussing how books from external sources will improve the RNN model to improve on this point."
  },
  {
    "objectID": "discussion.html#ml-work",
    "href": "discussion.html#ml-work",
    "title": "Feedback Discussion",
    "section": "",
    "text": "We did not received feedback for this section."
  },
  {
    "objectID": "discussion.html#websiteresults",
    "href": "discussion.html#websiteresults",
    "title": "Feedback Discussion",
    "section": "",
    "text": "The inability to view code on the website is a significant issue (We saw that you sent the code and data file separately by email, but it is better to be available on the website). Ensure that all notebooks are accessible to the audience in the later submission, as transparency in the coding process is crucial.\n\nWe requested to the instructional team to make our GitHub repo public. To keep the rendered pages clean, we prefer not filling up the body of the website with code. In each section, we provided links to the code in GitHub which are now accesible publicly.\n\nBe cautious about the professor’s advice on using only 10,000 rows in plotting; it seems that most graphs and tables contain over 10,000 data points.\n\nWe verified that this is not the case in any of the plots."
  },
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "Topics",
    "section": "",
    "text": "Topics\nThe questions we will address in this work are:\nIdea 1\n\nBusiness goal: Given a textual post, classify the subreddit to which it belongs.\nTechnical approach: Use NLP techniques to construct a feature set from the textual components of a post. These features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features. Use the newly created features to build a multi-class classification model capable of classifying the subreddit to which a post belongs.\n\nIdea 2\n\nBusiness goal: Given a textual post, predict the age and gender of the author of the post.\nTechnical approach: Use NLP techniques to extract the age and gender of the author of a post, if available (as an example: “My brother (24M) and I (23M) went to the store…”). Use NLP techniques to construct a feature set from the textual components of a post. Again, these features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features. Use the newly created features to build two models: a regression model for predicting the age of the author of the post, and a classification model for classifying the gender of the author of the post.\n\nIdea 3\n\nBusiness goal: Use a pre-trained model to build features surrounding NLP for our ML tasks.\nTechnical approach: Identify available pre-trained models in the realm of NLP and what they are trained to extract from textual data. Use a pre-trained model, or models, to perform NLP tasks for downstream tasks. For instance, use a pre-trained model to obtain word embeddings (vector representations of words) that can be used as features in a subsequent ML model.\n\nIdea 4\n\nBusiness goal: Predict the popularity of text-based submissions based on their text content.\nTechnical approach: Using NLP techniques on the text posts of various similarly sized subreddits (by number of subscribers) that typically contain posts with moderate to high word counts, extract the most pertinent features. Apply supervised machine learning models (e.g., regression) to predict the number of upvotes and/or comments a given text post will receive. Identify the most influential/important words and phrases that predict a post’s popularity and calculate accuracy metrics based on actual post popularity.\n\nIdea 5\n\nBusiness goal: Determine/predict “flairs” of Reddit posts in r/AITA based on their text content.\nTechnical approach: Focusing primarily on the subreddit r/AITA and its flairs (Asshole, Not the A-hole, Everyone Sucks, No A-holes here), apply NLP techniques such as tokenization on text-based submissions to extract the important contents of each post. Apply a multi-class classification model trained on labelled r/AITA posts (i.e., text posts with a flair) to predict which flair a given post will receive based on its text content. Present confusion matrices on testing dataset and identify the words/phrases most commonly associated with each flair type.\n\nIdea 6\n\nBusiness goal: Determine which subreddit posts belong to based on their sentiment.\nTechnical approach: Use NLP techniques to clean content of various text based subreddits. Apply sentiment modeling to these posts. Train a classification model on these posts. Assess accuracy of the model using unlabelled posts (i.e., posts where the subreddit is not identified). Present findings in confusion matrices and calculate various accuracy metrics.\n\nIdea 7\n\nBusiness goal: Evaluate the relationship between the number of comments and the score of Reddit posts to establish an ‘interaction_score’ as an aggregate engagement metric.\nTechnical approach: We plan to analyze the relationship between the number of comments and post scores on social media platforms. This involves collecting data grouped by subreddits or similar categories and using pyspark.sql.functions to calculate the correlation between these two metrics. We will develop an interaction_score metric based on our findings, averaging the number of comments and post scores. This new metric aims to provide a unified measure of user engagement across various posts and platforms.\n\nIdea 8\n\nBusiness goal: Assess the impact of not-safe-for-work (NSFW) content on user engagement.\nTechnical approach: To analyze the influence of NSFW content on user engagement, we will utilize a dataset of submissions, focusing on those marked with the over_18 flag. Considering the limited proportion of such posts, we’ll create a balanced dataset by randomly selecting an equivalent number of submissions without the NSFW tag. This approach ensures a fair comparison between NSFW and non-NSFW content. We will employ the interaction_score from Topic 1 as a primary measure of user engagement. Our methodology includes generating a boxplot to visualize the distribution of interaction scores for both NSFW and non-NSFW posts.\n\nIdea 9\n\nBusiness goal: Identify the times of the day when posts typically receive the most engagement.\nTechnical approach: Implement a data analysis process that focuses on understanding the temporal patterns of user engagement on social media posts. Utilize the created_utc column from the dataset to create two new variables: week_of_the_year and hour_of_the_day. Exclude the first two days of 2022 to maintain accurate weekly categorization, as these days are part of week 53 of 2021. Aggregate and analyze the data based on these new variables to reveal patterns in user engagement across different times of the day and weeks of the year. The outcome will be visualized through a comprehensive plot, illustrating the times when posts receive the most engagement, thus guiding content strategies for optimal post timing.\n\nIdea 10\n\nBusiness goal: Is there any correlation between engagement and how controversial a comment is?\nTechnical approach: Use NLP to determine if a controversial comment is more likely to correlate with different engagement metrics.\n\nIdea 11\n\nBusiness goal: Identify whether current events are shown in different subreddits and their level of engagement.\nTechnical approach: Use NLP to identify current events in different subreddits and their level of engagement.\n\nIdea 12\n\nBusiness goal: Summarise a reddit comment.\nTechnical approach: Use a pretrained machine learning model to summarize the contents within a comment."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA",
    "section": "",
    "text": "We selected a subset of the data related to subreddits dedicated to storytelling during 2022. Namely, we chose the 12 subreddits r/AITA, r/AskMen, r/AskWomen, r/TrueOffMyChest, r/unpopularopinion, r/tifu, r/socialskills, r/antiwork, r/relationship_advice, r/explainlikeimfive, r/OutOfTheLoop, and r/NoStupidQuestions.\nUsing PySpark, we processed the .parquet files provided for the project in the s3://bigdatateaching/reddit-parquet/ bucket by subseting it only to the subreddits of interest. We read the comments and submissions data, filtered them based on the subreddits list, and wrote the results to our own group S3 directory we created for the project. We ran this procedure as a PySparkProcessor job using Amazon SageMaker. We used 8 ml.m5.xlarge instances. The code for this procedures is available here. The data we acquired has a shape of 3,444,283 x 68 for the submissions table and 76,503,363 x 21 for the comments table."
  },
  {
    "objectID": "eda.html#sec-data-subset",
    "href": "eda.html#sec-data-subset",
    "title": "EDA",
    "section": "",
    "text": "We selected a subset of the data related to subreddits dedicated to storytelling during 2022. Namely, we chose the 12 subreddits r/AITA, r/AskMen, r/AskWomen, r/TrueOffMyChest, r/unpopularopinion, r/tifu, r/socialskills, r/antiwork, r/relationship_advice, r/explainlikeimfive, r/OutOfTheLoop, and r/NoStupidQuestions.\nUsing PySpark, we processed the .parquet files provided for the project in the s3://bigdatateaching/reddit-parquet/ bucket by subseting it only to the subreddits of interest. We read the comments and submissions data, filtered them based on the subreddits list, and wrote the results to our own group S3 directory we created for the project. We ran this procedure as a PySparkProcessor job using Amazon SageMaker. We used 8 ml.m5.xlarge instances. The code for this procedures is available here. The data we acquired has a shape of 3,444,283 x 68 for the submissions table and 76,503,363 x 21 for the comments table."
  },
  {
    "objectID": "eda.html#sec-cleaning-and-checks",
    "href": "eda.html#sec-cleaning-and-checks",
    "title": "EDA",
    "section": "Cleaning and Checks",
    "text": "Cleaning and Checks\nOur cleaning process includes removing from both datasets the columns that are not needed for each particular set of analyses. Also, in the comments table, we remove the rows where the body has been either removed, deleted, or is empty. In the submissions table, we also remove where the selftext has been removed, deleted, or is empty. The resulting rows are 977,181 for the submissions table and 70,594,314 for the comments table. We also performed data checks after these steps to ensure no missing values were in the processed dataset. Beyond this initial cleaning, every analysis required its own cleaning, which we detailed in each corresponding section.\nThe count of posts per subreddit in the resulting initial submissions dataset is detailed in Table 1:\n\n\n\n\nTable 1: Displays the count of posts in each subreddit.\n\n\nSubreddit\nCount\n\n\n\n\nrelationship_advice\n311,882\n\n\nNoStupidQuestions\n234,253\n\n\nTrueOffMyChest\n125,159\n\n\nAmItheAsshole\n115,659\n\n\nantiwork\n76,647\n\n\nunpopularopinion\n39,642\n\n\nsocialskills\n23,005\n\n\nAskMen\n18,240\n\n\nexplainlikeimfive\n15,002\n\n\ntifu\n11,921\n\n\nOutOfTheLoop\n3,054\n\n\nAskWomen\n2,717\n\n\n\n\n\n\nAnother component of the data that we’d like to observe is how many submissions and comments we can actually extract textual information from. Below, we can see the distribution of “valid” versus “invalid” posts for each subreddit, with “invalid” posts being those that have been removed or deleted. It seems very common that posts are removed or deleted. Figure 1 shows the count of valid comments per Subreddit:\n\n\n\n\n        \n        \nFigure 1: Shows the count of valid status of comments per subreddit.\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-data-transformations",
    "href": "eda.html#sec-data-transformations",
    "title": "EDA",
    "section": "Data transformations and additional variables",
    "text": "Data transformations and additional variables\nAlthough we summarize the new variables below, they are are described within the EDA sections below.\n\nengagements: the sum of submissions and comments for a particular subreddit.\ninteraction_score an equal-weighted average of the number of comments and the score in each post.\nweek_of_the_year: created from the created_utc column, it describes the week of the year corresponding to the particular post’s date.\nhour_of_the_day: created from the created_utc column, it describes the hour of the day corresponding to the particular post’s date."
  },
  {
    "objectID": "eda.html#sec-regex-search",
    "href": "eda.html#sec-regex-search",
    "title": "EDA",
    "section": "Regex Search and Dummies",
    "text": "Regex Search and Dummies\nTo gauge the overall engagement in the posts, we used regex to create dummy variables that indicate whether the words ‘fascinating,’ ‘entertaining,’ and ‘boring’ appear on a post. We then aggregated them, with the results shown in Table 2:\n\n\n\n\nTable 2: Displays the results of the dummy variable excercise using Regex.\n\n\ncount\nfascinating\nentertaining\nboring\n\n\n\n\n1\n51,424\n30,870\n99,068\n\n\n0\n76,451,939\n76,472,493\n76,404,295\n\n\n\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-external-data",
    "href": "eda.html#sec-external-data",
    "title": "EDA",
    "section": "External Data",
    "text": "External Data\nWe have two primary sources of external data. The first is a community members dataset detailing the number of members each of the 12 subreddits has. By combining this data into the submissions table, we will get a valuable extra data point for our analysis related to the engagement of posts. This data was collected from Reddit itself. Our second source is the text of the books Metamorphosis by Franz Kafka and The Scarlet Letter by Nathaniel Hawthorne. We aim to perform an NLP time-series sentiment analysis on these books and the most engaging long-form story posts, and by calculating the correlations, we’ll be able to determine if these take the reader through a similar sentiment pattern and infer whether that is the reason for their popularity. This data was collected from Project Gutenberg. These data sources are available at here."
  },
  {
    "objectID": "eda.html#sec-relationship-comments-score",
    "href": "eda.html#sec-relationship-comments-score",
    "title": "EDA",
    "section": "The relationship between the number of comments and the score of Reddit posts",
    "text": "The relationship between the number of comments and the score of Reddit posts\nThe number of comments (num_comments) and the score of a post (score), which is the upvotes minus the downvotes the post has received, are ways to gauge engagement with the post. Determining whether these variables are correlated can justify their combination into an aggregate engagement metric. To do this, we group the submissions by subreddit and leverage the corr function from pyspark.sql.functions. Table 3 shows the results:\n\n\n\n\nTable 3: Displays the results of the correlation calculation for number of comments and scores.\n\n\nSubreddit\nCorrelation Coefficient\n\n\n\n\nexplainlikeimfive\n0.88\n\n\nOutOfTheLoop\n0.86\n\n\nAskMen\n0.85\n\n\nunpopularopinion\n0.83\n\n\nantiwork\n0.82\n\n\ntifu\n0.82\n\n\nAmItheAsshole\n0.81\n\n\nTrueOffMyChest\n0.79\n\n\nNoStupidQuestions\n0.78\n\n\nAskWomen\n0.77\n\n\nsocialskills\n0.72\n\n\nrelationship_advice\n0.64\n\n\n\n\n\n\nWe can also visualize them separately as in Figure 2:\n\n\n\nFigure 2: Shows the mean score comments by subreddit\n\n\nWe can see in both the table and figure that they are significantly correlated, which leads to the creation of the interaction_score additional variable. This metric is an equal-weighted average of the number of comments and the score in each post.\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-nsfw-content",
    "href": "eda.html#sec-nsfw-content",
    "title": "EDA",
    "section": "The impact of not-safe-for-work (NSFW) content on user engagement.",
    "text": "The impact of not-safe-for-work (NSFW) content on user engagement.\nTo determine if not safe for work post affects user interactions, first, we filter the submissions dataset for where the over_18 flag is true (which in this tiny percentage of them). Then, we randomly sample an equal amount of false cases, and with this, we create a small, balanced dataset with the same amount of posts flagged as NSFW as those that are not.\nWe can create a boxplot with this small dataset to see the distribution. Since we know from Topic 1 that the interaction_score is a good gauge of overall interaction, we can plot that variable as shown in Figure 3:\n\n\n\nFigure 3: Shows the distribution of comments and score by over_18 status\n\n\nWe can infer from the plot that NSFW content increases the engagement with the post, although more analysis will be conducted in subsequent sections.\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-times-of-day",
    "href": "eda.html#sec-times-of-day",
    "title": "EDA",
    "section": "The times of the day when posts typically receive the most engagement.",
    "text": "The times of the day when posts typically receive the most engagement.\nTo determine the times of day when a post typically receives the most engagement, we create two additional variables: week_of_the_year and hour_of_the_day, both coming from the created_utc column. We remove the first two days of 2022 as these would be considered part of week 53 of 2021. Then, we can group and pivot the count of our new variables, resulting in the Figure 4:\n\n\n\nFigure 4: Shows the average comments per hour and per week.\n\n\nThis analysis clearly shows from roughly 6:00 AM to 11:00 AM UTC (or 1:00 AM to 6:00 AM Eastern time) is low on activity in the story time subreddits.\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-community-prediction",
    "href": "eda.html#sec-community-prediction",
    "title": "EDA",
    "section": "Subreddit community prediction",
    "text": "Subreddit community prediction\nOne interesting task that we have our sights set on is predicting the subreddit to which a post belongs, given the textual components of the post. In doing so, there are many components of the data that we would like to explore. For this, we also generated additional variable engagements as the sum of submissions and comments for a particular subreddit.\nOne component of the data that we’d like to look at is the distribution of subreddits among submissions and comments. In Figure 5, we can see this distribution, with many engagements coming from the AmItheA\\*hole, relationship_advice, and antiwork subreddits. These subreddits invoke a lot of engagement from other users, in the form of comments, so this level of engagement can be expected.\n\n\n\n\n        \n        \nFigure 5: Shows the counts of subreddit engagement per month.\n\n\n\n\n\n\n\n\n\nClick on the bars of each Subreddit to focus them!\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-flair-prediction",
    "href": "eda.html#sec-flair-prediction",
    "title": "EDA",
    "section": "Flair prediction",
    "text": "Flair prediction\nIn this section we take a closer look at the subreddit r/AmItheA*hole (henceforth referred to as r/AITA) with the goal of both analyzing and predicting what “flair” is assigned to each post based on its text content. The exploratory segment of this analysis involves analyzing and visualizing the frequency counts of each “flair” in r/AITA for 2022.\nIn r/AITA, users post stories about situations in the real world where they have performed some sort of action or behaved in some certain manner, but are questioning whether their actions are good/bad, in a sense. Other users on the subreddit then comment on these story-like submissions and state whether they think the way the original poster acted was good or bad. Thus, each post is assigned a “flair” (i.e., tag) denoting the “judgment” of the post and can be any one of the following: A*hole, Not the A-hole, Everyone Sucks, and Not the A-hole. The first flair indicates the original poster acted in a reprehensible/poor manner, the second is the opposite, the third flair denotes a situation in which all parties are at fault, and the final flair indicates that no one acted in a particularly poor manner.\nThe number of posts that are flaired as each of the flairs in the r/AITA is show in Figure 6:\n\n\n\nFigure 6: Shows the subreddit’s flairs in barchart form.\n\n\nFigure 7 shows the relative proportion of them:\n\n\n\nFigure 7: Shows the subreddit’s flairs in treemap form.\n\n\nAs we can see, Redditors by and large “judge” the majority of original posters to be “Not the A-hole,” but there are certainly plenty of posts where the judgments resulted in different outcomes. The frequency of these flairs’ occurrences as well as the possible reasons behind these occurrences will be explored further in the NLP and ML sections.\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "eda.html#sec-controversial-comments",
    "href": "eda.html#sec-controversial-comments",
    "title": "EDA",
    "section": "NLP EDA for Emerging Trends and Controversial Comments",
    "text": "NLP EDA for Emerging Trends and Controversial Comments\nFigure 8 shows a regex count of the current events per Subreddit:\n\n\n\nFigure 8: Shows the topic ocurrence counts in different subreddits.\n\n\nFigure 9 shows the performance of controversial posts:\n\n\n\nFigure 9: Shows the engagement by varying metrics.\n\n\n\n\n\n\n\n\nThe code used for this section is available here and here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Figure 1: Image generated with OpenAI DALL·E 3 with prompt “a group of four friends around a campfire sharing stories in modern digital illustration style”\n\n\n\n\n\n“The most powerful person in the world is the storyteller.”\n— Steve Jobs\n\nSince ancient times, storytelling has captivated the human imagination. It has been a powerful tool for imparting wisdom and shaping cultural identities. From the oral traditions of ancient civilizations to the written narratives of modern times, stories continually evolve to reflect the complexities of human emotions and values. Society in the social media era generates vast amounts of text data in which great stories are contained and often go unnoticed. In this project, we are interested in taking a deep dive into storytelling in Reddit: how it comes to be, how it engages the audience, and what lessons we can learn to make ourselves better storytellers.\nWith its extensive following, open discourse, and strong communities, Reddit provides a substantial opportunity to make sense of valuable data. However, understanding such a large amount of data creates a challenge because of the nuanced posts and niche communities. Our exploratory data analysis, natural language processing, and machine learning models will take multiple approaches to manage this large amount of data and generate powerful insights in the most digestible manner possible.\nOur exploration spans various aspects; for example, we analyze the language used in several storytelling subreddits. Is storytelling contextual enough so that we can classify posts into specific subreddits? We’re examining NSFW posts’ influence on story engagement and the correlation between a story’s scores and comments. We utilize Natural Language Processing (NLP) techniques to unearth trends within storytelling communities, predict “flairs,” and perform sentiment analysis of stories. We also aim to use machine learning techniques to distill meaningful insights from vast quantities of storytelling text data.\nWe selected the following subreddits related to storytelling for the project:\n\n\n\nFigure 2: Shows the banners of the twelve storytelling subreddits chosen for the project\n\n\nThe selected subreddits have a large Reddit following, each with millions of subscribed members. Below, we can see the distribution of subscribers from which we obtained our Reddit posts.\n\n\nTable 1: Shows the subreddits chosen for the projects, as well as the number of members on each as of November 23, 2023.\n\n\nSubreddit\nMembers\n\n\n\n\nr/relationship_advice\n10,380,573\n\n\nr/socialskills\n3,823,567\n\n\nr/NoStupidQuestions\n4,066,116\n\n\nr/AskMen\n5,763,889\n\n\nr/TrueOffMyChest\n2,198,361\n\n\nr/explainlikeimfive\n22,635,152\n\n\nr/AITA\n11,969,361\n\n\nr/tifu\n18,480,872\n\n\nr/antiwork\n2,775,125\n\n\nr/OutOfTheLoop\n3,234,415\n\n\nr/unpopularopinion\n4,002,725\n\n\nr/AskWomen\n5,519,321\n\n\n\n\n\n\n\nIn this work, we explore various ideas centered around storytelling on Reddit. We subset our data only to contain high-membership storytelling subreddits detailed in Table 1. First, we perform EDA on the data. Our first idea is to develop a new engagement metric by correlating the number of comments with post scores. Next, we compare NSFW and non-NSFW posts to assess how not-safe-for-work content affects user engagement. Another goal is to identify peak engagement times by analyzing the temporal patterns of posts. We also investigate the link between engagement and the controversial nature of comments. We then use NLP to determine the age and gender of post authors and analyze sentiments of posts in specific subreddits, like r/AmItheA**hole, correlating them with user engagement and post flairs. We also create feature sets using NLP for ML tasks such as predicting the subreddit a post belongs to, determining post flairs based on text content, and generating Reddit-style stories from given prompts. Finally, we summarize top comments on specific topics using a pre-trained model, evaluating the summaries with accuracy metrics.\nFor a complete list of analytical goals and technical proposals, see Section 1.3\n\n\n\n\n\nThe questions we address in this work are:\nIdea 1\nEDA\n\nBusiness goal: Establish a new engagement metric employing the relationship between the number of comments and the score of Reddit posts.\nTechnical approach: We plan to analyze the relationship between the number of comments and post scores on social media platforms. This involves collecting data grouped by subreddits or similar categories and using pyspark.sql.functions to calculate the correlation between these two metrics. We will develop an interaction_score metric based on our findings, averaging the number of comments and post scores. This new metric aims to provide a unified measure of user engagement across various posts and platforms.\nFor more on this goal, click here.\n\nIdea 2\n\nBusiness goal: Assess the impact of not-safe-for-work (NSFW) content on user engagement.\nTechnical approach: To analyze the influence of NSFW content on user engagement, we will utilize a dataset of submissions, focusing on those marked with the over_18 flag. Considering the limited proportion of such posts, we’ll create a balanced dataset by randomly selecting an equivalent number of submissions without the NSFW tag. This approach ensures a fair comparison between NSFW and non-NSFW content. We will employ the interaction_score from Topic 1 as a primary measure of user engagement. Our methodology includes generating a boxplot to visualize the distribution of interaction scores for both NSFW and non-NSFW posts.\nFor more on this goal, click here.\n\nIdea 3\n\nBusiness goal: Identify the times of the day when posts typically receive the most engagement.\nTechnical approach: Implement a data analysis process that focuses on understanding the temporal patterns of user engagement on social media posts. Utilize the created_utc column from the dataset to create two new variables: week_of_the_year and hour_of_the_day. Exclude the first two days of 2022 to maintain accurate weekly categorization, as these days are part of week 53 of 2021. Aggregate and analyze the data based on these new variables to reveal patterns in user engagement across different times of the day and weeks of the year. The outcome will be visualized through a comprehensive plot, illustrating the times when posts receive the most engagement, thus guiding content strategies for optimal post timing.\nFor more on this goal, click here.\n\nIdea 4\n\nBusiness goal: Is there any correlation between engagement and how controversial a comment is?\nTechnical approach: Use NLP to determine if a controversial comment is more likely to correlate with different engagement metrics.\nFor more on this goal, click here.\n\nIdea 5\n\nBusiness goal: Given a textual post, extract the age and gender of the author of the post.\nTechnical approach: Use NLP techniques to extract the age and gender of the author of a post, if available (as an example: “My brother (24M) and I (23M) went to the store…”). Use NLP techniques to construct a feature set from the textual components of a post. Again, these features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features.\nFor more on this goal, click here.\n\nIdea 6\n\nBusiness Goal: Determine and analyze the sentiments of r/AmItheA**hole posts with respect to user engagement and “flairs.”\nTechnical Proposal: Apply a pretrained sentiment model to all non empty text posts in r/AmItheAsshole created in 2022. Compare and contrast sentiments by the levels of the four primary “flairs” (Asshole, Not the A-hole, Everyone Sucks, No A-holes here) and by various measures of user engagement (e.g., number of comments). Create various visualizations displaying these comparisons and draw conclusions.\nFor more on this goal, click here.\n\nIdea 7\n\nBusiness goal: Create a feature set from the textual components of a Reddit post.\nTechnical approach: Use NLP techniques, including CountVectorizer, to construct a feature set from the textual components of a post. These features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features.\nFor more on this goal, click here.\n\nIdea 8\n\nBusiness goal: Given a textual post, classify the subreddit to which it belongs.\nTechnical approach: Using a feature set as a result of NLP goals, build a multi-class classification model capable of classifying the subreddit to which a post belongs. This model must be capable of handling multiple, imbalanced classes and cannot be limited to just binary classification.\nFor more on this goal, click here.\n\nIdea 9\n\nBusiness goal: Determine/predict “flairs” of Reddit posts in r/AITA based on their text content.\nTechnical approach: Focusing primarily on the subreddit r/AITA and its flairs (Asshole, Not the A-hole, Everyone Sucks, No A-holes here), apply NLP techniques such as tokenization on text-based submissions to extract the important contents of each post. Apply a multi-class classification model trained on labelled r/AITA posts (i.e., text posts with a flair) to predict which flair a given post will receive based on its text content. Present confusion matrices on testing dataset and identify the words/phrases most commonly associated with each flair type.\nFor more on this goal, click here.\n\nIdea 10\n\nBusiness goal: For a provided input prompt, generate a the text of story that can posted on Reddit.\nTechnical approach: We will subset, preprocess, and tokenize both Reddit stories and content from externally sourced books, preparing them to be fed into a Recurrent Neural Network. Utilizing Pytorch, we aim to train the model by minimizing loss and perplexity as accuracy measures. The trained model should be capable of accepting an input prompt and generating a new story text accordingly.\nFor more on this goal, click here.\n\nIdea 11\n\nBusiness goal: Summarize top comments of interest for a particular topic using a pre-trained model\nTechnical approach: Identify an open-source model that can accurately summarize top comments related to a particular topic of interest. Since we don’t want to label summaries by hand and accuracy can be subjective, we are using an API call to GPT-4 to get reference summaries, and then with those summaries, we generate rouge 1, 2, and L scores to evaluate the model. Hyperparameters are tuned to keep both summaries close to 25% of the original text.\nFor more on this goal, click here."
  },
  {
    "objectID": "index.html#the-project",
    "href": "index.html#the-project",
    "title": "Introduction",
    "section": "",
    "text": "“The most powerful person in the world is the storyteller.”\n— Steve Jobs\n\nSince ancient times, storytelling has captivated the human imagination. It has been a powerful tool for imparting wisdom and shaping cultural identities. From the oral traditions of ancient civilizations to the written narratives of modern times, stories continually evolve to reflect the complexities of human emotions and values. Society in the social media era generates vast amounts of text data in which great stories are contained and often go unnoticed. In this project, we are interested in taking a deep dive into storytelling in Reddit: how it comes to be, how it engages the audience, and what lessons we can learn to make ourselves better storytellers.\nWith its extensive following, open discourse, and strong communities, Reddit provides a substantial opportunity to make sense of valuable data. However, understanding such a large amount of data creates a challenge because of the nuanced posts and niche communities. Our exploratory data analysis, natural language processing, and machine learning models will take multiple approaches to manage this large amount of data and generate powerful insights in the most digestible manner possible.\nOur exploration spans various aspects; for example, we analyze the language used in several storytelling subreddits. Is storytelling contextual enough so that we can classify posts into specific subreddits? We’re examining NSFW posts’ influence on story engagement and the correlation between a story’s scores and comments. We utilize Natural Language Processing (NLP) techniques to unearth trends within storytelling communities, predict “flairs,” and perform sentiment analysis of stories. We also aim to use machine learning techniques to distill meaningful insights from vast quantities of storytelling text data.\nWe selected the following subreddits related to storytelling for the project:\n\n\n\nFigure 2: Shows the banners of the twelve storytelling subreddits chosen for the project\n\n\nThe selected subreddits have a large Reddit following, each with millions of subscribed members. Below, we can see the distribution of subscribers from which we obtained our Reddit posts.\n\n\nTable 1: Shows the subreddits chosen for the projects, as well as the number of members on each as of November 23, 2023.\n\n\nSubreddit\nMembers\n\n\n\n\nr/relationship_advice\n10,380,573\n\n\nr/socialskills\n3,823,567\n\n\nr/NoStupidQuestions\n4,066,116\n\n\nr/AskMen\n5,763,889\n\n\nr/TrueOffMyChest\n2,198,361\n\n\nr/explainlikeimfive\n22,635,152\n\n\nr/AITA\n11,969,361\n\n\nr/tifu\n18,480,872\n\n\nr/antiwork\n2,775,125\n\n\nr/OutOfTheLoop\n3,234,415\n\n\nr/unpopularopinion\n4,002,725\n\n\nr/AskWomen\n5,519,321"
  },
  {
    "objectID": "index.html#project-goals",
    "href": "index.html#project-goals",
    "title": "Introduction",
    "section": "",
    "text": "In this work, we explore various ideas centered around storytelling on Reddit. We subset our data only to contain high-membership storytelling subreddits detailed in Table 1. First, we perform EDA on the data. Our first idea is to develop a new engagement metric by correlating the number of comments with post scores. Next, we compare NSFW and non-NSFW posts to assess how not-safe-for-work content affects user engagement. Another goal is to identify peak engagement times by analyzing the temporal patterns of posts. We also investigate the link between engagement and the controversial nature of comments. We then use NLP to determine the age and gender of post authors and analyze sentiments of posts in specific subreddits, like r/AmItheA**hole, correlating them with user engagement and post flairs. We also create feature sets using NLP for ML tasks such as predicting the subreddit a post belongs to, determining post flairs based on text content, and generating Reddit-style stories from given prompts. Finally, we summarize top comments on specific topics using a pre-trained model, evaluating the summaries with accuracy metrics.\nFor a complete list of analytical goals and technical proposals, see Section 1.3"
  },
  {
    "objectID": "index.html#sec-appendix-a",
    "href": "index.html#sec-appendix-a",
    "title": "Introduction",
    "section": "",
    "text": "The questions we address in this work are:\nIdea 1\nEDA\n\nBusiness goal: Establish a new engagement metric employing the relationship between the number of comments and the score of Reddit posts.\nTechnical approach: We plan to analyze the relationship between the number of comments and post scores on social media platforms. This involves collecting data grouped by subreddits or similar categories and using pyspark.sql.functions to calculate the correlation between these two metrics. We will develop an interaction_score metric based on our findings, averaging the number of comments and post scores. This new metric aims to provide a unified measure of user engagement across various posts and platforms.\nFor more on this goal, click here.\n\nIdea 2\n\nBusiness goal: Assess the impact of not-safe-for-work (NSFW) content on user engagement.\nTechnical approach: To analyze the influence of NSFW content on user engagement, we will utilize a dataset of submissions, focusing on those marked with the over_18 flag. Considering the limited proportion of such posts, we’ll create a balanced dataset by randomly selecting an equivalent number of submissions without the NSFW tag. This approach ensures a fair comparison between NSFW and non-NSFW content. We will employ the interaction_score from Topic 1 as a primary measure of user engagement. Our methodology includes generating a boxplot to visualize the distribution of interaction scores for both NSFW and non-NSFW posts.\nFor more on this goal, click here.\n\nIdea 3\n\nBusiness goal: Identify the times of the day when posts typically receive the most engagement.\nTechnical approach: Implement a data analysis process that focuses on understanding the temporal patterns of user engagement on social media posts. Utilize the created_utc column from the dataset to create two new variables: week_of_the_year and hour_of_the_day. Exclude the first two days of 2022 to maintain accurate weekly categorization, as these days are part of week 53 of 2021. Aggregate and analyze the data based on these new variables to reveal patterns in user engagement across different times of the day and weeks of the year. The outcome will be visualized through a comprehensive plot, illustrating the times when posts receive the most engagement, thus guiding content strategies for optimal post timing.\nFor more on this goal, click here.\n\nIdea 4\n\nBusiness goal: Is there any correlation between engagement and how controversial a comment is?\nTechnical approach: Use NLP to determine if a controversial comment is more likely to correlate with different engagement metrics.\nFor more on this goal, click here.\n\nIdea 5\n\nBusiness goal: Given a textual post, extract the age and gender of the author of the post.\nTechnical approach: Use NLP techniques to extract the age and gender of the author of a post, if available (as an example: “My brother (24M) and I (23M) went to the store…”). Use NLP techniques to construct a feature set from the textual components of a post. Again, these features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features.\nFor more on this goal, click here.\n\nIdea 6\n\nBusiness Goal: Determine and analyze the sentiments of r/AmItheA**hole posts with respect to user engagement and “flairs.”\nTechnical Proposal: Apply a pretrained sentiment model to all non empty text posts in r/AmItheAsshole created in 2022. Compare and contrast sentiments by the levels of the four primary “flairs” (Asshole, Not the A-hole, Everyone Sucks, No A-holes here) and by various measures of user engagement (e.g., number of comments). Create various visualizations displaying these comparisons and draw conclusions.\nFor more on this goal, click here.\n\nIdea 7\n\nBusiness goal: Create a feature set from the textual components of a Reddit post.\nTechnical approach: Use NLP techniques, including CountVectorizer, to construct a feature set from the textual components of a post. These features may be word “dummy variables” (the existence or non-existence of a word), word counts, n-grams (sequences of n words), or other types of features.\nFor more on this goal, click here.\n\nIdea 8\n\nBusiness goal: Given a textual post, classify the subreddit to which it belongs.\nTechnical approach: Using a feature set as a result of NLP goals, build a multi-class classification model capable of classifying the subreddit to which a post belongs. This model must be capable of handling multiple, imbalanced classes and cannot be limited to just binary classification.\nFor more on this goal, click here.\n\nIdea 9\n\nBusiness goal: Determine/predict “flairs” of Reddit posts in r/AITA based on their text content.\nTechnical approach: Focusing primarily on the subreddit r/AITA and its flairs (Asshole, Not the A-hole, Everyone Sucks, No A-holes here), apply NLP techniques such as tokenization on text-based submissions to extract the important contents of each post. Apply a multi-class classification model trained on labelled r/AITA posts (i.e., text posts with a flair) to predict which flair a given post will receive based on its text content. Present confusion matrices on testing dataset and identify the words/phrases most commonly associated with each flair type.\nFor more on this goal, click here.\n\nIdea 10\n\nBusiness goal: For a provided input prompt, generate a the text of story that can posted on Reddit.\nTechnical approach: We will subset, preprocess, and tokenize both Reddit stories and content from externally sourced books, preparing them to be fed into a Recurrent Neural Network. Utilizing Pytorch, we aim to train the model by minimizing loss and perplexity as accuracy measures. The trained model should be capable of accepting an input prompt and generating a new story text accordingly.\nFor more on this goal, click here.\n\nIdea 11\n\nBusiness goal: Summarize top comments of interest for a particular topic using a pre-trained model\nTechnical approach: Identify an open-source model that can accurately summarize top comments related to a particular topic of interest. Since we don’t want to label summaries by hand and accuracy can be subjective, we are using an API call to GPT-4 to get reference summaries, and then with those summaries, we generate rouge 1, 2, and L scores to evaluate the model. Hyperparameters are tuned to keep both summaries close to 25% of the original text.\nFor more on this goal, click here."
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ML",
    "section": "",
    "text": "This section focuses on Machine Learning (ML) methods to answer our research questions. In particular, our first two goals for this section involve identifying characteristics of a Reddit post, while our last two goals are more text-based, involving Reddit post generation and summarization.\nFirst, we focus on predicting which subreddit a post belongs to based on its text. We provide a Random Forest model for this classification task, as well as a baseline model for comparison. In doing so, we find significant improvement from the baseline model to the Random Forest model, with areas of improvement still to address.\nSubsequently, we attempt to predict the flairs of posts in r/AmItheA**hole (r/AITA) using various possible predictors. We apply two Random Forest models using the text of posts in r/AITA and the user engagement on these posts, respectively. Similar to our first classification task, we find that these models perform better than a blind guess or baseline, but still have a lot of room for improvement.\nThen, we aim to create a model that can generate stories for new Reddit posts. We train a recurrent neural network (RNN) with a Long Short-Term Memory architecture on text from a mix of top stories from various subreddits and popular books. While the model successfully generates text that demonstrates an understanding of basic linguistic structures, it has yet to produce fully cohesive stories, marking a significant step towards more complex narrative generation.\nLastly, we use two pre-trained models, one for summarization and one for sentiment analysis, to understand topics selected in the natural language processing portion of our project within the r/NoStupidQuestions subreddit. With the summarization model, we identify popular comments of interest and are able to greatly reduce the amount of text while keeping the meaning of the comment. Additionally, the sentiment analysis model is able to effectively classify the sentiment of the comments in a subreddit, furthering our understanding of the discourse in the r/NoStupidQuestions subreddit.\nOverall, these studies highlight the challenges and progress in using ML for subreddit content analysis, demonstrating advancements from simple probabilistic approaches to more sophisticated models like Random Forest and RNNs. The models have varying degrees of success, suggesting room for further improvement in these areas."
  },
  {
    "objectID": "ml.html#executive-summary",
    "href": "ml.html#executive-summary",
    "title": "ML",
    "section": "",
    "text": "This section focuses on Machine Learning (ML) methods to answer our research questions. In particular, our first two goals for this section involve identifying characteristics of a Reddit post, while our last two goals are more text-based, involving Reddit post generation and summarization.\nFirst, we focus on predicting which subreddit a post belongs to based on its text. We provide a Random Forest model for this classification task, as well as a baseline model for comparison. In doing so, we find significant improvement from the baseline model to the Random Forest model, with areas of improvement still to address.\nSubsequently, we attempt to predict the flairs of posts in r/AmItheA**hole (r/AITA) using various possible predictors. We apply two Random Forest models using the text of posts in r/AITA and the user engagement on these posts, respectively. Similar to our first classification task, we find that these models perform better than a blind guess or baseline, but still have a lot of room for improvement.\nThen, we aim to create a model that can generate stories for new Reddit posts. We train a recurrent neural network (RNN) with a Long Short-Term Memory architecture on text from a mix of top stories from various subreddits and popular books. While the model successfully generates text that demonstrates an understanding of basic linguistic structures, it has yet to produce fully cohesive stories, marking a significant step towards more complex narrative generation.\nLastly, we use two pre-trained models, one for summarization and one for sentiment analysis, to understand topics selected in the natural language processing portion of our project within the r/NoStupidQuestions subreddit. With the summarization model, we identify popular comments of interest and are able to greatly reduce the amount of text while keeping the meaning of the comment. Additionally, the sentiment analysis model is able to effectively classify the sentiment of the comments in a subreddit, furthering our understanding of the discourse in the r/NoStupidQuestions subreddit.\nOverall, these studies highlight the challenges and progress in using ML for subreddit content analysis, demonstrating advancements from simple probabilistic approaches to more sophisticated models like Random Forest and RNNs. The models have varying degrees of success, suggesting room for further improvement in these areas."
  },
  {
    "objectID": "ml.html#analysis-report",
    "href": "ml.html#analysis-report",
    "title": "ML",
    "section": "Analysis Report",
    "text": "Analysis Report\n\nSubreddit Prediction\nFor our subreddit prediction task, we aim to take only the textual content of a post and classify the subreddit to which that post belongs. By using the 500 most common words across all posts, we hope to obtain important textual information that helps us determine which subreddit a post belongs to. For instance, the word “relationship” may be much more likely to appear in the r/relationship_advice subreddit than any others.\n\nBaseline Model\nBefore we dive into complex Machine Learning models, though, we start with a baseline model. The baseline model provides us with a point of comparison for our more complex Machine Learning models, allowing us to evaluate the performance of those models in comparison to the simple baseline. Our baseline model is very naïve - it simply predicts subreddits with probability equal to the proportion with which they make up the training dataset. For instance, if 50% of our training dataset contains observations from r/relationship_advice, 30% from r/NoStupidQuestions, and 20% from r/TrueOffMyChest, our baseline model will predict that a post belongs to the subreddit r/relationship_advice with probability 0.50, the subreddit r/NoStupidQuestions with probability 0.30, and the subreddit r/TrueOffMyChest with probability 0.20. In this case, of course, the model has 12 subreddits to choose from, each with their own associated probabilities. Note that the baseline model does not consider additional information, such as the textual content of the post, because it is so simple.\n\n\nRandom Forest Model - Full Dataset\nIn order to predict the subreddit to which a post belongs more effectively, we turn to the Random Forest. The Random Forest model is an ensemble method that constructs multiple “weak” classifiers, known as Decision Trees, and aggregates them to make decisions on the data provided.\nIn our case, we seek to construct a Random Forest that analyzes the word usage in a given Reddit post in order to decide to which subreddit that post belongs. For instance, the Random Forest may decide that, if a post mentions the word “relationship” more than three times, it belongs to the r/relationship_advice subreddit. Similarly, it might decide that a combination of the use of the words “work”, “job”, and “hate” in a post indicates that the post belongs to the r/antiwork subreddit. Note that the Random Forest model does consider the information present in the dataset, unlike the baseline, because it uses all present data to inform its decisions, rather than simply making a random guess.\n\n\nRandom Forest Model - Balanced Dataset\nAlthough the Random Forest model is a promising prospect for classifying the subreddit to which each of our Reddit posts belong, there are some concerns that may arise. Given the vast class imbalance present in the dataset - for example, r/relationship_advice having approximately 100 times as many posts as r/AskWomen - we fear that the Random Forest model might just learn to predict the more prevalent classes and ignore the less prevalent ones. For this reason, we seek to employ a technique that can help address the imblance of subreddit classes.\nHere, we decide to build a Random Forest model that is built upon a balanced representation of our dataset. To do so, we sample posts from each subreddit such that the number of posts in our dataset from each subreddit is roughly equivalent to the presence of the least common subreddit. Balancing the dataset in this way allows us to build a model that is not biased toward the overwhelming presence of one subreddit or another. However, it should be noted that this downsampling techniques forces us to set aside a considerable portion of our data in order to adhere to the least prevalent subreddit.\n\n\nResults\nHere, we explore the results of our collection of models in performing the subreddit prediction task.\n\nResults - Baseline Model\nBelow, in Figure 1, we can see the results of the baseline model predictions on the training data. As expected, the model predicts the more prevalent subreddits, such as r/relationship_advice and r/NoStupidQuestions, more often. Since the model only predicts subreddits proportionally to how they appear in the training data, it does not do a good job of actually identifying these subreddits correctly. In fact, its expected classification accuracy is equal to the sum of the squares of the probabilities with which each subreddit occurs, which amounts to approximately 0.20.\n\n\n\nFigure 1: Shows the confusion matrix for the Baseline Model, evaluated on the training data.\n\n\n\n\nResults - Random Forest Model - Full Dataset\nBelow, in Figure 2, we can see the results of the Random Forest model predictions on the training data. Here, we find that the performance of the model seems to outperform that of the baseline model, but still has its downfalls. The model predicts the two most prevalent subreddits, r/relationship_advice and r/NoStupidQuestions, nearly every time, failing to predict any of the less prevalent subreddits. In this sense, the model is vastly underperforming, as it is heavily biased to the subreddits that it has seen more often.\n\n\n\nFigure 2: Shows the confusion matrix for the Random Forest, evaluated on the full, unbalanced training data.\n\n\n\n\nResults - Random Forest Model - Balanced Dataset\nBelow, in Figure 3, we can see the results of the Random Forest model predictions on the downsampled, balanced training data. Here, we find that the model actually predicts each subreddit some number of times, without really avoiding any of them, like it did with the full, unbalanced dataset. For the vast majority of subreddits, it appears that the model correctly predicts the subreddit the majority of the time. However, there are still a lot of instances where the model predicts incorrectly, especially in cases of subreddits like r/AskMen and r/NoStupidQuestions.\n\n\n\nFigure 3: Shows the confusion matrix for the Random Forest, evaluated on the downsampled, balanced training data.\n\n\n\n\nResults - Evaluation Metrics\nBelow, in Figure 4, we can see the evaluation metrics for the three multi-class classification models described above. These metrics include:\n\nAccuracy: a measure of how often the model correctly classified the subreddit to which a post belongs\nF1-Score: the harmonic mean between the precision and recall scores (below)\nWeighted Precision: a weighted measure of how often the model correctly classified the subreddit to which a post belongs, given that it predicted a particular subreddit\nWeighted Recall: a weighted measure of how often the model correctly classified the subreddit to which a post belongs, given that the post belongs to particular subreddit\n\nBy comparing each model type, we find that the baseline model vastly underperforms when compared to the Random Forest models. However, almost surprisingly, we find that the Random Forest Model that was trained on the full, unbalanced dataset outperforms that of the downsampled, balanced dataset by as much as 10% across all evaluation metrics. It is likely that the balanced dataset exhibits a trade-off between the amount of data and the balance of subreddit classes that did not pay off. In any case, it is great to see that our Machine Learning models greatly outperform the baseline model for this task. However, we would like to see further improvement of this model while still maintaining all twelve storytelling subreddits.\n\n\n\n\n        \n        \nFigure 4: Shows the evaluation metrics for the multi-class classification models above.\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\n\n\nFlair Prediction Using Random Forest Classification\nIn this section we attempted to predict what flair is assigned to posts in r/AmItheA**hole (r/AITA) based on various different predictors using a Random Forest (RF) model to attempt to predict how Redditors “judge” these stories posted on r/AITA. We then compared these models with their varying predictors and compared them to a baseline model (random chance).\nThe first RF model we applied to the r/AITA data using token counts of the five hundred most common words which were extracted using CountVectorizer in the NLP section previously. We chose fifty trees as our hyperparameter to be used across all models used in this section to allow for consistent comparisons. However, due to the imbalanced nature of the r/AITA posts (as established in the EDA section of this project), the dataset was downsampled so that none of the four primary flairs (A**hole, Not the A-hole, Everyone Sucks, No A-holes here) are overrepresented to the extent that they would significantly hinder model performance. After a training and testing data split, the model was trained on the training subset and various model metrics were calculated for both the training and testing subsets via a SparkML pipeline. Measures and visualizations of this model’s efficacy are displayed below.\n\n\n\nFigure 5: Shows the confusion matrix for the Random Forest, evaluated on the training data.\n\n\n\n\n\nFigure 6: Shows the confusion matrix for the Random Forest, evaluated on the testing data.\n\n\n\n\n\nFigure 7: Shows the evalutation metrics for the Random Forest.\n\n\nBased on the metrics above, this model did not predict the flairs of these posts particularly accurately, but performed better than a blind random guess, which would have a theoretical accuracy of 25% compared to our model’s ~30-40%. For both the training and testing subsets, the model did an extremely poor job at predicting “Not the A-hole posts”. This model does perform reasonably well when predicting “Everyone Sucks” and “Asshole” flaired posts, but does not predict the posts with the more positively connoted flairs (No A-holes here and Not the A-hole).\nAnother potential set of predictors we identified were measures of user engagement, namely post “score” (number of upvotes minus number of downvotes) and the number of comments under a post. We applied a similar model using these predictors via another SparkML pipeline and compared them to the previous text-based model. The measures of model performance and confusion matrices are visualized below as such.\n\n\n\nFigure 8: Shows the confusion matrix for the Random Forest, evaluated on the training data.\n\n\n\n\n\nFigure 9: Shows the confusion matrix for the Random Forest, evaluated on the training data.\n\n\n\n\n\nFigure 10: Shows the evaluation metrics for the Random Forest.\n\n\nAs shown above, this model performs fairly similarly to the previous text-based model but with some slight improvements in some of the model performance metrics along with more comparative performance metrics of the model for the training and test sets. This model more effectively predicted posts with the flairs “Asshole” and “Everyone Sucks” compared to the text-based model, but similarly struggled to correctly identify posts with more positive flairs. Ultimately, while this model does perform slightly better than the previous text-based model, especially at predicting the posts with more negative flairs attached, it still would not serve as an effective tool for accurately predicting these flairs on a larger scale. It is possible that these data are too homogeneous to be easily differentiated using a machine learning model, or using different models and/or hyperparameters may generate more accurate predictions.\nWe also include the evaluation metrics in Table 1 below:\n\n\n\n\nTable 1: Displays the evaluation metrics of both engagement and text-based models.\n\n\n\n\n\n\n\n\n\nMetric\nEngagement Training\nEngagement Test\nText Training\nText Test\n\n\n\n\naccuracy\n0.396378\n0.396137\n0.420226\n0.329193\n\n\nf1\n0.393809\n0.392595\n0.382704\n0.28115\n\n\nprecision\n0.395841\n0.394806\n0.459258\n0.309424\n\n\nrecall\n0.396378\n0.396137\n0.420226\n0.329193\n\n\n\n\n\n\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nStory Generation\n\n\n\n\n\n\nNote\n\n\n\nThe saved model used for this section is available here.\n\n\nIn our project, we developed a story generation model using a recurrent neural network (RNN) model, which we built and trained using PySpark and PyTorch. To enhance the computation speed, we integrated CUDA into the process. Our data is the mix of top stories from various subreddits and popular books sourced externally, developed during the NLP portion of the project. The preprocessed portion of the dataset usable for training purposes is 1037 MB. We found that a smaller subset of 160.59 MB was adequate for the analysis.\nThe model’s architecture is based on a Long Short-Term Memory (LSTM) layer, which captures long-term dependencies in sequential data. In our setup, we defined hyperparameters such as the input size of 128, hidden state at 256, and used a two-layer LSTM. The model has 12,766,510 trainable parameters.\nFor the training process, we set the learning rate at 0.01, a maximum of 100 epochs, and a batch size of 64. To ensure the model didn’t overfit, we employed an early stopping mechanism with a patience of 5 epochs and a validation loss improvement threshold of 0.01. The Adam optimizer was chosen for optimization, paired with a cross-entropy loss function for calculating the model’s error rate. The total training time was 1h 41 mins.\nThe training and validation perplexities are shown in Figure 11:\n\n\n\nFigure 11: Shows the training and validation perplexities of the RNN training process\n\n\nTo evaluate the model’s effectiveness, we focused on loss and perplexity. A lower perplexity value suggests a higher predictive accuracy of the model. The model achieved a test loss of 1.6227 and a perplexity value of 5.0667, indicating a strong performance in predictive capabilities.\nWe were ready to generate stories with the trained. Some examples are shown in Table 2:\n\n\nTable 2: Shows the prompt and generated text by the RNN model\n\n\n\n\n\n\nPrompt\nGenerated Text\n\n\n\n\nOnce upon a time\nOnce upon a time to of a cause the gachen friends these any olden lands. it was the school all as the sach could boypheal, and they letes saying cut that ended about the told the asking.the said she so a kyastellow specially will wrong me have a glories and how in s\n\n\nThe sun set over the ancient, whispering forest\nThe sun set over the ancient forest and the sating the told the thought the pressated the hassed has all she was hands and the and the and of didnt conday her that there you hands my like i was the and i was the told when and the put all into the done to the down the sately and stouse\n\n\nThe sound of sirens pierced the night\nThe sound of sirens pierced the night stack and the down, my fear that my bly and were expetes his slaring when it becheads icky that my feelt and want and contores she givated hours. we let me processings that i only and low that on a mord of the past finding to this because oulling th\n\n\n\n\nAlthough our model cannot yet generate cohesive stories, it’s important to recognize the success in the underlying process. The characters produced by the model consistently combine to form coherent words, an ability that demonstrates the model’s understanding of basic linguistic structures, a foundational step toward more complex story generation. This aspect of the model’s output aligns well with the objectives of our project.\n\n\n\n\n\n\nThe code used for this section is available here.\n\n\n\n\n\nTop Comment Summary Generation\nAfter all of the NLP work to identify comments that were scored high and contained the topic we had chosen, in this case comments related to COVID-19 in the subreddit r/NoStupidQuestions. We were successfully able to identify comments that we would be useful to summarize to better gain an understanding of the narrative in a particular subreddit. One example of this is below.\nThe model used to perform this summarization was a “facebook/bart-large-cnn” [1].\n\n\n\nFigure 12: Shows the result of the sentiment analysis using the pre-trained model.\n\n\n\nBefore Summarization (After Stemming and removing StopWords):\n\nI can understand that frustration and getting tired of the cynicism or generation of toxic beliefs, but it should be noted that about that bit from his 1999 special about the immune system and all that, his family has outright said:\nSeveral times during the pandemic, Carlin has drawn attention for a routine from his 1999 special, “You Are All Diseased,” in which he mischievously suggests that a childhood spent swimming in the polluted Hudson River was the reason he didn’t catch polio.\n(“In my neighborhood, no one ever got polio,” he fulminates. “No one, ever. You know why? ’Cause we swam in raw sewage. It strengthened our immune systems. The polio never had a prayer.”)\nAs Kelly Carlin explained, some viewers concluded — wrongly — that her father would have opposed coronavirus vaccines.\n“Everyone’s like, see? George Carlin would have been anti-vaccination,” she said. “And I’m like, no. My dad was pro-science, pro-rational thinking, pro-evidence-based medicine. The man was a heart patient for 30 years. When he was a kid and the polio vaccine became available, he got the polio vaccine.”\n…\nIn efforts to divine his opinion, some Carlin fans pointed to a 1990 interview he gave to Larry King, when he expressed his misgivings about the crude standup of Andrew Dice Clay: “His targets are underdogs, and comedy has traditionally picked on power — people who abuse their power,” Carlin said at the time.\nKelly Carlin said her father “always took the stand that more speech is better than less speech” and would have supported Chappelle’s right to perform the special. But, she added, “if you’re a comedian, you’ve got to be funny.”\n\n\n\nAfter Summarization:\n\nCarlin has drawn attention for a routine from his 1999 special, “You Are All Diseased.” He mischievously suggests that a childhood spent swimming in the polluted Hudson River was the reason he didn’t catch polio. Some viewers concluded — wrongly — that her father would have opposed coronavirus vaccines. “Everyone’s like, see? George Carlin would have been anti-vaccination,” she said. ’I’m like, no. My dad was pro-science,. pro-rational thinking, pro-evidence-based medicine.\n\n\n\n\n\n\n\nThe code used for this section is available here."
  },
  {
    "objectID": "secondary/references.html",
    "href": "secondary/references.html",
    "title": "It's Storytime!",
    "section": "",
    "text": "References\n\n\n[1] The Reddit Archives. JSON. GitHub. 2016 [accessed 2023 Nov 2]. https://github.com/reddit-archive/reddit/wiki/JSON\n\n\n[2] Baumgartner J, Zannettou S, Keegan B, Squire M, Blackburn J. The Pushshift Reddit Dataset. 2020 [accessed 2023 Nov 1]. http://arxiv.org/abs/2001.08435. doi:10.48550/arXiv.2001.08435\n\n\n[3] Fitzgerald FS(FrancisS. The Great Gatsby. 2021. https://www.gutenberg.org/ebooks/64317\n\n\n[4] Kafka F. Metamorphosis. Wyllie D(Translator), translator. 2005. https://www.gutenberg.org/ebooks/5200\n\n\n[5] Homer. The Odyssey. Butler S, translator. 1999. https://www.gutenberg.org/ebooks/1727\n\n\n[6] Dostoyevsky F. Crime and Punishment. Garnett C, translator. 2006. https://www.gutenberg.org/ebooks/2554\n\n\n[7] Hawthorne N, Foote MH, Ipsen LS. The Scarlet Letter. 2008. https://www.gutenberg.org/ebooks/25344\n\n\n[8] Lewis M, Liu Y, Goyal N, Ghazvininejad M, Mohamed A, Levy O, Stoyanov V, Zettlemoyer L. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. 2019 [accessed 2023 Nov 30]. https://arxiv.org/abs/1910.13461. doi:10.48550/ARXIV.1910.13461\n\n\n[9] Huang AH, Wang H, Yang Y. FinBERT: A Large Language Model for Extracting Information from Financial Text. Contemporary Accounting Research. 2023 [accessed 2023 Nov 30];40(2):806–841. https://onlinelibrary.wiley.com/doi/10.1111/1911-3846.12832. doi:10.1111/1911-3846.12832"
  }
]