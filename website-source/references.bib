
@misc{the_reddit_archives_json_2016,
	title = {{JSON}},
	url = {https://github.com/reddit-archive/reddit/wiki/JSON},
	abstract = {historical code from reddit.com. Contribute to reddit-archive/reddit development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-11-02},
	journal = {GitHub},
	author = {{The Reddit Archives}},
	month = oct,
	year = {2016},
}

@misc{baumgartner_pushshift_2020,
	title = {The {Pushshift} {Reddit} {Dataset}},
	url = {http://arxiv.org/abs/2001.08435},
	doi = {10.48550/arXiv.2001.08435},
	abstract = {Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves. Reddit, the so called "front page of the Internet," in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and hundreds of billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically. In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08435 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Social and Information Networks},
}



@book{fitzgerald_great_2021,
	title = {The {Great} {Gatsby}},
	copyright = {Public domain in the USA.},
	url = {https://www.gutenberg.org/ebooks/64317},
	language = {English},
	urldate = {2023-11-20},
	author = {Fitzgerald, F. Scott (Francis Scott)},
	month = jan,
	year = {2021},
	keywords = {First loves -- Fiction, Long Island (N.Y.) -- Fiction, Married women -- Fiction, Psychological fiction, Rich people -- Fiction},
}

@book{kafka_metamorphosis_2005,
	title = {Metamorphosis},
	copyright = {Copyrighted. Read the copyright notice inside this book for details.},
	url = {https://www.gutenberg.org/ebooks/5200},
	language = {English},
	urldate = {2023-11-20},
	author = {Kafka, Franz},
	translator = {Wyllie, David (Translator)},
	month = aug,
	year = {2005},
	keywords = {Metamorphosis -- Fiction, Psychological fiction},
}



@book{homer_odyssey_1999,
	title = {The {Odyssey}},
	copyright = {Public domain in the USA.},
	url = {https://www.gutenberg.org/ebooks/1727},
	language = {English},
	urldate = {2023-11-20},
	author = {Homer},
	translator = {Butler, Samuel},
	month = apr,
	year = {1999},
	keywords = {Epic poetry, Greek -- Translations into English, Homer -- Translations into English, Odysseus, King of Ithaca (Mythological character)},
}

@book{dostoyevsky_crime_2006,
	title = {Crime and {Punishment}},
	copyright = {Public domain in the USA.},
	url = {https://www.gutenberg.org/ebooks/2554},
	language = {English},
	urldate = {2023-11-20},
	author = {Dostoyevsky, Fyodor},
	translator = {Garnett, Constance},
	month = mar,
	year = {2006},
	keywords = {Crime -- Psychological aspects -- Fiction, Detective and mystery stories, Murder -- Fiction, Psychological fiction, Saint Petersburg (Russia) -- Fiction},
}

@book{hawthorne_scarlet_2008,
	title = {The {Scarlet} {Letter}},
	copyright = {Public domain in the USA.},
	url = {https://www.gutenberg.org/ebooks/25344},
	language = {English},
	urldate = {2023-11-20},
	author = {Hawthorne, Nathaniel and Foote, Mary Hallock and Ipsen, Ludvig Sandöe},
	month = may,
	year = {2008},
	keywords = {Adultery -- Fiction, Boston (Mass.) -- History -- Colonial period, ca. 1600-1775 -- Fiction, Clergy -- Fiction, Historical fiction, Illegitimate children -- Fiction, Married women -- Fiction, Psychological fiction, Puritans -- Fiction, Revenge -- Fiction, Triangles (Interpersonal relations) -- Fiction, Women immigrants -- Fiction},
}


@article{lewis_bart_2019,
	title = {{BART}: {Denoising} {Sequence}-to-{Sequence} {Pre}-training for {Natural} {Language} {Generation}, {Translation}, and {Comprehension}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{BART}},
	url = {https://arxiv.org/abs/1910.13461},
	doi = {10.48550/ARXIV.1910.13461},
	abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
	urldate = {2023-11-30},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	year = {2019},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Machine Learning (stat.ML)},
}

@article{huang_span_2023,
	title = {{FinBERT}: {A} {Large} {Language} {Model} for {Extracting} {Information} from {Financial} {Text}},
	volume = {40},
	issn = {0823-9150, 1911-3846},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/1911-3846.12832},
	doi = {10.1111/1911-3846.12832},
	abstract = {ABSTRACT 
            We develop FinBERT, a state-of-the-art large language model that adapts to the finance domain. We show that FinBERT incorporates finance knowledge and can better summarize contextual information in financial texts. Using a sample of researcher‐labeled sentences from analyst reports, we document that FinBERT substantially outperforms the Loughran and McDonald dictionary and other machine learning algorithms, including naïve Bayes, support vector machine, random forest, convolutional neural network, and long short‐term memory, in sentiment classification. Our results show that FinBERT excels in identifying the positive or negative sentiment of sentences that other algorithms mislabel as neutral, likely because it uses contextual information in financial text. We find that FinBERT's advantage over other algorithms, and Google's original bidirectional encoder representations from transformers model, is especially salient when the training sample size is small and in texts containing financial words not frequently used in general texts. FinBERT also outperforms other models in identifying discussions related to environment, social, and governance issues. Last, we show that other approaches underestimate the textual informativeness of earnings conference calls by at least 18\% compared to FinBERT. Our results have implications for academic researchers, investment professionals, and financial market regulators. 
          ,  
            RÉSUMÉ 
             
              FinBERT : un grand modèle de langage pour l'extraction d'informations à partir de textes financiers 
             
             
              Les auteurs développent FinBERT, un grand modèle de langage innovateur qui s'adapte au domaine de la finance. Ils montrent que FinBERT intègre des connaissances financières et peut mieux résumer les informations contextuelles dans les textes financiers. À l'aide d'un échantillon de phrases classées par les chercheurs et provenant de rapports d'analystes, les auteurs démontrent que FinBERT surpasse considérablement le dictionnaire de Loughran et McDonald et d'autres algorithmes d'apprentissage automatique, y compris naïve Bayes, Support Vector Machine, la forêt aléatoire, le réseau de neurones convolutifs et la mémoire à long et court terme, dans la classification des sentiments. Leurs résultats montrent que FinBERT excelle dans l'identification du sentiment positif ou négatif dans des phrases déchiffrées à tort comme neutres par d'autres algorithmes, probablement parce que le modèle utilise les informations contextuelles issues du texte financier. Les auteurs constatent que l'avantage de FinBERT sur les autres algorithmes, et sur le modèle original BERT ( 
              bidirectional encoder representations from transformers 
              ) développé par Google, est particulièrement important lorsque la taille de l'échantillon d'entrainement est petite et dans les textes contenant des mots utilisés en finance qui ne sont pas fréquemment utilisés dans les textes généraux. FinBERT surpasse également les autres modèles dans l'identification des discussions liées aux questions environnementales, sociales et de gouvernance. Enfin, les auteurs démontrent que les autres approches sous‐estiment l'information textuelle issue des conférences téléphoniques sur les résultats, et ce d'au moins 18 \% comparativement à FinBERT. Leurs résultats ont des implications pour les chercheurs universitaires, les professionnels de l'investissement et les autorités de règlementation des marchés financiers.},
	language = {en},
	number = {2},
	urldate = {2023-11-30},
	journal = {Contemporary Accounting Research},
	author = {Huang, Allen H. and Wang, Hui and Yang, Yi},
	month = may,
	year = {2023},
	pages = {806--841},
}
