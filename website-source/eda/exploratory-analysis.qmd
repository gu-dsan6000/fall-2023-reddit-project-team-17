```{python}
#| echo: false

import pandas as pd
from tabulate import tabulate
import IPython.display as d
from IPython.display import display, HTML, IFrame
from IPython.display import Markdown


```

# Exploratory Analysis

## The Data Subset

We selected a subset of the data related to subreddits dedicated to storytelling during 2022. Namely, we chose the 12 subreddits `AITA`, `AskMen`, `AskWomen`, `TrueOffMyChest`, `unpopularopinion`, `tifu`, `socialskills`, `antiwork`, `relationship_advice`, `explainlikeimfive`, `OutOfTheLoop`, and `NoStupidQuestions`. The data we acquired has a shape of 3,444,283 x 68 for the submissions table and 76,503,363 x 21 for the comments table.

## External Data


## Cleaning 

Our cleaning process includes removing from the `comments` database both datasets the columns that are not needed for each particular set of analyses. Also, in the `comments` table, we remove the rows where the `body` has been either removed or deleted. In the `submissions` table, we also remove where the `selftext` has been removed or deleted. The resulting rows are 977,181 for the `submissions` table and 70,594,314 for the `comments` table. 

The count of posts per each subreddit is detailed below:

```{python}
#| echo: false

df = pd.read_csv("../../data/eda-data/subreddit-counts.csv")
# format
df = df.rename(columns={"subreddit": "Subreddit", "count": "Count"})
df = df.sort_values(by="Count", ascending=False)
df['Count'] = df['Count'].astype(int)
df['Count'] = df['Count'].apply(lambda x: f"{x:,}")

# show
md = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)

```




The plot below shows the count of valid comments per Subreddit:

```{python}
#| echo: false

width_percentage = "100%"
IFrame(src='../img/eda-plots/validity-counts.html', width=width_percentage, height=500)


```




::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-predicting-subreddits.ipynb).
::: 



## Dummies

To gauge the overall engagement in the posts, we used regex to create dummy variables that indicate whether the words 'fascinating,' 'entertaining,' and 'boring' appear on a post. We then aggregated them, with the results shown in the following table:


```{python}
#| echo: false

# load
fascinating = pd.read_csv("../../data/eda-data/keyword_counts_fascinating.csv", header=None)
entertaining = pd.read_csv("../../data/eda-data/keyword_counts_entertaining.csv", header=None)
boring = pd.read_csv("../../data/eda-data/keyword_counts_boring.csv", header=None)

# use first col as key
fascinating.columns = ['count', 'fascinating']
entertaining.columns = ['count', 'entertaining']
boring.columns = ['count', 'boring']

# convert count to string
fascinating['count'] = fascinating['count'].astype(str)
entertaining['count'] = entertaining['count'].astype(str)
boring['count'] = boring['count'].astype(str)

# merge
combined = fascinating.merge(entertaining, on='count').merge(boring, on='count')

# format
combined['fascinating'] = combined['fascinating'].apply(lambda x: "{:,}".format(int(x)))
combined['entertaining'] = combined['entertaining'].apply(lambda x: "{:,}".format(int(x)))
combined['boring'] = combined['boring'].apply(lambda x: "{:,}".format(int(x)))


md = tabulate(combined, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)



```



::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-exploratory.ipynb).
::: 



## The relationship between the number of comments and the score of Reddit posts

The number of comments (`num_comments`) and the score of a post (`score`), which is the upvotes minus the downvotes the post has received, are ways to gauge engagement with the post. Determining whether these variables are correlated can justify their combination into an aggregate engagement metric. To do this, we group the submissions by subreddit and leverage the `corr` function from `pyspark.sql.functions`. The table below shows the results:


```{python}
#| echo: false

df = pd.read_csv("../../data/eda-data/correlation_by_subreddit.csv")
# format
df = df.rename(columns={"subreddit": "Subreddit", "correlation_coefficient": "Correlation Coefficient"})
df = df.sort_values(by="Correlation Coefficient", ascending=False)
df['Correlation Coefficient'] = df['Correlation Coefficient'].round(2)

# show
md = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)

```




We can also visualize them separately as in the figure below:

![](../img/eda-plots/mean-score-comments-by-subreddit.png)



We can see in both the table and figure that they are significantly correlated, which leads to the creation of the `interaction_score` additional variable. This metric is an equal-weighted average of the number of comments and the score in each post.


::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-exploratory.ipynb).
::: 




## The impact of not-safe-for-work (NSFW) content on user engagement.


To determine if not safe for work post affects user interactions, first, we filter the `submissions` dataset for where the `over_18` flag is true (which in this tiny percentage of them). Then, we randomly sample an equal amount of false cases, and with this, we create a small, balanced dataset with the same amount of posts flagged as NSFW as those that are not. 

We can create a boxplot with this small dataset to see the distribution. Since we know from Topic 1 that the `interaction_score` is a good gauge of overall interaction, we can plot that variable as shown below:

![](../img/eda-plots/boxplot-comments-and-score-by-over-18-status.png)


::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-exploratory.ipynb).
::: 


## The times of the day when posts typically receive the most engagement.

To determine the times of day when a post typically receives the most engagement, we create two additional variables: ' week_of_the_year` and `hour_of_the_day`, both coming from the `created_utc` column. We remove the first two days of 2022 as these would be considered part of week 53 of 2021. Then, we can group and pivot the count of our new variables, resulting in the plot below:

![](../img/eda-plots/average-comments-hour-and-week-data.png)


This analysis clearly shows from roughly 6:00 AM to 11:00 AM UTC (or 1:00 AM to 6:00 AM Eastern time) is low on activity in the story time subreddits.



::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-exploratory.ipynb).
::: 



## Subreddit community prediction


```{python}
#| echo: false

width_percentage = "100%"
IFrame(src='../img/eda-plots/counts-over-time.html', width=width_percentage, height=700)


```


::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-predicting-subreddits.ipynb).
::: 



## Flair prediction

The number of posts that are flaired as each of the flairs in the r/AITA is show below:

![](../img/eda-plots/AITA-flairs-barchart-2022.png)


The treemap below shows the relative proportion of them:

![](../img/eda-plots/AITA-flairs-treemap-2022.png)





::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-eda/project-eda-aita-texts.ipynb).
::: 


