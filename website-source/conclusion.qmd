# Conclusion



Furthermore, we performed a deep dive into the subreddit `r/AmItheAsshole` with a particular focus on the flairs assigned to each post. We focused on four “primary flairs,” which were “Asshole,” “Not the A-hole,” “Everyone Sucks,” and “No A-holes here,” and performed exploratory analysis, natural language processing, sentiment analysis, and applied machine learning models. Through these analyses, we concluded that it is very difficult to accurately predict how Redditors will judge these stories according to these flairs. It is apparent that the overall manner in which Redditors judge one another may be more complex than purely the content of their posts. Additionally we found that the more positive the text of a post is, the more engagement it seemingly receives from other Reddit users, at least for this particular subreddit.

Next, we prepared a dataset for training a Recurrent Neural Network (RNN) to generate stories. We combined high-scoring Reddit submissions with texts from classic literature like "The Scarlet Letter" and "The Odyssey." The Reddit data was carefully curated for quality, and we applied natural language processing techniques like tokenization to prepare both sources for analysis. We then built and trained an RNN model using tools like PySpark, PyTorch, and CUDA. We implemented procedures to set the hyperparameters for better model training and avoiding overfitting. Our model showed strong predictive performance, indicated by low loss and perplexity values. Although the generated stories aren't entirely cohesive yet, the model successfully forms coherent words, demonstrating a grasp of basic linguistic structures and marking a significant step towards more complex story generation.



## Future Work

Further analysis related to this subreddit and how Redditors judge others could explore other types of models, other predictors, and other subreddits that also involve a similar dynamic between posters and responders.

Also in future work, we plan to employ more advanced techniques for text generation, mainly focusing on transformer models. These have shown superior performance in generating more coherent and diverse text. While our RNN understood basic grammar structures, it fell short of generating the stories we were expecting. Transformers are better at handling long-range dependencies, making them ideal for complex tasks like story generation. By incorporating transformers, we anticipate a significant enhancement in the model's ability to create stories that are linguistically correct and are closer to human-like storytelling.


