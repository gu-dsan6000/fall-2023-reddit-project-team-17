```{python}
#| echo: false

import pandas as pd
from tabulate import tabulate
import IPython.display as d
from IPython.display import display, HTML, IFrame
from IPython.display import Markdown


```

# ML

## Subreddit Prediction



![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/random-forest-cv-train-cm.png){#fig-random-forest-training width=70%}

![Shows the confusion matrix for the Random Forest, evaluated on the testing data.](img/ml-plots/random-forest-cv-test-cm.png){#fig-random-forest-testing width=70%}

```{python}
#| echo: false
#| label: fig-random-forest-eval
#| fig-cap: "Shows the evaluation metrics for the Random Forest Classifier."


width_percentage = "100%"
IFrame(src='img/ml-plots/random-forest-eval.html', width=width_percentage, height=500)


```


## Flair Prediction Using Random Forest Classification

In this section we attempted to predict what flair is assigned to posts in r/AmItheAsshole (r/AITA) based on various different predictors using a Random Forest (RF) model to attempt to predict how Redditors “judge” these stories posted on r/AITA. We then compared these models with their varying predictors and compared them to a baseline model (random chance).

The first RF model we applied to the r/AITA data using token counts of the five hundred most common words which were extracted using CountVectorizer in the NLP section previously. We chose fifty trees as our hyperparameter to be used across all models used in this section to allow for consistent comparisons. However, due to the imbalanced nature of the r/AITA posts (as established in the EDA section of this project), the dataset was downsampled so that none of the four primary flairs (Asshole, Not the A-hole, Everyone Sucks, No A-holes here) are overrepresented to the extent that they would significantly hinder model performance. After a training and testing data split, the model was trained on the training subset and various model metrics were calculated for both the training and testing subsets via a SparkML pipeline. Measures and visualizations of this model’s efficacy are displayed below.

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/flair-text-cm-train-plot.png){#fig-flair-text-train width=70%}

![Shows the confusion matrix for the Random Forest, evaluated on the testing data.](img/ml-plots/flair-text-cm-test-plot.png){#fig-flair-text-test width=70%}

![Shows the evalutation metrics for the Random Forest.](img/ml-plots/flair-text-model-metrics-plot.png){#fig-flair-text-metrics width=70%}

Based on the metrics above, this model did not predict the flairs of these posts particularly accurately, but performed better than a blind random guess, which would have a theoretical accuracy of 25% compared to our model’s ~30-40%. For both the training and testing subsets, the model did an extremely poor job at predicting “Not the A-hole posts”. This model does perform reasonably well when predicting “Everyone Sucks” and “Asshole” flaired posts, but does not predict the posts with the more positively connoted flairs (No A-holes here and Not the A-hole).

Another potential set of predictors we identified were measures of user engagement, namely post “score” (number of upvotes minus number of downvotes) and the number of comments under a post. We applied a similar model using these predictors via another SparkML pipeline and compared them to the previous text-based model. The measures of model performance and confusion matrices are visualized below as such.

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/flair-engagement-cm-train-plot.png){#fig-flair-engagement-train width=70%}

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/flair-engagement-cm-test-plot.png){#fig-flair-engagement-test width=70%}

![Shows the evaluation metrics for the Random Forest.](img/ml-plots/flair-engagement-model-metrics-plot.png){#fig-flair-engagement-metrics width=70%}

As shown above, this model performs fairly similarly to the previous text-based model but with some slight improvements in some of the model performance metrics along with more comparative performance metrics of the model for the training and test sets. This model more effectively predicted posts with the flairs “Asshole” and “Everyone Sucks”  compared to the text-based model, but similarly struggled to correctly identify posts with more positive flairs. Ultimately, while this model does perform slightly better than the previous text-based model, especially at predicting the posts with more negative flairs attached, it still would not serve as an effective tool for accurately predicting these flairs on a larger scale. It is possible that these data are too homogeneous to be easily differentiated using a machine learning model, or using different models and/or hyperparameters may generate more accurate predictions.

We also include the evaluation metrics in @tbl-flair-model-metrics-df below:


```{python}
#| echo: false
#| label: tbl-flair-model-metrics-df
#| tbl-cap-location: bottom
#| tbl-cap: Displays the evaluation metrics of both engagement and text-based models.


df_1 = pd.read_csv("../data/ml-data/flair-engagement-model-metrics-df.csv")
		
df_1 = df_1.rename(columns={"Unnamed: 0": "Metric", "training": "Engagement Training", "test": "Engagement Test"})


df_2 = pd.read_csv("../data/ml-data/flair-text-model-metrics-df.csv")

df_2 = df_2.rename(columns={"Unnamed: 0": "Metric Text", "training": "Text Training", "test": "Text Test"})

joined_df = pd.concat([df_1, df_2], axis=1)

joined_df = joined_df.drop('Metric Text', axis=1)


# show
md = tabulate(joined_df, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)

```







## Story Generation

In our project, we developed a story generation model using a recurrent neural network (RNN) model, which we built and trained using `PySpark` and `PyTorch`. To enhance the computation speed, we integrated CUDA into the process. Our data is the mix of top stories from various subreddits and popular books sourced externally, developed during the NLP portion of the project. The preprocessed portion of the dataset usable for training purposes is 1037 MB. We found that a smaller subset of 160.59 MB was adequate for the analysis.

The model's architecture is based on a Long Short-Term Memory (LSTM) layer, which captures long-term dependencies in sequential data. In our setup, we defined hyperparameters such as the input size of 128, hidden state at 256, and used a two-layer LSTM. The model has 12,766,510 trainable parameters.

For the training process, we set the learning rate at 0.01, a maximum of 100 epochs, and a batch size of 64. To ensure the model didn't overfit, we employed an early stopping mechanism with a patience of 5 epochs and a validation loss improvement threshold of 0.01. The Adam optimizer was chosen for optimization, paired with a cross-entropy loss function for calculating the model's error rate. The total training time was 1h 41 mins.

The training and validation perplexities are shown in @fig-rnn-training-validation-perplexity:

![Shows the training and validation perplexities of the RNN training process](img/ml-plots/training-validation-perplexity-plot-rnn.png){#fig-rnn-training-validation-perplexity}

To evaluate the model's effectiveness, we focused on loss and perplexity. A lower perplexity value suggests a higher predictive accuracy of the model. The model achieved a test loss of 1.6227 and a perplexity value of 5.0667, indicating a strong performance in predictive capabilities.

We were ready to generate stories with the trained. Some examples are shown in @tbl-rnn-generation:

| Prompt      | Generated Text |
| ----------- | ----------- |
| Once upon a time | Once upon a time to of a cause the gachen friends these any olden lands. it was the school all as the sach could boypheal, and they letes saying cut that ended about the told the asking.the said she so a kyastellow specially will wrong me have a glories and how in s |
| The sun set over the ancient, whispering forest   | The sun set over the ancient forest and the sating the told the thought the pressated the hassed has all she was hands and the and the and of didnt conday her that there you hands my like i was the and i was the told when and the put all into the done to the down the sately and stouse |
| The sound of sirens pierced the night | The sound of sirens pierced the night stack and the down, my fear that my bly and were expetes his slaring when it becheads icky that my feelt and want and contores she givated hours. we let me processings that i only and low that on a mord of the past finding to this because oulling th |

: Shows the prompt and generated text by the RNN model {#tbl-rnn-generation}



Although our model cannot yet generate cohesive stories, it's important to recognize the success in the underlying process. The characters produced by the model consistently combine to form coherent words, an ability that demonstrates the model's understanding of basic linguistic structures, a foundational step toward more complex story generation. This aspect of the model's output aligns well with the objectives of our project.


