```{python}
#| echo: false

import pandas as pd
from tabulate import tabulate
import IPython.display as d
from IPython.display import display, HTML, IFrame
from IPython.display import Markdown


```

# ML

## Executive Summary

This section focuses on Machine Learning (ML) methods to answer our research questions. In particular, our first two goals for this section involve identifying characteristics of a Reddit post, while our last two goals are more text-based, involving Reddit post generation and summarization.

First, we focus on predicting which subreddit a post belongs to based on its text. We provide a Random Forest model for this classification task, as well as a baseline model for comparison. In doing so, we find significant improvement from the baseline model to the Random Forest model, with areas of improvement still to address.

Subsequently, we attempt to predict the flairs of posts in `r/AmItheA\*hole` (`r/AITA`) using various possible predictors. We apply two Random Forest models using the text of posts in `r/AITA` and the user engagement on these posts, respectively. Similar to our first classification task, we find that these models perform better than a blind guess or baseline, but still have a lot of room for improvement. 

Then, we aim to create a model that can generate stories for new Reddit posts. We train a recurrent neural network (RNN) with a Long Short-Term Memory architecture on text from a mix of top stories from various subreddits and popular books. While the model successfully generates text that demonstrates an understanding of basic linguistic structures, it has yet to produce fully cohesive stories, marking a significant step towards more complex narrative generation.

Lastly, we use two pre-trained models, one for summarization and one for sentiment analysis, to understand topics selected in the natural language processing portion of our project within the `r/NoStupidQuestions` subreddit. With the summarization model, we identify popular comments of interest and are able to greatly reduce the amount of text while keeping the meaning of the comment. Additionally, the sentiment analysis model is able to effectively classify the sentiment of the comments in a subreddit, furthering our understanding of the discourse in the `r/NoStupidQuestions` subreddit.

Overall, these studies highlight the challenges and progress in using ML for subreddit content analysis, demonstrating advancements from simple probabilistic approaches to more sophisticated models like Random Forest and RNNs. The models have varying degrees of success, suggesting room for further improvement in these areas.

## Analysis Report

### Subreddit Prediction

For our subreddit prediction task, we aim to take only the textual content of a post and classify the subreddit to which that post belongs. By using the 500 most common words across all posts, we hope to obtain important textual information that helps us determine which subreddit a post belongs to. For instance, the word “relationship” may be much more likely to appear in the `r/relationship_advice` subreddit than any others.

#### Baseline Model

Before we dive into complex Machine Learning models, though, we start with a baseline model. The baseline model provides us with a point of comparison for our more complex Machine Learning models, allowing us to evaluate the performance of those models in comparison to the simple baseline.
Our baseline model is very naïve - it simply predicts subreddits with probability equal to the proportion with which they make up the training dataset. For instance, if `50%` of our training dataset contains observations from `r/relationship_advice`, `30%` from `r/NoStupidQuestions`, and `20%` from `r/TrueOffMyChest`, our baseline model will predict that a post belongs to the subreddit `r/relationship_advice` with probability `0.50`, the subreddit `r/NoStupidQuestions` with probability `0.30`, and the subreddit `r/TrueOffMyChest` with probability `0.20`. In this case, of course, the model has 12 subreddits to choose from, each with their own associated probabilities. Note that the baseline model does not consider additional information, such as the textual content of the post, because it is so simple.

Below, in @fig-baseline-training, we can see the results of the model predictions on the training data. As expected, the model predicts the more prevalent subreddits, such as `r/relationship_advice` and `r/NoStupidQuestions`, more often. Since the model only predicts subreddits proportionally to how they appear in the training data, it does not do a good job of actually identifying these subreddits correctly. In fact, its expected classification accuracy is equal to the sum of the squares of the probabilities with which each subreddit occurs, which amounts to approximately 0.20.

![Shows the confusion matrix for the Baseline Model, evaluated on the training data.](img/ml-plots/baseline-cv-train-cm.png){#fig-baseline-training width=70%}

Below, in @fig-baseline-testing, we can see the results of the model predictions on the testing data. As expected, the model’s performance on the testing set is no different than on the training set. We find the same characteristics, with the model performing quite poorly.

![Shows the confusion matrix for the Baseline Model, evaluated on the testing data.](img/ml-plots/baseline-cv-test-cm.png){#fig-baseline-testing width=70%}

Below, in @fig-baseline-eval, is a summary table of the performance metrics for the baseline model on both the training and testing sets. Each metric reports values of approximately 0.20, which is to be expected with such a simple model. We look to improve this performance with a Random Forest model below.

```{python}
#| echo: false
#| label: fig-baseline-eval
#| fig-cap: "Shows the evaluation metrics for the Baseline Model Classifier."


width_percentage = "100%"
IFrame(src='img/ml-plots/baseline-eval.html', width=width_percentage, height=500)


```

#### Random Forest Model

In order to predict the subreddit to which a post belongs more effectively, we turn to the Random Forest. <add RF explanation here>.

Below, in @fig-random-forest-training, we can see the results of the model predictions on the training data. Here, we find that the performance of the model seems to outperform that of the baseline model, but still has its downfalls. The model predicts the two most prevalent subreddits, `r/relationship_advice` and `r/NoStupidQuestions`, nearly every time, failing to predict any of the less prevalent subreddits. In this sense, the model is vastly underperforming, as it is heavily biased to the subreddits that it has seen more often.

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/random-forest-cv-train-cm.png){#fig-random-forest-training width=70%}

Below, in @fig-random-forest-testing, we can see the results of the model predictions on the testing data. As expected, the model’s performance on the testing set is no different than on the training set. We find the same characteristics, with the model performing well in some aspects, but with plenty of room for improvement.

![Shows the confusion matrix for the Random Forest, evaluated on the testing data.](img/ml-plots/random-forest-cv-test-cm.png){#fig-random-forest-testing width=70%}

Below, in @fig-random-forest-eval, is a summary table of the performance metrics for the Random Forest model on both the training and testing sets. Each metric reports values of approximately 0.50, which is a great improvement over the baseline model. However, we feel that further improvements can be made in order to achieve higher performance on our subreddit prediction task.

```{python}
#| echo: false
#| label: fig-random-forest-eval
#| fig-cap: "Shows the evaluation metrics for the Random Forest Classifier."


width_percentage = "100%"
IFrame(src='img/ml-plots/random-forest-eval.html', width=width_percentage, height=500)


```

::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-ml/project-ml-predicting-subreddits.ipynb).
::: 

### Flair Prediction Using Random Forest Classification

In this section we attempted to predict what flair is assigned to posts in r/AmItheAsshole (r/AITA) based on various different predictors using a Random Forest (RF) model to attempt to predict how Redditors “judge” these stories posted on r/AITA. We then compared these models with their varying predictors and compared them to a baseline model (random chance).

The first RF model we applied to the r/AITA data using token counts of the five hundred most common words which were extracted using CountVectorizer in the NLP section previously. We chose fifty trees as our hyperparameter to be used across all models used in this section to allow for consistent comparisons. However, due to the imbalanced nature of the r/AITA posts (as established in the EDA section of this project), the dataset was downsampled so that none of the four primary flairs (Asshole, Not the A-hole, Everyone Sucks, No A-holes here) are overrepresented to the extent that they would significantly hinder model performance. After a training and testing data split, the model was trained on the training subset and various model metrics were calculated for both the training and testing subsets via a SparkML pipeline. Measures and visualizations of this model’s efficacy are displayed below.

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/flair-text-cm-train-plot.png){#fig-flair-text-train width=70%}

![Shows the confusion matrix for the Random Forest, evaluated on the testing data.](img/ml-plots/flair-text-cm-test-plot.png){#fig-flair-text-test width=70%}

![Shows the evalutation metrics for the Random Forest.](img/ml-plots/flair-text-model-metrics-plot.png){#fig-flair-text-metrics width=70%}

Based on the metrics above, this model did not predict the flairs of these posts particularly accurately, but performed better than a blind random guess, which would have a theoretical accuracy of 25% compared to our model’s ~30-40%. For both the training and testing subsets, the model did an extremely poor job at predicting “Not the A-hole posts”. This model does perform reasonably well when predicting “Everyone Sucks” and “Asshole” flaired posts, but does not predict the posts with the more positively connoted flairs (No A-holes here and Not the A-hole).

Another potential set of predictors we identified were measures of user engagement, namely post “score” (number of upvotes minus number of downvotes) and the number of comments under a post. We applied a similar model using these predictors via another SparkML pipeline and compared them to the previous text-based model. The measures of model performance and confusion matrices are visualized below as such.

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/flair-engagement-cm-train-plot.png){#fig-flair-engagement-train width=70%}

![Shows the confusion matrix for the Random Forest, evaluated on the training data.](img/ml-plots/flair-engagement-cm-test-plot.png){#fig-flair-engagement-test width=70%}

![Shows the evaluation metrics for the Random Forest.](img/ml-plots/flair-engagement-model-metrics-plot.png){#fig-flair-engagement-metrics width=70%}

As shown above, this model performs fairly similarly to the previous text-based model but with some slight improvements in some of the model performance metrics along with more comparative performance metrics of the model for the training and test sets. This model more effectively predicted posts with the flairs “Asshole” and “Everyone Sucks”  compared to the text-based model, but similarly struggled to correctly identify posts with more positive flairs. Ultimately, while this model does perform slightly better than the previous text-based model, especially at predicting the posts with more negative flairs attached, it still would not serve as an effective tool for accurately predicting these flairs on a larger scale. It is possible that these data are too homogeneous to be easily differentiated using a machine learning model, or using different models and/or hyperparameters may generate more accurate predictions.

We also include the evaluation metrics in @tbl-flair-model-metrics-df below:


```{python}
#| echo: false
#| label: tbl-flair-model-metrics-df
#| tbl-cap-location: bottom
#| tbl-cap: Displays the evaluation metrics of both engagement and text-based models.


df_1 = pd.read_csv("../data/ml-data/flair-engagement-model-metrics-df.csv")
		
df_1 = df_1.rename(columns={"Unnamed: 0": "Metric", "training": "Engagement Training", "test": "Engagement Test"})


df_2 = pd.read_csv("../data/ml-data/flair-text-model-metrics-df.csv")

df_2 = df_2.rename(columns={"Unnamed: 0": "Metric Text", "training": "Text Training", "test": "Text Test"})

joined_df = pd.concat([df_1, df_2], axis=1)

joined_df = joined_df.drop('Metric Text', axis=1)


# show
md = tabulate(joined_df, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)

```

::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-ml/project-ml-aita-flair-prediction.ipynb).
:::

### Story Generation

::: {.callout-note appearance="tip"}
The saved model used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/data/ml-data/rnn_model_250_0.15.pth).
:::

In our project, we developed a story generation model using a recurrent neural network (RNN) model, which we built and trained using `PySpark` and `PyTorch`. To enhance the computation speed, we integrated CUDA into the process. Our data is the mix of top stories from various subreddits and popular books sourced externally, developed during the NLP portion of the project. The preprocessed portion of the dataset usable for training purposes is 1037 MB. We found that a smaller subset of 160.59 MB was adequate for the analysis.

The model's architecture is based on a Long Short-Term Memory (LSTM) layer, which captures long-term dependencies in sequential data. In our setup, we defined hyperparameters such as the input size of 128, hidden state at 256, and used a two-layer LSTM. The model has 12,766,510 trainable parameters.

For the training process, we set the learning rate at 0.01, a maximum of 100 epochs, and a batch size of 64. To ensure the model didn't overfit, we employed an early stopping mechanism with a patience of 5 epochs and a validation loss improvement threshold of 0.01. The Adam optimizer was chosen for optimization, paired with a cross-entropy loss function for calculating the model's error rate. The total training time was 1h 41 mins.

The training and validation perplexities are shown in @fig-rnn-training-validation-perplexity:

![Shows the training and validation perplexities of the RNN training process](img/ml-plots/training-validation-perplexity-plot-rnn.png){#fig-rnn-training-validation-perplexity}

To evaluate the model's effectiveness, we focused on loss and perplexity. A lower perplexity value suggests a higher predictive accuracy of the model. The model achieved a test loss of 1.6227 and a perplexity value of 5.0667, indicating a strong performance in predictive capabilities.

We were ready to generate stories with the trained. Some examples are shown in @tbl-rnn-generation:

| Prompt      | Generated Text |
| ----------- | ----------- |
| Once upon a time | Once upon a time to of a cause the gachen friends these any olden lands. it was the school all as the sach could boypheal, and they letes saying cut that ended about the told the asking.the said she so a kyastellow specially will wrong me have a glories and how in s |
| The sun set over the ancient, whispering forest   | The sun set over the ancient forest and the sating the told the thought the pressated the hassed has all she was hands and the and the and of didnt conday her that there you hands my like i was the and i was the told when and the put all into the done to the down the sately and stouse |
| The sound of sirens pierced the night | The sound of sirens pierced the night stack and the down, my fear that my bly and were expetes his slaring when it becheads icky that my feelt and want and contores she givated hours. we let me processings that i only and low that on a mord of the past finding to this because oulling th |

: Shows the prompt and generated text by the RNN model {#tbl-rnn-generation}



Although our model cannot yet generate cohesive stories, it's important to recognize the success in the underlying process. The characters produced by the model consistently combine to form coherent words, an ability that demonstrates the model's understanding of basic linguistic structures, a foundational step toward more complex story generation. This aspect of the model's output aligns well with the objectives of our project.

::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-ml/project-nlp-posts-and-books-model.ipynb).
:::

### ML Models in `r/NoStupidQuestions`

#### Top Comment Summary Generation 


After all of the NLP work to identify comments that were scored high and contained the topic we had chosen, in this case comments related to COVID-19 in the subreddit `r/NoStupidQuestions`. We were successfully able to identify comments that we would be useful to summarize to better gain an understanding of the narrative in a particular subreddit. One example of this is below. 

The model used to perform this summarization was a “facebook/bart-large-cnn” [@lewis_bart_2019].


##### Before Summarization (After Stemming and removing StopWords):

> I can understand that frustration and getting tired of the cynicism or generation of toxic beliefs, but it should be noted that about that bit from his 1999 special about the immune system and all that, his family has outright said:
>
> Several times during the pandemic, Carlin has drawn attention for a routine from his 1999 special, “You Are All Diseased,” in which he mischievously suggests that a childhood spent swimming in the polluted Hudson River was the reason he didn’t catch polio.
>
> (“In my neighborhood, no one ever got polio,” he fulminates. “No one, ever. You know why? ’Cause we swam in raw sewage. It strengthened our immune systems. The polio never had a prayer.”)
>
> As Kelly Carlin explained, some viewers concluded — wrongly — that her father would have opposed coronavirus vaccines.
>
> “Everyone’s like, see? George Carlin would have been anti-vaccination,” she said. “And I’m like, no. My dad was pro-science, pro-rational thinking, pro-evidence-based medicine. The man was a heart patient for 30 years. When he was a kid and the polio vaccine became available, he got the polio vaccine.”
>
> ...
>
> In efforts to divine his opinion, some Carlin fans pointed to a 1990 interview he gave to Larry King, when he expressed his misgivings about the crude standup of Andrew Dice Clay: “His targets are underdogs, and comedy has traditionally picked on power — people who abuse their power,” Carlin said at the time.
>
> Kelly Carlin said her father “always took the stand that more speech is better than less speech” and would have supported Chappelle’s right to perform the special. But, she added, “if you’re a comedian, you’ve got to be funny.”



##### After Summarization:

> Carlin has drawn attention for a routine from his 1999 special, “You Are All Diseased.” He mischievously suggests that a childhood spent swimming in the polluted Hudson River was the reason he didn’t catch polio. Some viewers concluded — wrongly — that her father would have opposed coronavirus vaccines. “Everyone’s like, see? George Carlin would have been anti-vaccination,” she said. ‘I’m like, no. My dad was pro-science,. pro-rational thinking, pro-evidence-based medicine.



#### Sentiment Analysis in `r/NoStupidQuestions`

Another topic that we needed to understand in relation to the COVID-19 comments in `r/NoStupidQuestions` was the sentiment. To do this we used “yiyanghkust/finbert-tone” [@huang_span_2023] on HuggingFace. This gave us a better understanding of the subreddit with regard to our topic. The results of which are visualized in @fig-sentiment-analysis-plot. Additionally, this is just a small subset to show that the model worked.


![Shows the result of the sentiment analysis using the pre-trained model.](img/ml-plots/sentiment_analysis.png){#fig-sentiment-analysis-plot}

::: {.callout-note appearance="simple"}
The code used for this section is available [here](https://github.com/gu-dsan6000/fall-2023-reddit-project-team-17/blob/main/code/project-ml/topic_summarization.ipynb).
:::

