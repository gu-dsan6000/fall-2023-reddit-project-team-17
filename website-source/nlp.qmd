```{python}
#| echo: false

import pandas as pd
from tabulate import tabulate
import IPython.display as d
from IPython.display import display, HTML, IFrame
from IPython.display import Markdown


```



# NLP


## Executive summary






## Flair Sentiment Model


In this section we explored the textual content of the posts of r/AmItheAsshole (r/AITA) with respect to their assigned flairs and subsequently applying a pretrained sentiment model. Using a sparkNLP pipeline, all text posts from r/AITA in 2022 with one of the four primary flairs (Asshole, Not the A-hole, Everybody Sucks, No A-holes Here) attached were processed/cleaned and run through a pretrained sentiment model. The sentiment model most commonly assigned these posts a "negative" sentiment, as shown in @fig-flair-sentiments-barchart. This holds true across posts assigned any of the four primary flairs.

![Shows the subreddit sentiment by flair.](img/nlp-plots/flair_sentiments_barchart.png){#fig-flair-sentiments-barchart}


To further delve into the sentiments of these posts in r/AITA, we analyzed these sentiment assignments with respect to the engagement a post receives, represented by the number of comments under each posts. In the  @fig-flair-engagements-barchart, the mean number of comments per post grouped by flair assignment and sentiment assignment.

![Shows the subreddit engagement by flair.](img/nlp-plots/flair_engagements_barchart.png){#fig-flair-engagements-barchart}

From this plot above, we can extract several conclusions. We can see that posts assigned the "Asshole" flair receive the most user engagement (on average) and posts assigned "No A-holes here" receive the least engagement, on average. Additionally, it is apparent that posts assigned a "negative" sentiment receive less user engagement than those with a "positive" sentiment across all four primary flairs. Thus, it is possible there could be a relationship wherein the more "positive" a post's sentiment/writing is, the more likely it is to receive more engagement (at least in the form of number of comments).



```{python}
#| echo: false
#| label: tbl-aita-sentiment-engagements
#| tbl-cap-location: bottom
#| tbl-cap: Displays the r/AITA subreddit sentiment engagements.


df = pd.read_parquet("../data/nlp-data/aita_sentiments_engagements.parquet")

# format
df = df.rename(columns={"negative": "Negative", "neutral": "Neutral", "positive": "Positive"})
df = df.rename_axis("Flair")

#df['Count'] = df['Count'].astype(int)
df['Negative'] = df['Negative'].round(2)
df['Neutral'] = df['Neutral'].round(2)
df['Positive'] = df['Positive'].round(2)


# show
md = tabulate(df, headers='keys', tablefmt='pipe', showindex=True)
Markdown(md)

```







## Preparing Reddit data for multi-class classification

See in @fig-sentiment-age.

![Shows how age is distributed across sentiment](img/nlp-plots/predicting-subreddits-density-age.png){#fig-sentiment-age}


See in @fig-sentiment-gender.

![Shows how gender is distributed across sentiment](img/nlp-plots/predicting-subreddits-density-gender.png){#fig-sentiment-gender}


See in @tbl-gender-table.


```{python}
#| echo: false
#| label: tbl-gender-table
#| tbl-cap-location: bottom
#| tbl-cap: Displays the distribution of sentiment across genders.


df = pd.read_csv("../data/nlp-data/predicting-subreddits-gender-table.csv")

# format
df = df.rename(columns={"f": "Female", "m": "Male", "other": "Other"})
#df['Count'] = df['Count'].astype(int)
df['Female'] = df['Female'].round(2)
df['Male'] = df['Male'].round(2)
df['Other'] = df['Other'].round(2)


# show
md = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)

```






## Preparing Covid-19 data in `NoStupidQuestions`


See in @fig-covid-nsq.

![Shows how gender is distributed across sentiment](img/nlp-plots/covid-nsq.png){#fig-covid-nsq}





## Preparing Reddit and External Data for Training an RNN 

For this exercise, we focused on preparing text that contains great storytelling to train a Recurrent Neural Network (RNN) that can generate new stories. We use our 12 months of Reddit submissions data described in the EDA section for the analysis. Additionally, we integrate external data containing the text of famous stories from Project Gutenberg books that have stood the test of time, specifically The Scarlet Letter by Nathaniel Hawthorne, The Odyssey by Homer, Crime and Punishment by Fyodor Dostoyevsky, Metamorphosis by Franz Kafka, and The Great Gatsby by F. Scott Fitzgerald.

To prepare the Reddit data, we extracted only the relevant information from the parquets, such as `subreddit`, `title`, `selftext`, `score`, and `URL`, and filtered out deleted or empty submissions. To select the best stories, we used a regular expressions pattern to remove any "Edit:" sections to remove post-edit additions that could skew the analysis. Since stories must be at least a few paragraphs, we removed all posts that didn't have at least 4500 characters (around 750 words). Then, we filtered for only stored with `score` in the top 85th percentile, thereby focusing on submissions that garnered significant user interaction.

We then combined the text sources. The data underwent a series of NLP transformations, including custom tokenization and lowercasing, to prepare it for advanced analysis. We constructed a vocabulary and transformed the individual characters into tokens. The resulting frequency of each token is shown in @fig-token-frequency-plot. Lastly, we stored the processed data in a structured Parquet format alongside the character-to-index mappings, crucial for the subsequent machine-learning modeling. 


![Shows the count of the Top 10 tokens](img/nlp-plots/books-and-stories-token-frequency-plot.png){#fig-token-frequency-plot}


As an additionally way to visualize the resulting dataset, we also can see the results of the top 10 N-Grams, where $N=5$ in @tbl-n-gram-results.

```{python}
#| echo: false
#| label: tbl-n-gram-results
#| tbl-cap-location: bottom
#| tbl-cap: displays the top 10 most frequent N-Grams where $N=5$.



# comma separate
def format_count(count):
    return f"{count:,}"

df = pd.read_csv("../data/nlp-data/books-and-stories-n-grams.csv")

# format
df = df.rename(columns={"ngram": "5-Gram (incl spaces)", "count": "Count"})
df['Count'] = df['Count'].apply(format_count)

# show
md = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)
Markdown(md)

```


