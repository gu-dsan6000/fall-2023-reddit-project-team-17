{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing large datasets with Apache Spark and Amazon SageMaker\n",
    "\n",
    "***This notebook run on `Data Science 3.0 - Python 3` kernel on a `ml.t3.large` instance***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing Jobs are used  to analyze data and evaluate machine learning models on Amazon SageMaker. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/Processing-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding diagram shows how Amazon SageMaker spins up a Processing job. Amazon SageMaker takes your script, copies your data from Amazon Simple Storage Service (Amazon S3), and then pulls a processing container. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide. The underlying infrastructure for a Processing job is fully managed by Amazon SageMaker. Cluster resources are provisioned for the duration of your job, and cleaned up when a job completes. The output of the Processing job is stored in the Amazon S3 bucket you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our workflow for processing large amounts of data with SageMaker\n",
    "\n",
    "We can divide our workflow into two steps:\n",
    "    \n",
    "1. Work with a small subset of the data with Spark running in local model in a SageMaker Studio Notebook.\n",
    "\n",
    "1. Once we are able to work with the small subset of data we can provide the same code (as a Python script rather than a series of interactive steps) to SageMaker Processing which launched a Spark cluster, runs out code and terminates the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook...\n",
    "\n",
    "We will process the [News Summarization](https://www.kaggle.com/datasets/sbhatti/news-summarization) dataset on Kaggle. The following data is intended for advancing news summarization research. It's three datasets (XSum, CNN/Daily Mail, Multi-News) combined into one easy-to-use CSV file, we have converted this to Parquet format for this lab.\n",
    "\n",
    "⚠️We have converted the CSV file to Parquet for this lab, why do you think that was done?⚠️\n",
    "\n",
    "1. The data is available as two Parquet files: `s3://bigdatateaching/news/data1000.parquet` which is a small subset of data and `s3://bigdatateaching/news/data.parquet` which is the full dataset.\n",
    "\n",
    "1. We will first read a small subset `s3://bigdatateaching/news/data1000.parquet` of the data locally and do all our analysis in this notebook. We will run some analytics (the usual derive new features, group-summarize type stuff) and then also some sentiment analysts using the [spark-nlp](https://sparknlp.org/) library.\n",
    "\n",
    "1. We will then repeat the same operation on the full dataset `s3://bigdatateaching/news/data.parquet`. This dataset is about 800,000 rows. This operation is too big to be run on a `ml.t3.large` instance (2 vCPU, 8GB RAM) so we will run this on 6 machines of `ml.m5.xlarge` instance type (4 vCPUs, 16GB RAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark NLP\n",
    "\n",
    "The Spark NLP library provided by _John Snow Labs_ is available to us in Python via the [spark-nlp==5.1.3](https://pypi.org/project/spark-nlp/) Python package and the [Spark NLP Assembly Jar spark-nlp-assembly-5.1.3.jar](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-5.1.3.jar) Java Archive (JAR) file, read mode [here](https://sparknlp.org/docs/en/install) (BOTH the Python package and Java archive are required to use Spark NLP).\n",
    "\n",
    "Also see these compatability tables ([1](https://github.com/JohnSnowLabs/spark-nlp#apache-spark-support), [2](https://github.com/JohnSnowLabs/spark-nlp#scala-and-python-support)) for what version of Spark NLP is compatible with which all versions of Spark, Python, Java and Scala.\n",
    "\n",
    ">Why do we need a Assembly JAR file? Because the Spark NLP JAR has dependencies, all of which need to be downloaded as well to use Spark NLP, so John Snow Labs has made it convenient to download all the JAR files in one single Assembly JAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.9.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.9.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.7.22          |  py310h06a4308_0         153 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    openssl-1.1.1w             |       h7f8727e_0         3.7 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       345.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> pkgs/main::ca-certificates-2023.08.22-h06a4308_0 \n",
      "  openssl                                 1.1.1v-h7f8727e_0 --> 1.1.1w-h7f8727e_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/linux-64::certifi-2023.7.22-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2023.7.22    | 153 KB    |                                       |   0% \n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "ca-certificates-2023 | 123 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | ######8                               |  18% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | 7                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #2                                    |   4% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "openssl-1.1.1w       | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##7                                   |   7% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####3                                 |  12% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #####4                                |  15% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ######6                               |  18% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #######6                              |  21% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ########5                             |  23% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #########4                            |  26% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########4                           |  28% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###########3                          |  31% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ############4                         |  34% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #############5                        |  37% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############6                       |  40% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###############9                      |  43% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #################3                    |  47% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################5                   |  50% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################9                  |  54% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #####################5                |  58% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #######################2              |  63% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ########################7             |  67% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########################4           |  71% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ############################          |  76% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | #############################6        |  80% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ###############################3      |  85% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ################################9     |  89% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################################5   |  93% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################################1 |  98% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spark-nlp==5.1.3\n",
      "  Obtaining dependency information for spark-nlp==5.1.3 from https://files.pythonhosted.org/packages/cd/7d/bc0eca4c9ec4c9c1d9b28c42c2f07942af70980a7d912d0aceebf8db32dd/spark_nlp-5.1.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached spark_nlp-5.1.3-py2.py3-none-any.whl.metadata (53 kB)\n",
      "Using cached spark_nlp-5.1.3-py2.py3-none-any.whl (537 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.4.0\n",
    "\n",
    "# install spark-nlp\n",
    "%pip install spark-nlp==5.1.3\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://bigdatateaching/news/data1000.parquet to ./data1000.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://bigdatateaching/news/data1000.parquet . --request-payer requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "copy: s3://bigdatateaching/news/data1000.parquet to s3://sagemaker-us-east-1-038932893404/lab8/news/data1000.parquet\n",
      "copy: s3://bigdatateaching/news/data.parquet to s3://sagemaker-us-east-1-038932893404/lab8/news/data.parquet\n"
     ]
    }
   ],
   "source": [
    "## Copy the full dataset to the SageMaker bucket\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "!aws s3 cp s3://bigdatateaching/news/data1000.parquet s3://{bucket}/lab8/news/ --request-payer requester\n",
    "!aws s3 cp s3://bigdatateaching/news/data.parquet s3://{bucket}/lab8/news/ --request-payer requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 20:15:00 1617893454 data.parquet\n",
      "2023-10-15 20:14:58    2833149 data1000.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{bucket}/lab8/news/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With with the local PySpark cluster and Spark NLP\n",
    "\n",
    "Now we are going to start a local [Spark NLP session with Python](https://sparknlp.org/docs/en/install#start-spark-nlp-session-from-python). ***Do NOT forget to remove the `.master(\"local[*]\")` line when moving this code to a script that runs on a Spark cluster***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-804d719a-edab-44b8-b008-b10766fe2840;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.1.3 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.15.0 in central\n",
      ":: resolution report :: resolve 4588ms :: artifacts dl 273ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.1.3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.15.0 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-804d719a-edab-44b8-b008-b10766fe2840\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 72 already retrieved (0kB/79ms)\n",
      "23/10/15 18:44:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.4.0\n",
      "sparknlp version: 5.1.3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"sparknlp version: {sparknlp.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data into a Spark Dataframe\n",
    "Note that we will be using the \"s3a\" adapter (read more [here](https://aws.amazon.com/blogs/opensource/community-collaboration-the-s3a-story)). S3A enables Hadoop to directly read and write Amazon S3 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------+\n",
      "|                  ID|             Content|             Summary|       Dataset|\n",
      "+--------------------+--------------------+--------------------+--------------+\n",
      "|f49ee725a0360aa68...|New York police a...|Police have inves...|CNN/Daily Mail|\n",
      "|808fe317a53fbd313...|By . Ryan Lipman ...|Porn star Angela ...|CNN/Daily Mail|\n",
      "|98fd67bd343e58bc4...|This was, Sergio ...|American draws in...|CNN/Daily Mail|\n",
      "|e12b5bd7056287049...|An Ebola outbreak...|World Health Orga...|CNN/Daily Mail|\n",
      "|b83e8bcfcd5141984...|By . Associated P...|A sinkhole opened...|CNN/Daily Mail|\n",
      "|9c2b9de4b8928f63b...|Jerusalem woke up...|Two Palestinians ...|CNN/Daily Mail|\n",
      "|550c7ea14b4ec91db...|An Australian fat...|Zia Abdul Haq is ...|CNN/Daily Mail|\n",
      "|c6dbfd89aa8485511...|A mother whose pr...|Jocelyn Bennett a...|CNN/Daily Mail|\n",
      "|85fa186e116866297...|A community stalw...|Rahmat Ali Raja, ...|CNN/Daily Mail|\n",
      "|f6e79e2f206634f24...|(CNN) -- Congress...|House panel adds ...|CNN/Daily Mail|\n",
      "|661dcb04dae4747cd...|(CNN) -- He was l...|He had a cameo in...|CNN/Daily Mail|\n",
      "|c719083a7b193fa85...|When Jose Mourinh...|The FA announced ...|CNN/Daily Mail|\n",
      "|a1bee53abcb1e12b7...|(CNN) -- When it ...|India's Punjab st...|CNN/Daily Mail|\n",
      "|6c9ecd04c8b2bd960...|(CNN) -- Three De...|Police arrested T...|CNN/Daily Mail|\n",
      "|ae2ed380bd3c38a3c...|By . Tamara Cohen...|Fixed Odds Bettin...|CNN/Daily Mail|\n",
      "|8cef5db9bb699175f...|By . Cindy Tran f...|Members of a femi...|CNN/Daily Mail|\n",
      "|872dc5f31eedcb249...|Queens Park Range...|Hull and Stoke ar...|CNN/Daily Mail|\n",
      "|b8e14b9ce93a52020...|Scathing: A repor...|Sir Cliff first s...|CNN/Daily Mail|\n",
      "|c21eb9da12232628c...|Driving tests sho...|Matthew Hancock s...|CNN/Daily Mail|\n",
      "|087a492736e8ba4b1...|By . Daily Mail R...|Pamela Smith, who...|CNN/Daily Mail|\n",
      "+--------------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 6.78 ms, sys: 3.16 ms, total: 9.94 ms\n",
      "Wall time: 9.71 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fpath = \"data1000.parquet\"\n",
    "df = spark.read.parquet(fpath,\n",
    "    header=True\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Content: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Dataset: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.repartition(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics operations\n",
    "\n",
    "Let us now do a few analytics operations locally in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>    (59 + 2) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataframe is 1,000x4\n",
      "CPU times: user 5.71 ms, sys: 411 µs, total: 6.13 ms\n",
      "Wall time: 2.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"shape of the dataframe is {df.count():,}x{len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct four dummy variables using pyspark and the regex function `rlike`. You will make each dummy variable using the regex statement provided below:\n",
    "\n",
    "|dummy | regex|\n",
    "|-----------|-----------|\n",
    "|politics|**(?i)politics\\|(?i)political\\|(?i)senate\\|(?i)government\\|(?i)president\\|(?i)prime minister\\|(?i)congress**|\n",
    "|sports|**(?i)sport\\|(?i)ball\\|(?i)coach\\|(?i)goal\\|(?i)baseball\\|(?i)football\\|(?i)basketball**|\n",
    "|arts|**(?i)art\\|(?i)painting\\|(?i)artist\\|(?i)museum\\|(?i)photography\\|(?i)sculpture**|\n",
    "|history|**(?i)history\\|(?i)historical\\|(?i)ancient\\|(?i)archaeology\\|(?i)heritage\\|(?i)fossil**|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df\\\n",
    "    .withColumn('politics', F.col(\"Content\").rlike(\"\"\"(?i)politics|(?i)political|(?i)senate|(?i)democrats|\n",
    "(?i)republicans|(?i)government|(?i)president|(?i)prime minister|(?i)congress\"\"\"))\\\n",
    "    .withColumn('sports', F.col(\"Content\").rlike(\"\"\"(?i)sport|(?i)ball|(?i)coach|(?i)goal|(?i)baseball|(?i)football|(?i)basketball\"\"\"))\\\n",
    "    .withColumn('arts', F.col(\"Content\").rlike(\"\"\"(?i)art|(?i)painting|(?i)artist|(?i)museum|(?i)photography|(?i)sculpture\"\"\"))\\\n",
    "    .withColumn('history', F.col(\"Content\").rlike(\"\"\"(?i)history|(?i)historical|(?i)ancient|(?i)archaeology|(?i)heritage|(?i)fossil\"\"\")).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+\n",
      "|politics|sports|arts|history|\n",
      "+--------+------+----+-------+\n",
      "|   false| false|true|  false|\n",
      "|   false|  true|true|  false|\n",
      "|   false| false|true|  false|\n",
      "|    true| false|true|  false|\n",
      "|   false|  true|true|  false|\n",
      "+--------+------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('politics','sports','arts','history').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show counts of each dummy variable in the dataset. Save the result for the dummy count of `arts` to the variable name specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|politics|count|\n",
      "+--------+-----+\n",
      "|    true|  326|\n",
      "|   false|  674|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| arts|count|\n",
      "+-----+-----+\n",
      "| true|  809|\n",
      "|false|  191|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|sports|count|\n",
      "+------+-----+\n",
      "|  true|  342|\n",
      "| false|  658|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=================================================>      (57 + 2) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|history|count|\n",
      "+-------+-----+\n",
      "|   true|  131|\n",
      "|  false|  869|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "categories = ['politics', 'arts', 'sports', 'history']\n",
    "for c in categories:\n",
    "    df.groupBy(c).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'arts': True, 'count': 809}, {'arts': False, 'count': 191}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_art_soln = df.groupBy('arts').count().toPandas().to_dict(orient='records')\n",
    "df_art_soln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a SparkNLP Pipeline to construct positive/negative sentiment\n",
    "\n",
    "We are going to make a sentiment model work for this dataset. Remember, we are working with a smaller dataset before we move on to the larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ | ]tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 18:45:59.242674: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 18:46:04.996808: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2023-10-15 18:46:05.063314: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2023-10-15 18:46:05.127811: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2023-10-15 18:46:05.192137: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n",
      "2023-10-15 18:46:05.256899: W external/org_tensorflow/tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 60236800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-core_2.12-3.4.0.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "sentimentdl_use_twitter download started this may take some time.\n",
      "Approximate size to download 11.4 MB\n",
      "[ | ]sentimentdl_use_twitter download started this may take some time.\n",
      "Approximate size to download 11.4 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME='sentimentdl_use_twitter'\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"Content\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "\n",
    "sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "nlpPipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipelineModel = nlpPipeline.fit(df)\n",
    "results = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Content: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Dataset: string (nullable = true)\n",
      " |-- politics: boolean (nullable = true)\n",
      " |-- sports: boolean (nullable = true)\n",
      " |-- arts: boolean (nullable = true)\n",
      " |-- history: boolean (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence_embeddings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentiment: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull out the sentiment output into its own column in the main dataframe. Create a new dataframe that includes sentiment, news source, and your four dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-------+--------------+---------+\n",
      "|politics| arts|sports|history|   news_source|sentiment|\n",
      "+--------+-----+------+-------+--------------+---------+\n",
      "|   false| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true|  true|  false|CNN/Daily Mail| negative|\n",
      "|   false| true| false|  false|CNN/Daily Mail| negative|\n",
      "|    true| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true|  true|  false|CNN/Daily Mail| negative|\n",
      "|    true| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true| false|  false|          XSum| negative|\n",
      "|   false| true| false|  false|          XSum| negative|\n",
      "|    true| true| false|   true|CNN/Daily Mail| negative|\n",
      "|   false| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false|false| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true|  true|  false|    Multi-News| negative|\n",
      "|   false| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true|  true|   true|CNN/Daily Mail| negative|\n",
      "|    true| true| false|  false|CNN/Daily Mail| negative|\n",
      "|   false|false| false|  false|CNN/Daily Mail| negative|\n",
      "|   false| true|  true|  false|          XSum| positive|\n",
      "|   false| true| false|  false|CNN/Daily Mail| positive|\n",
      "|   false|false|  true|  false|CNN/Daily Mail| negative|\n",
      "+--------+-----+------+-------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results=results.withColumn('sentiment',F.explode(results.sentiment.result))\n",
    "final_data=results.select('politics','arts','sports','history', F.expr(\"Dataset\").alias(\"news_source\"),'sentiment')\n",
    "final_data.persist()\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a summary table of the count of articles grouped by your `politics` dummy variable, news source `news_source`, and sentiment classification `sentiment`. Save the resulting dataframe into a variable called `df_sent_baseline`, similar to the previous step for saving the Pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following code gives an exception because the `ml.t3.large` instance type runs out of memory and we cant change the instance type to one with more memory, but the code works whe we put it on a Spark cluster as part of a script***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsum_counts = final_data.groupBy(['politics', 'news_source', 'sentiment']).count()\\ndf_sent_baseline = sum_counts.toPandas().to_dict(orient='records')\\ndf_sent_baseline\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sum_counts = final_data.groupBy(['politics', 'news_source', 'sentiment']).count()\n",
    "df_sent_baseline = sum_counts.toPandas().to_dict(orient='records')\n",
    "df_sent_baseline\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Spark NLP NLP Assembly JAR file\n",
    "\n",
    "Notice how we are able to write the file directly to S3 without having to first store it in this notebook locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "!wget -qO- https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-5.1.3.jar | aws s3 cp - s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\n",
    "!aws s3 ls s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/process.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path\", type=str, help=\"Path of dataset in S3\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "    logger.info(f\"args={args}\")\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    logger.info(f\"Spark version: {spark.version}\")\n",
    "    logger.info(f\"sparknlp version: {sparknlp.version()}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"ID\", StringType(), True),\n",
    "            StructField(\"Content\", StringType(), True),\n",
    "            StructField(\"Summary\", StringType(), True),\n",
    "            StructField(\"Dataset\", StringType(), True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path}\")\n",
    "    df = spark.read.parquet(args.s3_dataset_path, header=True, schema=schema)\n",
    "    df = df.repartition(64)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # get count\n",
    "    row_count = df.count()\n",
    "    # create a temp rdd and save to s3\n",
    "    line = [f\"count={row_count}\"]\n",
    "    logger.info(line)\n",
    "    l = [('count', row_count)]\n",
    "    tmp_df = spark.createDataFrame(l)\n",
    "    s3_path = \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"count\")\n",
    "    logger.info(f\"going to save count to {s3_path}\")\n",
    "    # we want to write to a single file so coalesce\n",
    "    tmp_df.coalesce(1).write.format('csv').option('header', 'false').mode(\"overwrite\").save(s3_path)\n",
    "    \n",
    "    df = df\\\n",
    "    .withColumn('politics', F.col(\"Content\").rlike(\"\"\"(?i)politics|(?i)political|(?i)senate|(?i)democrats|(?i)republicans|(?i)government|(?i)president|(?i)prime minister|(?i)congress\"\"\"))\\\n",
    "    .withColumn('sports', F.col(\"Content\").rlike(\"\"\"(?i)sport|(?i)ball|(?i)coach|(?i)goal|(?i)baseball|(?i)football|(?i)basketball\"\"\"))\\\n",
    "    .withColumn('arts', F.col(\"Content\").rlike(\"\"\"(?i)art|(?i)painting|(?i)artist|(?i)museum|(?i)photography|(?i)sculpture\"\"\"))\\\n",
    "    .withColumn('history', F.col(\"Content\").rlike(\"\"\"(?i)history|(?i)historical|(?i)ancient|(?i)archaeology|(?i)heritage|(?i)fossil\"\"\")).persist()\n",
    "    \n",
    "    categories = ['politics', 'arts', 'sports', 'history']\n",
    "    for c in categories:\n",
    "        df_soln = df.groupBy(c).count() #.toPandas().to_dict(orient='records')        \n",
    "        s3_path = \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, c)\n",
    "        logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "        # we want to write to a single file so coalesce\n",
    "        df_soln.coalesce(1).write.format('csv').option('header', 'false').mode(\"overwrite\").save(s3_path)\n",
    "\n",
    "    # sentiment analysis\n",
    "    MODEL_NAME = 'sentimentdl_use_twitter'\n",
    "    logger.info(f\"setting up an nlp pipeline with model={MODEL_NAME}\")\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"Content\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "    use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
    "     .setInputCols([\"document\"])\\\n",
    "     .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "    sentimentdl = SentimentDLModel.pretrained(name=MODEL_NAME, lang=\"en\")\\\n",
    "    .setInputCols([\"sentence_embeddings\"])\\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "    nlp_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          use,\n",
    "          sentimentdl\n",
    "      ])\n",
    "    logger.info(f\"going to fit and transform pipeline on dataframe\")\n",
    "    pipeline_model = nlp_pipeline.fit(df)\n",
    "    results = pipeline_model.transform(df)\n",
    "    logger.info(f\"done with fit and transform pipeline on dataframe\")\n",
    "    \n",
    "    results=results.withColumn('sentiment', F.explode(results.sentiment.result))\n",
    "    final_data=results.select('politics', 'arts', 'sports', 'history', F.expr(\"Dataset\").alias(\"news_source\"),'sentiment')\n",
    "    final_data.persist()\n",
    "    #final_data.show()\n",
    "    cols = ['politics', 'news_source', 'sentiment']\n",
    "    logger.info(f\"going to run a group by and count on columns={cols}\")\n",
    "    sum_counts = final_data.groupBy(cols).count()\n",
    "    logger.info(f\"going to convert sum_counts to dict\")\n",
    "    df_sent_baseline = sum_counts #.toPandas().to_dict(orient='records')\n",
    "    logger.info(df_sent_baseline)\n",
    "    s3_path = \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"sentiment_baseline\")\n",
    "    logger.info(f\"going to save dataframe to {s3_path}\")\n",
    "    # we want to write to a single file so coalesce\n",
    "    df_sent_baseline.coalesce(1).write.format('csv').option('header', 'false').mode(\"overwrite\").save(s3_path)\n",
    "    logger.info(\"all done\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit this code to SageMaker Processing Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "account_id=038932893404, s3_dataset_path=s3://sagemaker-us-east-1-038932893404/lab8/news/data.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-lab8-2023-10-15-21-05-07-498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-lab8\",\n",
    "    image_uri=f\"{account_id}.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark:latest\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "s3_dataset_path = f\"s3://{bucket}/lab8/news/data.parquet\"\n",
    "print(f\"account_id={account_id}, s3_dataset_path={s3_dataset_path}\")\n",
    "output_prefix_data = f\"lab8/data\"\n",
    "output_prefix_logs = f\"lab8/spark_logs\"\n",
    "\n",
    "\n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/process.py\",\n",
    "    submit_jars=[f\"s3://{bucket}/lab8/spark-nlp-assembly-5.1.3.jar\"],\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path\",\n",
    "        s3_dataset_path,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        output_prefix_data,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the processing job completes, you can check the actual processing time for which your account will be billed in the SageMaker Processing Job details as shown in the screenshot below.\n",
    "\n",
    "![SageMaker Processing Job details](./img/sm-process-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result files\n",
    "All result files are now available in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-15 20:08:12  708534094 lab8/-\n",
      "2023-10-15 21:15:52          0 lab8/data/arts/_SUCCESS\n",
      "2023-10-15 21:15:52         29 lab8/data/arts/part-00000-8d5ba47c-0e7e-4b2d-86f2-28c8ec65da30-c000.csv\n",
      "2023-10-15 21:12:43          0 lab8/data/count/_SUCCESS\n",
      "2023-10-15 21:12:43         13 lab8/data/count/part-00000-4f8b5f90-f273-41f4-9cba-b187cefc41c9-c000.csv\n",
      "2023-10-15 21:15:57          0 lab8/data/history/_SUCCESS\n",
      "2023-10-15 21:15:57         29 lab8/data/history/part-00000-94a20b52-4a3e-4ba5-a04c-97de54817bde-c000.csv\n",
      "2023-10-15 21:15:50          0 lab8/data/politics/_SUCCESS\n",
      "2023-10-15 21:15:49         29 lab8/data/politics/part-00000-db25f8a8-c386-4abe-b3b4-a38e9f5a6232-c000.csv\n",
      "2023-10-15 21:23:12          0 lab8/data/sentiment_baseline/_SUCCESS\n",
      "2023-10-15 21:23:12        575 lab8/data/sentiment_baseline/part-00000-1de077ff-fbf7-4bad-9ffb-70e99f7bcdbb-c000.csv\n",
      "2023-10-15 21:15:54          0 lab8/data/sports/_SUCCESS\n",
      "2023-10-15 21:15:54         29 lab8/data/sports/part-00000-a0cbb054-1665-4757-a30d-a174b7c49c11-c000.csv\n",
      "2023-10-15 20:15:00 1617893454 lab8/news/data.parquet\n",
      "2023-10-15 20:14:58    2833149 lab8/news/data1000.parquet\n",
      "2023-10-15 20:10:43  708534094 lab8/spark-nlp-assembly-5.1.3.jar\n",
      "2023-10-15 21:23:29    4584947 lab8/spark_logs/spark_event_logs/application_1697404247468_0001\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://{bucket}/lab8/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
