{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-29 23:43:16 sagemaker-studio-692960231031-wo7kgoszj2g\n",
      "2023-08-29 23:50:01 sagemaker-us-east-1-692960231031\n",
      "2023-08-30 00:34:21 vad49\n",
      "2023-09-16 16:02:10 vad49-labdata\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 18:55:58          0 _SUCCESS\n",
      "2023-11-18 18:55:54    2866925 part-00000-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:55    2564497 part-00001-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:56      26078 part-00035-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:56      12157 part-00051-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:57      28228 part-00053-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:57      30806 part-00059-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://project17-bucket-alex/stories-and-books-nlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save books into bucket if needed\n",
    "if True is False:\n",
    "    !aws s3 cp ../../data/external-data/books/ s3://project17-bucket-alex/books --recursive --exclude \"*\" --exclude \".ipynb_checkpoints/*\" --include \"*.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True is True: # set to true only for the first un\n",
    "    # Setup - Run only once per Kernel App\n",
    "    %conda install openjdk -y\n",
    "\n",
    "    # install PySpark\n",
    "    #%pip install pyspark==3.2.0 s3fs pyarrow spark-nlp\n",
    "\n",
    "    %pip install s3fs pyarrow\n",
    "\n",
    "        # install PySpark\n",
    "    %pip install pyspark==3.4.0\n",
    "\n",
    "    # install spark-nlp\n",
    "    %pip install spark-nlp==5.1.3\n",
    "\n",
    "    # restart kernel\n",
    "    from IPython.core.display import HTML\n",
    "    HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, length, isnan, when, count, regexp_extract, weekofyear, hour, avg, to_date, unix_timestamp, lit, corr\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150) \n",
    "#pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3,org.apache.hadoop:hadoop-aws:3.2.2\")\\\n",
    "    .config(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.ContainerCredentialsProvider\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in submissions and comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 17:13:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/11/18 17:13:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.3 ms, sys: 7.08 ms, total: 64.4 ms\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "required_columns = ['subreddit', 'title', 'selftext', 'score', 'created_utc', 'url']\n",
    "\n",
    "\n",
    "# read the full year\n",
    "\n",
    "# Read in data from project bucket\n",
    "bucket = \"project17-bucket-alex\"\n",
    "\n",
    "# List of 12 directories each containing 1 month of data\n",
    "directories = [\"project_2022_\"+str(i)+\"/submissions\" for i in range(1,13)]\n",
    "\n",
    "# Iterate through 12 directories and merge each monthly data set to create one big data set\n",
    "submissions = None\n",
    "for directory in directories:\n",
    "    s3_path = f\"s3a://{bucket}/{directory}\"\n",
    "    month_df = spark.read.parquet(s3_path).select(*required_columns)\n",
    "    \n",
    "    if submissions is None:\n",
    "        submissions = month_df\n",
    "    else:\n",
    "        submissions = submissions.union(month_df)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small dfs\n",
    "\n",
    "submissions_small = submissions.sample(withReplacement=False, fraction=0.001, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which to use\n",
    "\n",
    "use_small = True  # to easily swap between the small and small dfs\n",
    "submissions_active = submissions_small if use_small else submissions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache - only for when working with the small version\n",
    "\n",
    "#submissions_active.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process Submissions Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions_active.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conduct basic data quality checks! Make sure there are no missing values, check the length of the comments, and remove rows of data that might be corrupted. Even if you think all your data is perfect, you still need to demonstrate that with your analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove submissions without a body should obviously go, but what about the submissions without a self text (deleted, removed or empty). We can keep where the author is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_submissions(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # define conditions\n",
    "    conditions = (col('selftext') != \"[removed]\") & (col('selftext') != \"[deleted]\") & (col('selftext').isNotNull() & (col('selftext') != \"\"))\n",
    "\n",
    "    \n",
    "    # apply filter\n",
    "    cleaned_df = df.filter(conditions)\n",
    "  \n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions_active = clean_submissions(submissions_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>I need advice</td>\n",
       "      <td>So my gf and I are both 19 and live separately but both in college and she works. We will live together within 2ish years after she graduates but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-25 17:01:32</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/scid7f/i_need_advice/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>Too smart to be this stupid; logics vs heart.</td>\n",
       "      <td>My estranged bf of the last almost 2yrs has vowed to make his life mission, along others aid, to make me have misery and regret. \\n   Call me cr...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-18 17:12:58</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/s72fh8/too_smart_to_be_this_stupid_logics_vs_heart/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antiwork</td>\n",
       "      <td>High hopes for the future</td>\n",
       "      <td>Just kidding, this sub is doomed\\n\\nHey mods, stop doing interviews \\n\\nYou don’t speak for the people of this sub. You are a janitor who’s only p...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-27 19:05:44</td>\n",
       "      <td>https://www.reddit.com/r/antiwork/comments/se5xf0/high_hopes_for_the_future/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NoStupidQuestions</td>\n",
       "      <td>I think I just ejaculated without trying</td>\n",
       "      <td>Okay so has anyone else have this happen. Right altering peeing. I get extreme pain I mean like eye shutting crouching over pain and there this cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-05 19:26:45</td>\n",
       "      <td>https://www.reddit.com/r/NoStupidQuestions/comments/rwuuej/i_think_i_just_ejaculated_without_trying/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>socialskills</td>\n",
       "      <td>Is it weird to tell my depressed friend I'm \"proud of him\" after he's overcome a bad mental phase?</td>\n",
       "      <td>One of my best friends suffers from depression and experiences \"down phases\" in irregular intervals. He always has a particularly bad one in winte...</td>\n",
       "      <td>131</td>\n",
       "      <td>2022-01-13 18:13:09</td>\n",
       "      <td>https://www.reddit.com/r/socialskills/comments/s35q0j/is_it_weird_to_tell_my_depressed_friend_im_proud/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit  \\\n",
       "0  relationship_advice   \n",
       "1  relationship_advice   \n",
       "2             antiwork   \n",
       "3    NoStupidQuestions   \n",
       "4         socialskills   \n",
       "\n",
       "                                                                                                title  \\\n",
       "0                                                                                       I need advice   \n",
       "1                                                       Too smart to be this stupid; logics vs heart.   \n",
       "2                                                                           High hopes for the future   \n",
       "3                                                            I think I just ejaculated without trying   \n",
       "4  Is it weird to tell my depressed friend I'm \"proud of him\" after he's overcome a bad mental phase?   \n",
       "\n",
       "                                                                                                                                                selftext  \\\n",
       "0  So my gf and I are both 19 and live separately but both in college and she works. We will live together within 2ish years after she graduates but ...   \n",
       "1    My estranged bf of the last almost 2yrs has vowed to make his life mission, along others aid, to make me have misery and regret. \\n   Call me cr...   \n",
       "2  Just kidding, this sub is doomed\\n\\nHey mods, stop doing interviews \\n\\nYou don’t speak for the people of this sub. You are a janitor who’s only p...   \n",
       "3  Okay so has anyone else have this happen. Right altering peeing. I get extreme pain I mean like eye shutting crouching over pain and there this cl...   \n",
       "4  One of my best friends suffers from depression and experiences \"down phases\" in irregular intervals. He always has a particularly bad one in winte...   \n",
       "\n",
       "   score         created_utc  \\\n",
       "0      1 2022-01-25 17:01:32   \n",
       "1      1 2022-01-18 17:12:58   \n",
       "2      0 2022-01-27 19:05:44   \n",
       "3      1 2022-01-05 19:26:45   \n",
       "4    131 2022-01-13 18:13:09   \n",
       "\n",
       "                                                                                                         url  \n",
       "0                                https://www.reddit.com/r/relationship_advice/comments/scid7f/i_need_advice/  \n",
       "1  https://www.reddit.com/r/relationship_advice/comments/s72fh8/too_smart_to_be_this_stupid_logics_vs_heart/  \n",
       "2                               https://www.reddit.com/r/antiwork/comments/se5xf0/high_hopes_for_the_future/  \n",
       "3       https://www.reddit.com/r/NoStupidQuestions/comments/rwuuej/i_think_i_just_ejaculated_without_trying/  \n",
       "4    https://www.reddit.com/r/socialskills/comments/s35q0j/is_it_weird_to_tell_my_depressed_friend_im_proud/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# uncomment to check if needed\n",
    "\n",
    "#display(f\"submissions shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n",
    "\n",
    "\n",
    "# display(submissions_active.limit(5).toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use regex the remove text after 'Edit: ' or 'edit: '\n",
    "\n",
    "# The regular expression pattern\n",
    "pattern = r\"(?i)^(.*?)(?=Edit:|$)\"\n",
    "\n",
    "# Apply the regular expression to create a new column with the modified text\n",
    "submissions_active = submissions_active.withColumn(\"selftext_modified\", regexp_extract(col(\"selftext\"), pattern, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define stories as posts longer than a certain length \n",
    "\n",
    "story_length = 4500\n",
    "\n",
    "submissions_active = submissions_active.filter(length(col(\"selftext\")) > story_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keep only the 25% most engaging posts\n",
    "\n",
    "\n",
    "# Calculate the approximate percentile of the 'score' column\n",
    "quantile_value = submissions_active.approxQuantile(\"score\", [0.85], 0.05)  # 0.05 is the relative error\n",
    "\n",
    "# Filter the DataFrame to keep scores above or equal to this value\n",
    "submissions_active = submissions_active.filter(col(\"score\") >= quantile_value[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>Is dating online cheating.. the whole story</td>\n",
       "      <td>Due to the overwhelming messages and requests for the whole story on about my last post. I decided to do a tell-all. So I hope you have a minute. ...</td>\n",
       "      <td>29</td>\n",
       "      <td>2022-07-13 15:48:56</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/vy6fh0/is_dating_online_cheating_the_whole_story/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>AITA for expecting my son to share his room?</td>\n",
       "      <td>Background: My (40sf) husband (40sm) and I bought a 3-bedroom house a few years ago, shortly before the panini. We of course took the master bedro...</td>\n",
       "      <td>16459</td>\n",
       "      <td>2022-10-26 08:51:01</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/ydt2w5/aita_for_expecting_my_son_to_share_his_room/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>WIBTA for taking my (15f) laundry basket full of clothes, which my stepdad (49m) threw out for not putting them away, out of the trash?</td>\n",
       "      <td>Sorry for bad English!\\n\\nThe morning before my clothes were thrown away my mom put them in a laundry basket beside my door. She did tell me ofcou...</td>\n",
       "      <td>95</td>\n",
       "      <td>2022-11-07 19:59:01</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/yoyf1r/wibta_for_taking_my_15f_laundry_basket_full_of/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>My housemate (25f) did something I (20f) consider morally unacceptable and its causing a lot of conflict in the house (6 students).</td>\n",
       "      <td>Tl;dr version: I live in a student houseshare. One of my housemates is a student nurse and was extremely inappropriate with a patient which I foun...</td>\n",
       "      <td>421</td>\n",
       "      <td>2022-11-04 20:32:06</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/ym9kw7/my_housemate_25f_did_something_i_20f_consider/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>socialskills</td>\n",
       "      <td>How to Talk About Yourself (and How to Have Good Conversation)</td>\n",
       "      <td>**TL:DR talk about yourself by saying just a little bit in a way that's relevant to the topic at hand, and give the other person the implicit choi...</td>\n",
       "      <td>27</td>\n",
       "      <td>2022-12-01 14:46:48</td>\n",
       "      <td>https://www.reddit.com/r/socialskills/comments/z9polj/how_to_talk_about_yourself_and_how_to_have_good/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit  \\\n",
       "0  relationship_advice   \n",
       "1        AmItheAsshole   \n",
       "2        AmItheAsshole   \n",
       "3  relationship_advice   \n",
       "4         socialskills   \n",
       "\n",
       "                                                                                                                                     title  \\\n",
       "0                                                                                              Is dating online cheating.. the whole story   \n",
       "1                                                                                             AITA for expecting my son to share his room?   \n",
       "2  WIBTA for taking my (15f) laundry basket full of clothes, which my stepdad (49m) threw out for not putting them away, out of the trash?   \n",
       "3      My housemate (25f) did something I (20f) consider morally unacceptable and its causing a lot of conflict in the house (6 students).   \n",
       "4                                                                           How to Talk About Yourself (and How to Have Good Conversation)   \n",
       "\n",
       "                                                                                                                                                selftext  \\\n",
       "0  Due to the overwhelming messages and requests for the whole story on about my last post. I decided to do a tell-all. So I hope you have a minute. ...   \n",
       "1  Background: My (40sf) husband (40sm) and I bought a 3-bedroom house a few years ago, shortly before the panini. We of course took the master bedro...   \n",
       "2  Sorry for bad English!\\n\\nThe morning before my clothes were thrown away my mom put them in a laundry basket beside my door. She did tell me ofcou...   \n",
       "3  Tl;dr version: I live in a student houseshare. One of my housemates is a student nurse and was extremely inappropriate with a patient which I foun...   \n",
       "4  **TL:DR talk about yourself by saying just a little bit in a way that's relevant to the topic at hand, and give the other person the implicit choi...   \n",
       "\n",
       "   score         created_utc  \\\n",
       "0     29 2022-07-13 15:48:56   \n",
       "1  16459 2022-10-26 08:51:01   \n",
       "2     95 2022-11-07 19:59:01   \n",
       "3    421 2022-11-04 20:32:06   \n",
       "4     27 2022-12-01 14:46:48   \n",
       "\n",
       "                                                                                                           url  \\\n",
       "0      https://www.reddit.com/r/relationship_advice/comments/vy6fh0/is_dating_online_cheating_the_whole_story/   \n",
       "1          https://www.reddit.com/r/AmItheAsshole/comments/ydt2w5/aita_for_expecting_my_son_to_share_his_room/   \n",
       "2       https://www.reddit.com/r/AmItheAsshole/comments/yoyf1r/wibta_for_taking_my_15f_laundry_basket_full_of/   \n",
       "3  https://www.reddit.com/r/relationship_advice/comments/ym9kw7/my_housemate_25f_did_something_i_20f_consider/   \n",
       "4       https://www.reddit.com/r/socialskills/comments/z9polj/how_to_talk_about_yourself_and_how_to_have_good/   \n",
       "\n",
       "  selftext_modified  \n",
       "0                    \n",
       "1                    \n",
       "2                    \n",
       "3                    \n",
       "4                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# uncomment to check if needed\n",
    "\n",
    "#display(f\"submissions shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n",
    "\n",
    "\n",
    "#display(submissions_active.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 17:33:32 WARN TaskSetManager: Stage 65 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|       Metamorphosis|\n",
      "|      by Franz Kafka|\n",
      "|Translated by Dav...|\n",
      "|                   I|\n",
      "|One morning, when...|\n",
      "|“What’s happened ...|\n",
      "|Gregor then turne...|\n",
      "|“Oh, God”, he tho...|\n",
      "|He slid back into...|\n",
      "|And he looked ove...|\n",
      "|He was still hurr...|\n",
      "|The first thing h...|\n",
      "|It was a simple m...|\n",
      "|The first thing h...|\n",
      "|So then he tried ...|\n",
      "|It took just as m...|\n",
      "|But then he said ...|\n",
      "|When Gregor was a...|\n",
      "|After a while he ...|\n",
      "|“Something’s fall...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 17:33:32 WARN TaskSetManager: Stage 66 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'shape: (7825, 1)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def process_gutenberg_books_to_df(directory_path):\n",
    "    all_rows = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):  # Assuming the files are in .txt format\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                start_reading = False\n",
    "                paragraph = \"\"\n",
    "                for line in file:\n",
    "                    if \"*** END OF THE PROJECT GUTENBERG\" in line:\n",
    "                        break\n",
    "                    if start_reading:\n",
    "                        if line.strip() == \"\":\n",
    "                            if paragraph.strip():\n",
    "                                all_rows.append(Row(text=paragraph.strip()))\n",
    "                                paragraph = \"\"\n",
    "                        else:\n",
    "                            paragraph += line\n",
    "                    elif \"*** START OF THE PROJECT GUTENBERG\" in line:\n",
    "                        start_reading = True\n",
    "\n",
    "    # Create a DataFrame from the list of Rows\n",
    "    return spark.createDataFrame(all_rows)\n",
    "\n",
    "# Process the Gutenberg books and create a DataFrame\n",
    "books = process_gutenberg_books_to_df('../../data/external-data/books')\n",
    "books.show()\n",
    "\n",
    "display(f\"shape: ({books.count()}, {len(books.columns)})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "# Process `submissions_active` to extract and transform the 'selftext' column\n",
    "submissions_active = submissions_active.select(\"selftext\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the Project Gutenberg DataFrame\n",
    "#books = books.withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "# For the submissions DataFrame\n",
    "submissions_active = submissions_active.withColumnRenamed(\"selftext\", \"text\")\n",
    "\n",
    "#display(f\"shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now you can union them\n",
    "all_model_text = books.unionByName(submissions_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 17:33:54 WARN TaskSetManager: Stage 69 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|       Metamorphosis|\n",
      "|      by Franz Kafka|\n",
      "|Translated by Dav...|\n",
      "|                   I|\n",
      "|One morning, when...|\n",
      "|“What’s happened ...|\n",
      "|Gregor then turne...|\n",
      "|“Oh, God”, he tho...|\n",
      "|He slid back into...|\n",
      "|And he looked ove...|\n",
      "|He was still hurr...|\n",
      "|The first thing h...|\n",
      "|It was a simple m...|\n",
      "|The first thing h...|\n",
      "|So then he tried ...|\n",
      "|It took just as m...|\n",
      "|But then he said ...|\n",
      "|When Gregor was a...|\n",
      "|After a while he ...|\n",
      "|“Something’s fall...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_model_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#########\n",
    "#clean\n",
    "#########\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "import string\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "from pyspark.sql.functions import col, lower\n",
    "\n",
    "# Define a UDF to lowercase all elements in the array\n",
    "def lowercase_array(arr):\n",
    "    return [elem.lower() for elem in arr]\n",
    "\n",
    "# Define a UDF for custom tokenization\n",
    "def custom_tokenize(text):\n",
    "    # List of characters to keep\n",
    "    keep_chars = set(\" .,!?;\")\n",
    "    tokens = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for char in text:\n",
    "        if char in keep_chars:\n",
    "            if buffer:\n",
    "                tokens.append(buffer)\n",
    "                buffer = \"\"\n",
    "            tokens.append(char)\n",
    "        elif char.isalnum():  # Keep alphanumeric characters\n",
    "            buffer += char\n",
    "\n",
    "    if buffer:\n",
    "        tokens.append(buffer)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "lowercase_array_udf = udf(lowercase_array, ArrayType(StringType()))\n",
    "\n",
    "custom_tokenize_udf = udf(custom_tokenize, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to your DataFrame\n",
    "all_model_text = all_model_text.withColumn(\"custom_tokens\", custom_tokenize_udf(\"text\"))\n",
    "\n",
    "# Concatenate the tokens into a single string\n",
    "all_model_text = all_model_text.withColumn(\"concatenated_tokens\", concat_ws(\" \", \"custom_tokens\"))\n",
    "\n",
    "# Document assembler configuration\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"concatenated_tokens\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizer configuration (if still needed)\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Finisher configuration\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "# Define and build the pipeline\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "# Apply the pipeline\n",
    "all_model_text = pipeline.fit(all_model_text).transform(all_model_text)\n",
    "\n",
    "# Apply the lowercase UDF to the custom_tokens column\n",
    "all_model_text = all_model_text.withColumn(\"custom_tokens\", lowercase_array_udf(col(\"custom_tokens\")))\n",
    "\n",
    "# Drop the concatenated_tokens and tokens columns\n",
    "all_model_text = all_model_text.drop(\"concatenated_tokens\", \"tokens\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 18:42:17 WARN TaskSetManager: Stage 89 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|       custom_tokens|\n",
      "+--------------------+--------------------+\n",
      "|       Metamorphosis|     [metamorphosis]|\n",
      "|      by Franz Kafka|[by,  , franz,  ,...|\n",
      "|Translated by Dav...|[translated,  , b...|\n",
      "|                   I|                 [i]|\n",
      "|One morning, when...|[one,  , morning,...|\n",
      "|“What’s happened ...|[whats,  , happen...|\n",
      "|Gregor then turne...|[gregor,  , then,...|\n",
      "|“Oh, God”, he tho...|[oh, ,,  , god, ,...|\n",
      "|He slid back into...|[he,  , slid,  , ...|\n",
      "|And he looked ove...|[and,  , he,  , l...|\n",
      "|He was still hurr...|[he,  , was,  , s...|\n",
      "|The first thing h...|[the,  , first,  ...|\n",
      "|It was a simple m...|[it,  , was,  , a...|\n",
      "|The first thing h...|[the,  , first,  ...|\n",
      "|So then he tried ...|[so,  , then,  , ...|\n",
      "|It took just as m...|[it,  , took,  , ...|\n",
      "|But then he said ...|[but,  , then,  ,...|\n",
      "|When Gregor was a...|[when,  , gregor,...|\n",
      "|After a while he ...|[after,  , a,  , ...|\n",
      "|“Something’s fall...|[somethings,  , f...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_model_text.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 18:47:18 WARN TaskSetManager: Stage 91 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "import string\n",
    "\n",
    "\n",
    "# Build a global vocabulary from the entire dataset\n",
    "all_texts = [row.custom_tokens for row in all_model_text.collect()]  # Adjust as per your DataFrame structure\n",
    "global_vocab = sorted(set(\"\".join(sum(all_texts, []))))  # Sum flattens the list of lists\n",
    "char2idx = {char: idx + 1 for idx, char in enumerate(global_vocab)}  # Start indexing from 1\n",
    "char2idx[\"UNK\"] = 0  # Reserve 0 for unknown characters\n",
    "\n",
    "def chars_to_ints(text):\n",
    "    return [char2idx.get(char, char2idx[\"UNK\"]) for char in \"\".join(text)]\n",
    "\n",
    "chars_to_ints_udf = udf(chars_to_ints, ArrayType(IntegerType()))\n",
    "\n",
    "all_model_text = all_model_text.withColumn(\"text_as_int\", chars_to_ints_udf(col(\"custom_tokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 18:51:08 WARN TaskSetManager: Stage 92 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|       custom_tokens|         text_as_int|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|       Metamorphosis|     [metamorphosis]|[29, 21, 36, 17, ...|\n",
      "|      by Franz Kafka|[by,  , franz,  ,...|[18, 41, 1, 22, 3...|\n",
      "|Translated by Dav...|[translated,  , b...|[36, 34, 17, 30, ...|\n",
      "|                   I|                 [i]|                [25]|\n",
      "|One morning, when...|[one,  , morning,...|[31, 30, 21, 1, 2...|\n",
      "|“What’s happened ...|[whats,  , happen...|[39, 24, 17, 36, ...|\n",
      "|Gregor then turne...|[gregor,  , then,...|[23, 34, 21, 23, ...|\n",
      "|“Oh, God”, he tho...|[oh, ,,  , god, ,...|[31, 24, 3, 1, 23...|\n",
      "|He slid back into...|[he,  , slid,  , ...|[24, 21, 1, 35, 2...|\n",
      "|And he looked ove...|[and,  , he,  , l...|[17, 30, 20, 1, 2...|\n",
      "|He was still hurr...|[he,  , was,  , s...|[24, 21, 1, 39, 1...|\n",
      "|The first thing h...|[the,  , first,  ...|[36, 24, 21, 1, 2...|\n",
      "|It was a simple m...|[it,  , was,  , a...|[25, 36, 1, 39, 1...|\n",
      "|The first thing h...|[the,  , first,  ...|[36, 24, 21, 1, 2...|\n",
      "|So then he tried ...|[so,  , then,  , ...|[35, 31, 1, 36, 2...|\n",
      "|It took just as m...|[it,  , took,  , ...|[25, 36, 1, 36, 3...|\n",
      "|But then he said ...|[but,  , then,  ,...|[18, 37, 36, 1, 3...|\n",
      "|When Gregor was a...|[when,  , gregor,...|[39, 24, 21, 30, ...|\n",
      "|After a while he ...|[after,  , a,  , ...|[17, 22, 36, 21, ...|\n",
      "|“Something’s fall...|[somethings,  , f...|[35, 31, 29, 21, ...|\n",
      "|“Gregor”, said hi...|[gregor, ,,  , sa...|[23, 34, 21, 23, ...|\n",
      "|So why did his si...|[so,  , why,  , d...|[35, 31, 1, 39, 2...|\n",
      "|The chief clerk n...|[the,  , chief,  ...|[36, 24, 21, 1, 1...|\n",
      "|“But Sir”, called...|[but,  , sir, ,, ...|[18, 37, 36, 1, 3...|\n",
      "|And while Gregor ...|[and,  , while,  ...|[17, 30, 20, 1, 3...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_model_text.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 18:52:50 WARN TaskSetManager: Stage 93 contains a task of very large size (1417 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_model_text.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://project17-bucket-alex/stories-and-books-nlp/\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
