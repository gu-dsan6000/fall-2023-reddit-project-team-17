{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-29 23:43:16 sagemaker-studio-692960231031-wo7kgoszj2g\n",
      "2023-08-29 23:50:01 sagemaker-us-east-1-692960231031\n",
      "2023-08-30 00:34:21 vad49\n",
      "2023-09-16 16:02:10 vad49-labdata\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://project17-bucket-alex/stories-and-books-nlp/all-model-text/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save books into bucket if needed\n",
    "if True is False:\n",
    "    !aws s3 cp ../../data/external-data/books/ s3://project17-bucket-alex/books --recursive --exclude \"*\" --exclude \".ipynb_checkpoints/*\" --include \"*.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (13.0.0)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.10/site-packages (from s3fs) (1.31.63)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.26.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.4.0\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.7 (from pyspark==3.4.0)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.2\n",
      "    Uninstalling py4j-0.10.9.2:\n",
      "      Successfully uninstalled py4j-0.10.9.2\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.2.0\n",
      "    Uninstalling pyspark-3.2.0:\n",
      "      Successfully uninstalled pyspark-3.2.0\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spark-nlp==5.1.3 in /opt/conda/lib/python3.10/site-packages (5.1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "if True is True: # set to true only for the first un\n",
    "    # Setup - Run only once per Kernel App\n",
    "    %conda install openjdk -y\n",
    "\n",
    "    # install PySpark\n",
    "    #%pip install pyspark==3.2.0 s3fs pyarrow spark-nlp\n",
    "\n",
    "    %pip install s3fs pyarrow\n",
    "\n",
    "        # install PySpark\n",
    "    %pip install pyspark==3.4.0\n",
    "\n",
    "    # install spark-nlp\n",
    "    %pip install spark-nlp==5.1.3\n",
    "\n",
    "    # restart kernel\n",
    "    from IPython.core.display import HTML\n",
    "    HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, length, isnan, when, count, regexp_extract, weekofyear, hour, avg, to_date, unix_timestamp, lit, corr\n",
    "\n",
    "import json\n",
    "import sparknlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150) \n",
    "#pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3,org.apache.hadoop:hadoop-aws:3.2.2\")\\\n",
    "    .config(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.ContainerCredentialsProvider\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in submissions and comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49 ms, sys: 1.97 ms, total: 51 ms\n",
      "Wall time: 5.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "required_columns = ['subreddit', 'title', 'selftext', 'score', 'created_utc', 'url']\n",
    "\n",
    "\n",
    "# read the full year\n",
    "\n",
    "# Read in data from project bucket\n",
    "bucket = \"project17-bucket-alex\"\n",
    "\n",
    "# List of 12 directories each containing 1 month of data\n",
    "directories = [\"project_2022_\"+str(i)+\"/submissions\" for i in range(1,13)]\n",
    "\n",
    "# Iterate through 12 directories and merge each monthly data set to create one big data set\n",
    "submissions = None\n",
    "for directory in directories:\n",
    "    s3_path = f\"s3a://{bucket}/{directory}\"\n",
    "    month_df = spark.read.parquet(s3_path).select(*required_columns)\n",
    "    \n",
    "    if submissions is None:\n",
    "        submissions = month_df\n",
    "    else:\n",
    "        submissions = submissions.union(month_df)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_small = submissions.sample(withReplacement=False, fraction=0.001, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small dfs\n",
    "\n",
    "use_small = True  # to easily swap between the small and small dfs\n",
    "submissions_active = submissions_small if use_small else submissions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache - only for when working with the small version\n",
    "#submissions_active.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process Submissions Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions_active.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conduct basic data quality checks! Make sure there are no missing values, check the length of the comments, and remove rows of data that might be corrupted. Even if you think all your data is perfect, you still need to demonstrate that with your analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove submissions without a body should obviously go, but what about the submissions without a self text (deleted, removed or empty). We can keep where the author is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_submissions(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # define conditions\n",
    "    conditions = (col('selftext') != \"[removed]\") & (col('selftext') != \"[deleted]\") & (col('selftext').isNotNull() & (col('selftext') != \"\"))\n",
    "\n",
    "    \n",
    "    # apply filter\n",
    "    cleaned_df = df.filter(conditions)\n",
    "  \n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions_active = clean_submissions(submissions_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>I need advice</td>\n",
       "      <td>So my gf and I are both 19 and live separately but both in college and she works. We will live together within 2ish years after she graduates but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-25 17:01:32</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/scid7f/i_need_advice/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>Too smart to be this stupid; logics vs heart.</td>\n",
       "      <td>My estranged bf of the last almost 2yrs has vowed to make his life mission, along others aid, to make me have misery and regret. \\n   Call me cr...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-18 17:12:58</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/s72fh8/too_smart_to_be_this_stupid_logics_vs_heart/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>antiwork</td>\n",
       "      <td>High hopes for the future</td>\n",
       "      <td>Just kidding, this sub is doomed\\n\\nHey mods, stop doing interviews \\n\\nYou don’t speak for the people of this sub. You are a janitor who’s only p...</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-27 19:05:44</td>\n",
       "      <td>https://www.reddit.com/r/antiwork/comments/se5xf0/high_hopes_for_the_future/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NoStupidQuestions</td>\n",
       "      <td>I think I just ejaculated without trying</td>\n",
       "      <td>Okay so has anyone else have this happen. Right altering peeing. I get extreme pain I mean like eye shutting crouching over pain and there this cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-05 19:26:45</td>\n",
       "      <td>https://www.reddit.com/r/NoStupidQuestions/comments/rwuuej/i_think_i_just_ejaculated_without_trying/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>socialskills</td>\n",
       "      <td>Is it weird to tell my depressed friend I'm \"proud of him\" after he's overcome a bad mental phase?</td>\n",
       "      <td>One of my best friends suffers from depression and experiences \"down phases\" in irregular intervals. He always has a particularly bad one in winte...</td>\n",
       "      <td>131</td>\n",
       "      <td>2022-01-13 18:13:09</td>\n",
       "      <td>https://www.reddit.com/r/socialskills/comments/s35q0j/is_it_weird_to_tell_my_depressed_friend_im_proud/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit  \\\n",
       "0  relationship_advice   \n",
       "1  relationship_advice   \n",
       "2             antiwork   \n",
       "3    NoStupidQuestions   \n",
       "4         socialskills   \n",
       "\n",
       "                                                                                                title  \\\n",
       "0                                                                                       I need advice   \n",
       "1                                                       Too smart to be this stupid; logics vs heart.   \n",
       "2                                                                           High hopes for the future   \n",
       "3                                                            I think I just ejaculated without trying   \n",
       "4  Is it weird to tell my depressed friend I'm \"proud of him\" after he's overcome a bad mental phase?   \n",
       "\n",
       "                                                                                                                                                selftext  \\\n",
       "0  So my gf and I are both 19 and live separately but both in college and she works. We will live together within 2ish years after she graduates but ...   \n",
       "1    My estranged bf of the last almost 2yrs has vowed to make his life mission, along others aid, to make me have misery and regret. \\n   Call me cr...   \n",
       "2  Just kidding, this sub is doomed\\n\\nHey mods, stop doing interviews \\n\\nYou don’t speak for the people of this sub. You are a janitor who’s only p...   \n",
       "3  Okay so has anyone else have this happen. Right altering peeing. I get extreme pain I mean like eye shutting crouching over pain and there this cl...   \n",
       "4  One of my best friends suffers from depression and experiences \"down phases\" in irregular intervals. He always has a particularly bad one in winte...   \n",
       "\n",
       "   score         created_utc  \\\n",
       "0      1 2022-01-25 17:01:32   \n",
       "1      1 2022-01-18 17:12:58   \n",
       "2      0 2022-01-27 19:05:44   \n",
       "3      1 2022-01-05 19:26:45   \n",
       "4    131 2022-01-13 18:13:09   \n",
       "\n",
       "                                                                                                         url  \n",
       "0                                https://www.reddit.com/r/relationship_advice/comments/scid7f/i_need_advice/  \n",
       "1  https://www.reddit.com/r/relationship_advice/comments/s72fh8/too_smart_to_be_this_stupid_logics_vs_heart/  \n",
       "2                               https://www.reddit.com/r/antiwork/comments/se5xf0/high_hopes_for_the_future/  \n",
       "3       https://www.reddit.com/r/NoStupidQuestions/comments/rwuuej/i_think_i_just_ejaculated_without_trying/  \n",
       "4    https://www.reddit.com/r/socialskills/comments/s35q0j/is_it_weird_to_tell_my_depressed_friend_im_proud/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(f\"submissions shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n",
    "\n",
    "display(submissions_active.limit(5).toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use regex the remove text after 'Edit: ' or 'edit: '\n",
    "\n",
    "# The regular expression pattern\n",
    "pattern = r\"(?i)^(.*?)(?=Edit:|$)\"\n",
    "\n",
    "# Apply the regular expression to create a new column with the modified text\n",
    "submissions_active = submissions_active.withColumn(\"selftext_modified\", regexp_extract(col(\"selftext\"), pattern, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define stories as posts longer than a certain length \n",
    "\n",
    "story_length = 4500\n",
    "\n",
    "submissions_active = submissions_active.filter(length(col(\"selftext\")) > story_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keep only the 25% most engaging posts\n",
    "\n",
    "\n",
    "# Calculate the approximate percentile of the 'score' column\n",
    "quantile_value = submissions_active.approxQuantile(\"score\", [0.85], 0.05)  # 0.05 is the relative error\n",
    "\n",
    "# Filter the DataFrame to keep scores above or equal to this value\n",
    "submissions_active = submissions_active.filter(col(\"score\") >= quantile_value[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>Is dating online cheating.. the whole story</td>\n",
       "      <td>Due to the overwhelming messages and requests for the whole story on about my last post. I decided to do a tell-all. So I hope you have a minute. ...</td>\n",
       "      <td>29</td>\n",
       "      <td>2022-07-13 15:48:56</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/vy6fh0/is_dating_online_cheating_the_whole_story/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>AITA for expecting my son to share his room?</td>\n",
       "      <td>Background: My (40sf) husband (40sm) and I bought a 3-bedroom house a few years ago, shortly before the panini. We of course took the master bedro...</td>\n",
       "      <td>16459</td>\n",
       "      <td>2022-10-26 08:51:01</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/ydt2w5/aita_for_expecting_my_son_to_share_his_room/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AmItheAsshole</td>\n",
       "      <td>WIBTA for taking my (15f) laundry basket full of clothes, which my stepdad (49m) threw out for not putting them away, out of the trash?</td>\n",
       "      <td>Sorry for bad English!\\n\\nThe morning before my clothes were thrown away my mom put them in a laundry basket beside my door. She did tell me ofcou...</td>\n",
       "      <td>95</td>\n",
       "      <td>2022-11-07 19:59:01</td>\n",
       "      <td>https://www.reddit.com/r/AmItheAsshole/comments/yoyf1r/wibta_for_taking_my_15f_laundry_basket_full_of/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relationship_advice</td>\n",
       "      <td>My housemate (25f) did something I (20f) consider morally unacceptable and its causing a lot of conflict in the house (6 students).</td>\n",
       "      <td>Tl;dr version: I live in a student houseshare. One of my housemates is a student nurse and was extremely inappropriate with a patient which I foun...</td>\n",
       "      <td>421</td>\n",
       "      <td>2022-11-04 20:32:06</td>\n",
       "      <td>https://www.reddit.com/r/relationship_advice/comments/ym9kw7/my_housemate_25f_did_something_i_20f_consider/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>socialskills</td>\n",
       "      <td>How to Talk About Yourself (and How to Have Good Conversation)</td>\n",
       "      <td>**TL:DR talk about yourself by saying just a little bit in a way that's relevant to the topic at hand, and give the other person the implicit choi...</td>\n",
       "      <td>27</td>\n",
       "      <td>2022-12-01 14:46:48</td>\n",
       "      <td>https://www.reddit.com/r/socialskills/comments/z9polj/how_to_talk_about_yourself_and_how_to_have_good/</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit  \\\n",
       "0  relationship_advice   \n",
       "1        AmItheAsshole   \n",
       "2        AmItheAsshole   \n",
       "3  relationship_advice   \n",
       "4         socialskills   \n",
       "\n",
       "                                                                                                                                     title  \\\n",
       "0                                                                                              Is dating online cheating.. the whole story   \n",
       "1                                                                                             AITA for expecting my son to share his room?   \n",
       "2  WIBTA for taking my (15f) laundry basket full of clothes, which my stepdad (49m) threw out for not putting them away, out of the trash?   \n",
       "3      My housemate (25f) did something I (20f) consider morally unacceptable and its causing a lot of conflict in the house (6 students).   \n",
       "4                                                                           How to Talk About Yourself (and How to Have Good Conversation)   \n",
       "\n",
       "                                                                                                                                                selftext  \\\n",
       "0  Due to the overwhelming messages and requests for the whole story on about my last post. I decided to do a tell-all. So I hope you have a minute. ...   \n",
       "1  Background: My (40sf) husband (40sm) and I bought a 3-bedroom house a few years ago, shortly before the panini. We of course took the master bedro...   \n",
       "2  Sorry for bad English!\\n\\nThe morning before my clothes were thrown away my mom put them in a laundry basket beside my door. She did tell me ofcou...   \n",
       "3  Tl;dr version: I live in a student houseshare. One of my housemates is a student nurse and was extremely inappropriate with a patient which I foun...   \n",
       "4  **TL:DR talk about yourself by saying just a little bit in a way that's relevant to the topic at hand, and give the other person the implicit choi...   \n",
       "\n",
       "   score         created_utc  \\\n",
       "0     29 2022-07-13 15:48:56   \n",
       "1  16459 2022-10-26 08:51:01   \n",
       "2     95 2022-11-07 19:59:01   \n",
       "3    421 2022-11-04 20:32:06   \n",
       "4     27 2022-12-01 14:46:48   \n",
       "\n",
       "                                                                                                           url  \\\n",
       "0      https://www.reddit.com/r/relationship_advice/comments/vy6fh0/is_dating_online_cheating_the_whole_story/   \n",
       "1          https://www.reddit.com/r/AmItheAsshole/comments/ydt2w5/aita_for_expecting_my_son_to_share_his_room/   \n",
       "2       https://www.reddit.com/r/AmItheAsshole/comments/yoyf1r/wibta_for_taking_my_15f_laundry_basket_full_of/   \n",
       "3  https://www.reddit.com/r/relationship_advice/comments/ym9kw7/my_housemate_25f_did_something_i_20f_consider/   \n",
       "4       https://www.reddit.com/r/socialskills/comments/z9polj/how_to_talk_about_yourself_and_how_to_have_good/   \n",
       "\n",
       "  selftext_modified  \n",
       "0                    \n",
       "1                    \n",
       "2                    \n",
       "3                    \n",
       "4                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display(f\"submissions shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n",
    "\n",
    "\n",
    "display(submissions_active.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|       Metamorphosis|\n",
      "|      by Franz Kafka|\n",
      "|Translated by Dav...|\n",
      "|                   I|\n",
      "|One morning, when...|\n",
      "|“What’s happened ...|\n",
      "|Gregor then turne...|\n",
      "|“Oh, God”, he tho...|\n",
      "|He slid back into...|\n",
      "|And he looked ove...|\n",
      "|He was still hurr...|\n",
      "|The first thing h...|\n",
      "|It was a simple m...|\n",
      "|The first thing h...|\n",
      "|So then he tried ...|\n",
      "|It took just as m...|\n",
      "|But then he said ...|\n",
      "|When Gregor was a...|\n",
      "|After a while he ...|\n",
      "|“Something’s fall...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 15:25:53 WARN TaskSetManager: Stage 39 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def process_gutenberg_books_to_df(directory_path):\n",
    "    all_rows = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.txt'):  # Assuming the files are in .txt format\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                start_idx = content.find(\"*** START OF THE PROJECT GUTENBERG\")\n",
    "                if start_idx != -1:\n",
    "                    start_idx = content.find('\\n', start_idx) + 1\n",
    "                    content = content[start_idx:]\n",
    "                # Splitting the content into paragraphs or sentences\n",
    "                for paragraph in content.split('\\n\\n'):\n",
    "                    if paragraph.strip():  # Check if the paragraph is not just whitespace\n",
    "                        all_rows.append(Row(text=paragraph.strip()))\n",
    "\n",
    "    # Create a DataFrame from the list of Rows\n",
    "    return spark.createDataFrame(all_rows)\n",
    "\n",
    "# Process the Gutenberg books and create a DataFrame\n",
    "books = process_gutenberg_books_to_df('../../data/external-data/books')\n",
    "books.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "# Process `submissions_active` to extract and transform the 'selftext' column\n",
    "submissions_active = submissions_active.select(\"selftext\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For the Project Gutenberg DataFrame\n",
    "#books = books.withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "# For the submissions DataFrame\n",
    "submissions_active = submissions_active.withColumnRenamed(\"selftext\", \"text\")\n",
    "\n",
    "# Now you can union them\n",
    "all_model_text = books.unionByName(submissions_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|       Metamorphosis|\n",
      "|      by Franz Kafka|\n",
      "|Translated by Dav...|\n",
      "|                   I|\n",
      "|One morning, when...|\n",
      "|“What’s happened ...|\n",
      "|Gregor then turne...|\n",
      "|“Oh, God”, he tho...|\n",
      "|He slid back into...|\n",
      "|And he looked ove...|\n",
      "|He was still hurr...|\n",
      "|The first thing h...|\n",
      "|It was a simple m...|\n",
      "|The first thing h...|\n",
      "|So then he tried ...|\n",
      "|It took just as m...|\n",
      "|But then he said ...|\n",
      "|When Gregor was a...|\n",
      "|After a while he ...|\n",
      "|“Something’s fall...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 15:25:54 WARN TaskSetManager: Stage 40 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "all_model_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#########\n",
    "#clean\n",
    "#########\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "import string\n",
    "\n",
    "# Start Spark NLP pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True) \\\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCols([\"clean_text\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "# Define the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    finisher\n",
    "])\n",
    "\n",
    "# Apply the pipeline to the submissions and books DataFrames\n",
    "#submissions_active_df = pipeline.fit(submissions_active).transform(submissions_active)\n",
    "all_model_text = pipeline.fit(all_model_text).transform(all_model_text)\n",
    "\n",
    "\n",
    "def chars_to_ints(text):\n",
    "    vocab = sorted(set(\"\".join(text)))  # Create a vocabulary from the cleaned text\n",
    "    char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "    return [char2idx.get(c, 0) for c in \"\".join(text)]  # Convert each char in text to its integer representation\n",
    "\n",
    "chars_to_ints_udf = udf(chars_to_ints, ArrayType(IntegerType()))\n",
    "\n",
    "# Apply the UDF to the 'clean_text' column\n",
    "#submissions_active_df = submissions_active_df.withColumn(\"text_as_int\", chars_to_ints_udf(col(\"clean_text\")))\n",
    "all_model_text = all_model_text.withColumn(\"text_as_int\", chars_to_ints_udf(col(\"clean_text\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 15:25:55 WARN TaskSetManager: Stage 41 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|          clean_text|         text_as_int|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|       Metamorphosis|     [metamorphosis]|[4, 1, 9, 0, 4, 5...|\n",
      "|      by Franz Kafka|  [by, franz, kafka]|[1, 6, 2, 5, 0, 4...|\n",
      "|Translated by Dav...|[translated, by, ...|[9, 7, 0, 6, 8, 5...|\n",
      "|                   I|                 [i]|                 [0]|\n",
      "|One morning, when...|[one, morning, wh...|[13, 12, 4, 11, 1...|\n",
      "|“What’s happened ...|[whats, happened,...|[19, 7, 0, 16, 15...|\n",
      "|Gregor then turne...|[gregor, then, tu...|[6, 16, 4, 6, 13,...|\n",
      "|“Oh, God”, he tho...|[oh, god, he, tho...|[13, 7, 6, 13, 3,...|\n",
      "|He slid back into...|[he, slid, back, ...|[7, 4, 17, 11, 8,...|\n",
      "|And he looked ove...|[and, he, looked,...|[0, 12, 3, 7, 4, ...|\n",
      "|He was still hurr...|[he, was, still, ...|[7, 4, 21, 0, 17,...|\n",
      "|The first thing h...|[the, first, thin...|[17, 7, 4, 5, 8, ...|\n",
      "|It was a simple m...|[it, was, a, simp...|[8, 17, 20, 0, 16...|\n",
      "|The first thing h...|[the, first, thin...|[17, 7, 4, 5, 8, ...|\n",
      "|So then he tried ...|[so, then, he, tr...|[18, 14, 19, 7, 4...|\n",
      "|It took just as m...|[it, took, just, ...|[8, 19, 19, 14, 1...|\n",
      "|But then he said ...|[but, then, he, s...|[1, 20, 19, 19, 7...|\n",
      "|When Gregor was a...|[when, gregor, wa...|[20, 7, 4, 12, 6,...|\n",
      "|After a while he ...|[after, a, while,...|[0, 5, 18, 4, 16,...|\n",
      "|“Something’s fall...|[somethings, fall...|[18, 14, 12, 4, 1...|\n",
      "|“Gregor”, said hi...|[gregor, said, hi...|[6, 16, 4, 6, 14,...|\n",
      "|So why did his si...|[so, why, did, hi...|[17, 14, 21, 7, 2...|\n",
      "|The chief clerk n...|[the, chief, cler...|[19, 7, 4, 2, 7, ...|\n",
      "|“But Sir”, called...|[but, sir, called...|[1, 20, 19, 18, 8...|\n",
      "|And while Gregor ...|[and, while, greg...|[0, 13, 3, 22, 7,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_model_text.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 15:25:57 WARN TaskSetManager: Stage 42 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_model_text.write.mode(\"overwrite\").format(\"parquet\").save(\"s3a://project17-bucket-alex/stories-and-books-nlp/\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
