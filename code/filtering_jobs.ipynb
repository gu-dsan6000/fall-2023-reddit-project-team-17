{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing large datasets with Apache Spark and Amazon SageMaker\n",
    "\n",
    "***This notebook run on `Data Science 3.0 - Python 3` kernel on a `ml.t3.large` instance***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Processing Jobs are used  to analyze data and evaluate machine learning models on Amazon SageMaker. With Processing, you can use a simplified, managed experience on SageMaker to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. You can also use the Amazon SageMaker Processing APIs during the experimentation phase and after the code is deployed in production to evaluate performance.\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/Processing-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding diagram shows how Amazon SageMaker spins up a Processing job. Amazon SageMaker takes your script, copies your data from Amazon Simple Storage Service (Amazon S3), and then pulls a processing container. The processing container image can either be an Amazon SageMaker built-in image or a custom image that you provide. The underlying infrastructure for a Processing job is fully managed by Amazon SageMaker. Cluster resources are provisioned for the duration of your job, and cleaned up when a job completes. The output of the Processing job is stored in the Amazon S3 bucket you specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our workflow for processing large amounts of data with SageMaker\n",
    "\n",
    "We can divide our workflow into two steps:\n",
    "    \n",
    "1. Work with a small subset of the data with Spark running in local model in a SageMaker Studio Notebook.\n",
    "\n",
    "1. Once we are able to work with the small subset of data we can provide the same code (as a Python script rather than a series of interactive steps) to SageMaker Processing which launched a Spark cluster, runs out code and terminates the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook...\n",
    "\n",
    "We will analyze the [Pushshift Reddit dataset](https://arxiv.org/pdf/2001.08435.pdf) to be used for the project and then we will run a SageMaker Processing Job to filter out the comments and submissions from subreddits of interest. The filtered data will be stored in your account's s3 bucket and it is this filtered data that you will be using for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We need an available Java installation to run pyspark. The easiest way to do this is to install JDK and set the proper paths using conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.10.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.10.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2023.08.22 |       h06a4308_0         123 KB\n",
      "    certifi-2023.7.22          |  py310h06a4308_0         153 KB\n",
      "    openjdk-11.0.13            |       h87a67e3_0       341.0 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       341.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/linux-64::openjdk-11.0.13-h87a67e3_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> pkgs/main::ca-certificates-2023.08.22-h06a4308_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/linux-64::certifi-2023.7.22-py310h06a4308_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2023 | 123 KB    |                                       |   0% \n",
      "openjdk-11.0.13      | 341.0 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "ca-certificates-2023 | 123 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "certifi-2023.7.22    | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | 8                                     |   2% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #9                                    |   5% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###                                   |   8% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ####                                  |  11% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #####1                                |  14% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ######                                |  16% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #######                               |  19% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ########                              |  22% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #########                             |  24% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #########9                            |  27% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###########                           |  30% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ############1                         |  33% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #############2                        |  36% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############2                       |  39% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###############2                      |  41% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ################3                     |  44% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #################3                    |  47% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################3                   |  50% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################3                  |  52% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################3                 |  55% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #####################4                |  58% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ######################5               |  61% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #######################6              |  64% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ########################6             |  67% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #########################6            |  69% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##########################6           |  72% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###########################6          |  75% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ############################5         |  77% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #############################5        |  80% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##############################5       |  83% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###############################5      |  85% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ################################5     |  88% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | #################################4    |  91% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ##################################4   |  93% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ###################################4  |  96% \u001b[A\n",
      "openjdk-11.0.13      | 341.0 MB  | ####################################6 |  99% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspark==3.3.0\n",
      "  Using cached pyspark-3.3.0-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.9.5 (from pyspark==3.3.0)\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.3.0\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize S3 Data within local PySpark\n",
    "* By specifying the `hadoop-aws` jar in our Spark config we're able to access S3 datasets using the s3a file prefix. \n",
    "* Since we've already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as `ContainerCredentialsProvider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a06924fe-9bef-4b71-84ca-40168a34ad5e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 448ms :: artifacts dl 28ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a06924fe-9bef-4b71-84ca-40168a34ad5e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/23ms)\n",
      "23/11/07 20:59:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/07 20:59:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data into a Spark Dataframe\n",
    "Note that we will be using the \"s3a\" adapter (read more [here](https://aws.amazon.com/blogs/opensource/community-collaboration-the-s3a-story)). S3A enables Hadoop to directly read and write Amazon S3 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/18 19:39:49 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+---------+----------+------------+-----+--------+--------------------+------------+\n",
      "|             author|author_flair_css_class|   author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|  link_id| parent_id|retrieved_on|score|stickied|           subreddit|subreddit_id|\n",
      "+-------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+---------+----------+------------+-----+--------+--------------------+------------+\n",
      "|          y26404986|                  null|                null|Mmmmm ... is this...|               0| 1641098735|         null|  null|     0|hqwcail|t3_rtzn1c| t3_rtzn1c|  1645567209|    1|   false|          RedditSets|   t5_3psukr|\n",
      "|            iTwango|                  null|                null|\"Aren't you a lit...|               0| 1641098735|         null|  null|     0|hqwcaim|t3_rtwci2|t1_hqvbwwp|  1645567209|    2|   false|   NoStupidQuestions|    t5_2w844|\n",
      "|       SandiegoJack|                  null|                null|â€œFullâ€\\n\\nAlso if...|               0| 1641098735|         null|  null|     0|hqwcain|t3_rsu2gf|t1_hqp04wj|  1645567209|    1|   false|WarhammerCompetitive|    t5_3mrre|\n",
      "|        Happy-Kevin|                  null|                null|Could possibly be...|               0| 1641098735|         null|  null|     0|hqwcaio|t3_rop938|t1_hq3sq1e|  1645567209|    1|   false|          OculusLink|   t5_25gzcy|\n",
      "|         branden110|  oklahoma-sheet1-r...|:oklahoma: :wyomi...|YOURE GONNA DIE I...|               0| 1641098735|         null|  null|     0|hqwcaip|t3_rtx6ev|t1_hqwc6yz|  1645567209|    2|   false|                 CFB|    t5_2qm9d|\n",
      "|         imsexytoby|                  null|                null|My husband once n...|               0| 1641098735|         null|  null|     0|hqwcaiq|t3_rtfail|t1_hqst17e|  1645567209|    1|   false|       breastfeeding|    t5_2rdj4|\n",
      "|      tryantsupermo|                  null|        Kleptomaniac|Only in time Ochacko|               0| 1641098735|         null|  null|     0|hqwcair|t3_rtja2w| t3_rtja2w|  1645567209|    5|   false|  BokuNoMetaAcademia|    t5_9mbir|\n",
      "|        jstoltpojke|                  null|                null|Where do we get t...|               0| 1641098735|         null|  null|     0|hqwcais|t3_ru1hre| t3_ru1hre|  1645567209|    1|   false|           jerkbudss|   t5_3b1ay1|\n",
      "|          ManaMonoR|                  null|                null|ate some hot dogs...|               0| 1641098735|         null|  null|     0|hqwcait|t3_ru1km9|t1_hqwc6za|  1645567209|    2|   false|              delta8|   t5_2kpdau|\n",
      "|         cassity282|                  null|    Partassipant [4]|why would you ass...|               0| 1641098735|         null|  null|     0|hqwcaiu|t3_rtugjy|t1_hqw4xt7|  1645567209|   37|   false|       AmItheAsshole|    t5_2xhvq|\n",
      "|     BalanceMaestro|                  null|Moron, son of Mor...|Yes.  some people...|               0| 1641098735|         null|  null|     0|hqwcaiv|t3_rtq1hj| t3_rtq1hj|  1645567209|    2|   false|            exmormon|    t5_2r0gj|\n",
      "|      MrVictor01010|                  null|OU - Computer Eng...|       Did you pass?|               0| 1641098735|         null|  null|     0|hqwcaiw|t3_kwimro|t1_gj7ws35|  1645567209|    1|   false| EngineeringStudents|    t5_2sh0b|\n",
      "|            inminit|                  null|                null|I saw this.\\n\\n&a...|               0| 1641098735|         null|  null|     0|hqwcaix|t3_ru0q6u|t1_hqw9lpw|  1645567209|    5|   false|    AlgorandOfficial|    t5_wm2x2|\n",
      "|       xxxC0Y0T3xxx|                  null|                null|They were a bit s...|               0| 1641098735|         null|  null|     0|hqwcaiy|t3_rpzvm5|t1_hqwc3gs|  1645567209|    2|   false|     WorldCrossovers|   t5_2i51nq|\n",
      "| SharedZoneBeyMoist|                  null|                null|Dry Herb Vape, I'...|               0| 1641098735|         null|  null|     0|hqwcaiz|t3_rti5m6|t1_hqsvwbw|  1645567209|    0|   false|             uktrees|    t5_2si9d|\n",
      "|          [deleted]|                  null|                null|           [removed]|               0| 1641098735|         null|  null|     0|hqwcaj0|t3_rtx6ev| t3_rtx6ev|  1645567209|    7|   false|                 CFB|    t5_2qm9d|\n",
      "|     twinnedcalcite|                  null|                null|No a single place...|               0| 1641098735|         null|  null|     0|hqwcaj1|t3_rt95xd|t1_hquwpnt|  1645567209|    1|   false|          iceskating|    t5_2rguz|\n",
      "|             FE4R3D|                  null|                null|I was just prescr...|               0| 1641098735|         null|  null|     0|hqwcaj2|t3_rtptnx| t3_rtptnx|  1645567209|    1|   false|              Asthma|    t5_2s8vp|\n",
      "|Dazzling_Ostrich606|                  null|                null|I never realized ...|               0| 1641098735|         null|  null|     0|hqwcaj3|t3_rtx1lp|t1_hqw1zmm|  1645567209|    3|   false|       marvelstudios|    t5_2uii8|\n",
      "|  Every_Thought5834|                  null|                null|Been in your shoe...|               0| 1641098735|         null|  null|     0|hqwcaj4|t3_ru0mzq| t3_ru0mzq|  1645567209|    5|   false|            Marriage|    t5_2reak|\n",
      "+-------------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+------+------+-------+---------+----------+------------+-----+--------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 191 ms, sys: 25.9 ms, total: 217 ms\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "comments = spark.read.parquet(\n",
    "    \"s3a://bigdatateaching/reddit/parquet/comments/yyyy=*/mm=*/*comments*.parquet\",\n",
    "    header=True\n",
    ")\n",
    "comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/07 15:30:43 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+--------------------+-----------+-------------+--------------------+-------------+------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "|            author|author_flair_css_class|   author_flair_text|created_utc|distinguished|              domain|       edited|    id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|           subreddit|subreddit_id|               title|                 url|\n",
      "+------------------+----------------------+--------------------+-----------+-------------+--------------------+-------------+------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|           i.redd.it|         null|rkrds9|  false| false|           0|  false|     false|  1656064284|    3|           [deleted]|   false|antitheistcheesecake|   t5_56ml5q|           straw man|https://i.redd.it...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|self.antitheistch...|         null|rkrhyh|   true| false|           1|  false|     false|  1656064280|    1|           [removed]|   false|antitheistcheesecake|   t5_56ml5q|   [deleted by user]|https://www.reddi...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|           i.redd.it|         null|rkrjf5|  false| false|          12|  false|     false|  1656064278|   55|                    |   false|antitheistcheesecake|   t5_56ml5q|The sexual revolu...|https://i.redd.it...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|self.antitheistch...|         null|rkrk3b|   true| false|           1|  false|     false|  1656064278|    2|           [removed]|   false|antitheistcheesecake|   t5_56ml5q|   [deleted by user]|https://www.reddi...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|           i.redd.it|         null|rkrlu7|  false| false|           6|  false|     false|  1656064276|   41|                    |   false|antitheistcheesecake|   t5_56ml5q|in reality is the...|https://i.redd.it...|\n",
      "|         [deleted]|                  null|                null| 1640997058|         null|self.antitheistch...|         null|rkrmkl|   true| false|           1|  false|     false|  1656064276|    1|           [removed]|   false|antitheistcheesecake|   t5_56ml5q|   [deleted by user]|https://www.reddi...|\n",
      "|            qatlrn|                  null|                null| 1640996362|    moderator|  self.ProjectListen|1.640996737E9|rt0ohv|   true| false|          28|  false|     false|  1654200024|   14|As we bring in th...|   false|       ProjectListen|   t5_4ie55f|Introducing The P...|https://www.reddi...|\n",
      "|           NBA_MOD|                   NBA|  [NBA] Scott Foster| 1640995200|         null|            self.nba|1.641006311E9|rt6ogv|   true| false|          21|  false|     false|  1654199632|    9|##General Informa...|   false|                 nba|    t5_2qo4s|GAME THREAD: New ...|https://www.reddi...|\n",
      "|      FPPenSwapBot|                  null|                 Bot| 1640995200|         null|       self.Pen_Swap|         null|rt6ogw|   true| false|        1884|  false|     false|  1654199632|   22|Post your confirm...|   false|            Pen_Swap|    t5_2tyn2|January 2022 Conf...|https://www.reddi...|\n",
      "|    Donald_Filimon|                  null|        ðŸŽ¨ Artist ðŸŽ¨| 1640995200|         null|          opensea.io|         null|rt6ogx|  false| false|           2|  false|     false|  1654199632|    1|                    |   false|     NFTsMarketplace|   t5_4147ke|CryptoCoinBank â€“ ...|https://opensea.i...|\n",
      "|       YarnSwapBot|                  null|                 Bot| 1640995200|         null|       self.Yarnswap|         null|rt6ogy|   true| false|          16|  false|     false|  1654199632|    2|Post your confirm...|   false|            Yarnswap|    t5_2rx58|January 2022 Conf...|https://www.reddi...|\n",
      "|           NBA_MOD|                   NBA|  [NBA] Scott Foster| 1640995200|         null|            self.nba|1.641007038E9|rt6ogz|   true| false|          26|  false|     false|  1654199632|   11|##General Informa...|   false|                 nba|    t5_2qo4s|GAME THREAD: San ...|https://www.reddi...|\n",
      "|         [deleted]|                  null|                null| 1640995200|         null|    self.isthislegal|         null|rt6oh0|   true| false|           2|  false|     false|  1654199632|    3|           [removed]|   false|         isthislegal|    t5_2t5h4|   [deleted by user]|https://www.reddi...|\n",
      "|          OtterSou|                  null|                null| 1640995200|         null|        self.ISO8601|         null|rt6oh1|   true| false|          13|  false|     false|  1654199632|  404|Here's to another...|   false|             ISO8601|    t5_2tez4|Happy 2022-01-01T...|https://www.reddi...|\n",
      "|       nearbyareas|                  null|                null| 1640995200|         null|           self.COPD|         null|rt6oh2|   true| false|          13|  false|     false|  1654199632|    3|Also can PFT test...|   false|                COPD|    t5_2wk9n|Copd life expectancy|https://www.reddi...|\n",
      "|           Tokyono|                  null|                null| 1640995200|         null|           i.redd.it|         null|rt6oh5|  false| false|           0|  false|     false|  1654199632|   32|                    |   false|              museum|    t5_2rti0|Briton Riviere - ...|https://i.redd.it...|\n",
      "|Ok-Assignment-6458|                  null|                null| 1640995200|         null|         redgifs.com|         null|rt6oh6|  false| false|           0|   true|     false|  1654199632|    6|                    |   false|       HaveToHaveHer|    t5_3brmn|Blonde With A Nic...|https://www.redgi...|\n",
      "|        Ok-Fly1879|                  null|                null| 1640995200|         null|           i.redd.it|         null|rt6oh7|  false| false|           9|  false|     false|  1654199632|  125|                    |   false|       DannyGonzalez|    t5_mljgb|      Danny GONzalez|https://i.redd.it...|\n",
      "|    Shimmering-Sky|                      |:AMQ::STAR::MAL:h...| 1640995200|         null|          self.anime|         null|rt6oh8|   true| false|          44|  false|     false|  1654199632|   32|#***[Hajimari wa ...|   false|               anime|    t5_2qh22|REMINDER - Cross ...|https://www.reddi...|\n",
      "|          Abyss002|                  null|                null| 1640995200|         null|     self.allthemods|         null|rt6oha|   true| false|           0|  false|     false|  1654199632|    1|           [removed]|   false|          allthemods|    t5_32q4b|[Modded Server | ...|https://www.reddi...|\n",
      "+------------------+----------------------+--------------------+-----------+-------------+--------------------+-------------+------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 30.1 ms, sys: 10.1 ms, total: 40.2 ms\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submissions = spark.read.parquet(\n",
    "    \"s3a://bigdatateaching/reddit/parquet/submissions/yyyy=*/mm=*/*submissions*.parquet\",\n",
    "    header=True\n",
    ")\n",
    "submissions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:======================================================>(450 + 1) / 451]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 313,972,841x21\n",
      "CPU times: user 268 ms, sys: 47.8 ms, total: 315 ms\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 3,076,907,105x17\n",
      "CPU times: user 2.12 s, sys: 523 ms, total: 2.64 s\n",
      "Wall time: 21min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "Let us find the number of submissions and comments per subreddit. We will use SparkSQL for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions.createOrReplaceTempView(\"submissions\")\n",
    "comments.createOrReplaceTempView(\"comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|          subreddit|  count|\n",
      "+-------------------+-------+\n",
      "|          AskReddit|2820312|\n",
      "|       dirtykikpals|1797324|\n",
      "|           dirtyr4r|1645658|\n",
      "|        GaySnapchat|1294169|\n",
      "|          jerkbudss|1226116|\n",
      "|          teenagers| 992427|\n",
      "|      DirtySnapchat| 924343|\n",
      "|      FreeKarma4You| 740495|\n",
      "|relationship_advice| 714845|\n",
      "|      AutoNewspaper| 701442|\n",
      "|        MassiveCock| 694869|\n",
      "|            FemBoys| 653319|\n",
      "|               cock| 642364|\n",
      "|              memes| 631451|\n",
      "|     wifepictrading| 621277|\n",
      "|          Eldenring| 616378|\n",
      "|           gonewild| 607159|\n",
      "|         needysluts| 584244|\n",
      "|     PokemonGoRaids| 575693|\n",
      "|      Roleplaybuddy| 555313|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 453 ms, sys: 104 ms, total: 558 ms\n",
      "Wall time: 8min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sql_str=\"select subreddit, count(id) as count from submissions group by subreddit order by count desc\"\n",
    "submissions_subreddit_count = spark.sql(sql_str)\n",
    "submissions_subreddit_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process S3 data with SageMaker Processing Job `PySparkProcessor`\n",
    "\n",
    "We are going to move the above processing code in a Python file and then submit that file to SageMaker Processing Job's [`PySparkProcessor`](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#pysparkprocessor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./eda_code/process.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./eda_code/process.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s,%(levelname)s,%(module)s,%(filename)s,%(lineno)d,%(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_dataset_path_commments\", type=str, help=\"Path of dataset in S3 for reddit comments\")\n",
    "    parser.add_argument(\"--s3_dataset_path_submissions\", type=str, help=\"Path of dataset in S3 for reddit submissions\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str, help=\"s3 output prefix\")\n",
    "    parser.add_argument(\"--subreddits\", type=str, help=\"comma separate list of subreddits of interest\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    logger.info(f\"spark version = {spark.version}\")\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    sc = spark.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "\n",
    "   \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    logger.info(f\"going to read {args.s3_dataset_path_commments}\")\n",
    "    comments = spark.read.parquet(args.s3_dataset_path_commments, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    logger.info(f\"going to read {args.s3_dataset_path_submissions}\")\n",
    "    submissions = spark.read.parquet(args.s3_dataset_path_submissions, header=True)\n",
    "    logger.info(f\"finished reading files...\")\n",
    "    \n",
    "    # filter the dataframe to only keep the subreddits of interest\n",
    "    subreddits = [s.strip() for s in args.subreddits.split(\",\")]\n",
    "    submissions_filtered = submissions.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    comments_filtered = comments.where(lower(col(\"subreddit\")).isin(subreddits))\n",
    "    \n",
    "    # save the filtered dataframes so that these files can now be used for future analysis\n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/comments\"\n",
    "    logger.info(f\"going to write comments for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the comments_filtered dataframe is {comments_filtered.count():,}x{len(comments_filtered.columns)}\")\n",
    "    comments_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "    \n",
    "    s3_path = f\"s3://{args.s3_output_bucket}/{args.s3_output_prefix}/submissions\"\n",
    "    logger.info(f\"going to write submissions for {subreddits} in {s3_path}\")\n",
    "    logger.info(f\"shape of the submissions_filtered dataframe is {submissions_filtered.count():,}x{len(submissions_filtered.columns)}\")\n",
    "    submissions_filtered.write.mode(\"overwrite\").parquet(s3_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now submit this code to SageMaker Processing Job.\n",
    "\n",
    "# Processing Job for 12 Subreddits of Interest\n",
    "\n",
    "The code below calls a job to filter the overall Reddit comments and submissions datasets to only the subreddits that our analysis focuses upon. This data is saved to a bucket contained within Landon Carpenter's AWS account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-11-07-21-00-21-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................!CPU times: user 3.43 s, sys: 376 ms, total: 3.8 s\n",
      "Wall time: 10min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = \"project17-bucket-alex\"\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-parquet/comments/year=2021/month=1/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-parquet/submissions/year=2021/month=1/*.parquet\"\n",
    "output_prefix_data = \"project_jan2021\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "subreddits = \"amitheasshole,askmen,askwomen,trueoffmychest,unpopularopinion,tifu,socialskills,antiwork,relationship_advice,explainlikeimfive,outoftheloop,nostupidquestions\"\n",
    "#subreddits = \"technology\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./eda_code/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for r/AmItheAsshole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sm-spark-project-2023-11-07-17-03-52-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................!CPU times: user 1.01 s, sys: 81.6 ms, total: 1.09 s\n",
      "Wall time: 10min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\n",
    "role = sagemaker.get_execution_role()\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-project\",\n",
    "    framework_version=\"3.3\",\n",
    "    role=role,\n",
    "    instance_count=6,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# s3 paths\n",
    "session = sagemaker.Session()\n",
    "bucket = \"project17-bucket0\"\n",
    "s3_dataset_path_commments = \"s3://bigdatateaching/reddit-parquet/comments/year=2021/month=1/*.parquet\"\n",
    "s3_dataset_path_submissions = \"s3://bigdatateaching/reddit-parquet/submissions/year=2021/month=1/*.parquet\"\n",
    "output_prefix_data = \"project_amitheasshole\"\n",
    "output_prefix_logs = f\"spark_logs\"\n",
    "\n",
    "# modify this comma separated list to choose the subreddits of interest\n",
    "#subreddits = \"amitheasshole,askmen,askwomen,trueoffmychest,unpopularopinion,tifu,socialskills,antiwork,relationship_advice,explainlikeimfive,outoftheloop,nostupidquestions\"\n",
    "subreddits = \"amitheasshole\"\n",
    "    \n",
    "# run the job now, the arguments array is provided as command line to the Python script (Spark code in this case).\n",
    "spark_processor.run(\n",
    "    submit_app=\"./eda_code/process.py\",\n",
    "    arguments=[\n",
    "        \"--s3_dataset_path_commments\",\n",
    "        s3_dataset_path_commments,\n",
    "        \"--s3_dataset_path_submissions\",\n",
    "        s3_dataset_path_submissions,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_prefix\",\n",
    "        output_prefix_data,\n",
    "        \"--subreddits\",\n",
    "        subreddits,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, output_prefix_logs),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the filtered data\n",
    "\n",
    "Now that we have filtered the data to only keep submissions and comments from subreddits of interest. Let us read data from the s3 path where we saved the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://project17-bucket0/project_amitheasshole/comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the comments dataframe is 1,363,161x21\n",
      "CPU times: user 20.6 ms, sys: 3.75 ms, total: 24.3 ms\n",
      "Wall time: 39.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/comments\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "comments = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+----------+---------+-------+-------------------+\n",
      "|    subreddit|              author|                body| parent_id|  link_id|     id|        created_utc|\n",
      "+-------------+--------------------+--------------------+----------+---------+-------+-------------------+\n",
      "|AmItheAsshole|     Throwaway41790a|NTA. So shocked a...| t3_kzvl4s|t3_kzvl4s|gjqmigj|2021-01-18 17:43:39|\n",
      "|AmItheAsshole|       AutoModerator|^^^^AUTOMOD  ***T...| t3_kzzisl|t3_kzzisl|gjqmij1|2021-01-18 17:43:40|\n",
      "|AmItheAsshole|RadicalResponseRobot|Please keep us up...| t3_kznp6n|t3_kznp6n|gjqmil2|2021-01-18 17:43:41|\n",
      "|AmItheAsshole|          Rindingaro|NTA get the hell ...| t3_kzwryu|t3_kzwryu|gjqmj30|2021-01-18 17:43:47|\n",
      "|AmItheAsshole|           jetttward|This is really on...| t3_kzv1e5|t3_kzv1e5|gjqmj3z|2021-01-18 17:43:47|\n",
      "|AmItheAsshole|            Chi_Baby|I feel like my po...|t1_gjqmcu9|t3_kzzcnv|gjqmj6l|2021-01-18 17:43:48|\n",
      "|AmItheAsshole|          miladyelle|...and? Waking up...|t1_gjqltnk|t3_kzmt2t|gjqmjrk|2021-01-18 17:43:56|\n",
      "|AmItheAsshole|          ItIsMe2125|Yup, my uncle â€œgi...|t1_gjqlddt|t3_kzsv8f|gjqmjs1|2021-01-18 17:43:56|\n",
      "|AmItheAsshole|         ScottIespre|I think the curri...|t1_gjdro75|t3_kxx8wg|gjqmjus|2021-01-18 17:43:57|\n",
      "|AmItheAsshole|  Judgement_Bot_AITA|Welcome to /r/AmI...| t3_kzzhyu|t3_kzzhyu|gjqmjw2|2021-01-18 17:43:57|\n",
      "|AmItheAsshole|           PasDeTout|YTA. You are way ...| t3_kzokru|t3_kzokru|gjqmjw8|2021-01-18 17:43:57|\n",
      "|AmItheAsshole|       Doris_Useless|They lived there ...| t3_kzz52p|t3_kzz52p|gjqmk0t|2021-01-18 17:43:59|\n",
      "|AmItheAsshole|         luxxlifenow|Worth a try. I've...|t1_gjql54h|t3_kzxhr2|gjqmk1j|2021-01-18 17:43:59|\n",
      "|AmItheAsshole|       AutoModerator|^^^^AUTOMOD  ***T...| t3_kzzj1i|t3_kzzj1i|gjqmk2d|2021-01-18 17:44:00|\n",
      "|AmItheAsshole|             cmlobue|NAH.  You are bot...| t3_kzz5qo|t3_kzz5qo|gjqmk66|2021-01-18 17:44:01|\n",
      "|AmItheAsshole|           [deleted]|I am nearly 30 an...|t1_gjqltbd|t3_kzh6wb|gjqmk6s|2021-01-18 17:44:01|\n",
      "|AmItheAsshole|     Blahblahblah210|NTA, and I am ver...| t3_kzefby|t3_kzefby|gjqmkdr|2021-01-18 17:44:04|\n",
      "|AmItheAsshole|   dumpsterfiregroup|YTA, you either s...| t3_kzdk0g|t3_kzdk0g|gjqmkme|2021-01-18 17:44:07|\n",
      "|AmItheAsshole|        Happyfun0160|Actually itâ€™s not...|t1_gjqiy9j|t3_kzvl4s|gjqmkp1|2021-01-18 17:44:08|\n",
      "|AmItheAsshole|          xessywintr|Ok let's pretend ...|t1_gjq3uj6|t3_kz59kg|gjqmkpu|2021-01-18 17:44:09|\n",
      "+-------------+--------------------+--------------------+----------+---------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "comments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading submissions from s3a://project17-bucket0/project_amitheasshole/submissions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the submissions dataframe is 32,084x68\n",
      "CPU times: user 15.3 ms, sys: 0 ns, total: 15.3 ms\n",
      "Wall time: 20.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3_path = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\n",
    "print(f\"reading submissions from {s3_path}\")\n",
    "submissions = spark.read.parquet(s3_path, header=True)\n",
    "print(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adserver_click_url: string (nullable = true)\n",
      " |-- adserver_imp_pixel: string (nullable = true)\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- brand_safe: boolean (nullable = true)\n",
      " |-- contest_mode: boolean (nullable = true)\n",
      " |-- created_utc: timestamp (nullable = true)\n",
      " |-- crosspost_parent: string (nullable = true)\n",
      " |-- crosspost_parent_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- approved_at_utc: string (nullable = true)\n",
      " |    |    |-- approved_by: string (nullable = true)\n",
      " |    |    |-- archived: boolean (nullable = true)\n",
      " |    |    |-- author: string (nullable = true)\n",
      " |    |    |-- author_flair_css_class: string (nullable = true)\n",
      " |    |    |-- author_flair_text: string (nullable = true)\n",
      " |    |    |-- banned_at_utc: string (nullable = true)\n",
      " |    |    |-- banned_by: string (nullable = true)\n",
      " |    |    |-- brand_safe: boolean (nullable = true)\n",
      " |    |    |-- can_gild: boolean (nullable = true)\n",
      " |    |    |-- can_mod_post: boolean (nullable = true)\n",
      " |    |    |-- clicked: boolean (nullable = true)\n",
      " |    |    |-- contest_mode: boolean (nullable = true)\n",
      " |    |    |-- created: double (nullable = true)\n",
      " |    |    |-- created_utc: double (nullable = true)\n",
      " |    |    |-- distinguished: string (nullable = true)\n",
      " |    |    |-- domain: string (nullable = true)\n",
      " |    |    |-- downs: long (nullable = true)\n",
      " |    |    |-- edited: boolean (nullable = true)\n",
      " |    |    |-- gilded: long (nullable = true)\n",
      " |    |    |-- hidden: boolean (nullable = true)\n",
      " |    |    |-- hide_score: boolean (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- is_crosspostable: boolean (nullable = true)\n",
      " |    |    |-- is_reddit_media_domain: boolean (nullable = true)\n",
      " |    |    |-- is_self: boolean (nullable = true)\n",
      " |    |    |-- is_video: boolean (nullable = true)\n",
      " |    |    |-- likes: string (nullable = true)\n",
      " |    |    |-- link_flair_css_class: string (nullable = true)\n",
      " |    |    |-- link_flair_text: string (nullable = true)\n",
      " |    |    |-- locked: boolean (nullable = true)\n",
      " |    |    |-- media: string (nullable = true)\n",
      " |    |    |-- mod_reports: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- num_comments: long (nullable = true)\n",
      " |    |    |-- num_crossposts: long (nullable = true)\n",
      " |    |    |-- num_reports: string (nullable = true)\n",
      " |    |    |-- over_18: boolean (nullable = true)\n",
      " |    |    |-- parent_whitelist_status: string (nullable = true)\n",
      " |    |    |-- permalink: string (nullable = true)\n",
      " |    |    |-- pinned: boolean (nullable = true)\n",
      " |    |    |-- quarantine: boolean (nullable = true)\n",
      " |    |    |-- removal_reason: string (nullable = true)\n",
      " |    |    |-- report_reasons: string (nullable = true)\n",
      " |    |    |-- saved: boolean (nullable = true)\n",
      " |    |    |-- score: long (nullable = true)\n",
      " |    |    |-- secure_media: string (nullable = true)\n",
      " |    |    |-- selftext: string (nullable = true)\n",
      " |    |    |-- selftext_html: string (nullable = true)\n",
      " |    |    |-- spoiler: boolean (nullable = true)\n",
      " |    |    |-- stickied: boolean (nullable = true)\n",
      " |    |    |-- subreddit: string (nullable = true)\n",
      " |    |    |-- subreddit_id: string (nullable = true)\n",
      " |    |    |-- subreddit_name_prefixed: string (nullable = true)\n",
      " |    |    |-- subreddit_type: string (nullable = true)\n",
      " |    |    |-- suggested_sort: string (nullable = true)\n",
      " |    |    |-- thumbnail: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: string (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- ups: long (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_reports: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- view_count: string (nullable = true)\n",
      " |    |    |-- visited: boolean (nullable = true)\n",
      " |    |    |-- whitelist_status: string (nullable = true)\n",
      " |-- disable_comments: boolean (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- domain_override: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- embed_type: string (nullable = true)\n",
      " |-- embed_url: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- hidden: boolean (nullable = true)\n",
      " |-- hide_score: boolean (nullable = true)\n",
      " |-- href_url: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- imp_pixel: string (nullable = true)\n",
      " |-- is_crosspostable: boolean (nullable = true)\n",
      " |-- is_reddit_media_domain: boolean (nullable = true)\n",
      " |-- is_self: boolean (nullable = true)\n",
      " |-- is_video: boolean (nullable = true)\n",
      " |-- link_flair_css_class: string (nullable = true)\n",
      " |-- link_flair_text: string (nullable = true)\n",
      " |-- locked: boolean (nullable = true)\n",
      " |-- media: struct (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- oembed: struct (nullable = true)\n",
      " |    |    |-- author_name: string (nullable = true)\n",
      " |    |    |-- author_url: string (nullable = true)\n",
      " |    |    |-- cache_age: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html: string (nullable = true)\n",
      " |    |    |-- provider_name: string (nullable = true)\n",
      " |    |    |-- provider_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: long (nullable = true)\n",
      " |    |    |-- thumbnail_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- reddit_video: struct (nullable = true)\n",
      " |    |    |-- dash_url: string (nullable = true)\n",
      " |    |    |-- duration: long (nullable = true)\n",
      " |    |    |-- fallback_url: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- hls_url: string (nullable = true)\n",
      " |    |    |-- is_gif: boolean (nullable = true)\n",
      " |    |    |-- scrubber_media_url: string (nullable = true)\n",
      " |    |    |-- transcoding_status: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- media_embed: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- scrolling: boolean (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- mobile_ad_url: string (nullable = true)\n",
      " |-- num_comments: long (nullable = true)\n",
      " |-- num_crossposts: long (nullable = true)\n",
      " |-- original_link: string (nullable = true)\n",
      " |-- over_18: boolean (nullable = true)\n",
      " |-- parent_whitelist_status: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- pinned: boolean (nullable = true)\n",
      " |-- post_hint: string (nullable = true)\n",
      " |-- preview: struct (nullable = true)\n",
      " |    |-- enabled: boolean (nullable = true)\n",
      " |    |-- images: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |-- variants: struct (nullable = true)\n",
      " |    |    |    |    |-- gif: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- mp4: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- nsfw: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |-- obfuscated: struct (nullable = true)\n",
      " |    |    |    |    |    |-- resolutions: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |    |    |    |    |    |-- source: struct (nullable = true)\n",
      " |    |    |    |    |    |    |-- height: long (nullable = true)\n",
      " |    |    |    |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- width: long (nullable = true)\n",
      " |-- promoted: boolean (nullable = true)\n",
      " |-- promoted_by: string (nullable = true)\n",
      " |-- promoted_display_name: string (nullable = true)\n",
      " |-- promoted_url: string (nullable = true)\n",
      " |-- retrieved_on: timestamp (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- secure_media: struct (nullable = true)\n",
      " |    |-- event_id: string (nullable = true)\n",
      " |    |-- oembed: struct (nullable = true)\n",
      " |    |    |-- author_name: string (nullable = true)\n",
      " |    |    |-- author_url: string (nullable = true)\n",
      " |    |    |-- cache_age: long (nullable = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- height: long (nullable = true)\n",
      " |    |    |-- html: string (nullable = true)\n",
      " |    |    |-- provider_name: string (nullable = true)\n",
      " |    |    |-- provider_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_height: long (nullable = true)\n",
      " |    |    |-- thumbnail_url: string (nullable = true)\n",
      " |    |    |-- thumbnail_width: long (nullable = true)\n",
      " |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- version: string (nullable = true)\n",
      " |    |    |-- width: long (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- secure_media_embed: struct (nullable = true)\n",
      " |    |-- content: string (nullable = true)\n",
      " |    |-- height: long (nullable = true)\n",
      " |    |-- media_domain_url: string (nullable = true)\n",
      " |    |-- scrolling: boolean (nullable = true)\n",
      " |    |-- width: long (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- spoiler: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- suggested_sort: string (nullable = true)\n",
      " |-- third_party_trackers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- third_party_tracking: string (nullable = true)\n",
      " |-- third_party_tracking_2: string (nullable = true)\n",
      " |-- thumbnail: string (nullable = true)\n",
      " |-- thumbnail_height: long (nullable = true)\n",
      " |-- thumbnail_width: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- whitelist_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submissions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+-------------------+------------+---------------+\n",
      "|    subreddit|              author|               title|            selftext|        created_utc|num_comments|link_flair_text|\n",
      "+-------------+--------------------+--------------------+--------------------+-------------------+------------+---------------+\n",
      "|AmItheAsshole|           [deleted]|AITA for purposef...|           [removed]|2021-01-24 21:50:12|           1|           null|\n",
      "|AmItheAsshole|      ThatGirlMariaB|AITA for ghosting...|           [removed]|2021-01-24 21:52:22|           1|           null|\n",
      "|AmItheAsshole|           [deleted]|GF called me a ph...|           [removed]|2021-01-24 21:52:33|           1|           null|\n",
      "|AmItheAsshole|           [deleted]|Hey everyone, i'd...|           [removed]|2021-01-24 21:52:38|           1|           null|\n",
      "|AmItheAsshole|      ThatGirlMariaB|AITA for ending a...|           [removed]|2021-01-24 21:53:56|           1|           null|\n",
      "|AmItheAsshole|    vascoliveira2511|AITA for wanting ...|So firstly I'm 20...|2021-01-24 21:54:46|          28| Not the A-hole|\n",
      "|AmItheAsshole|   akvapesandtattoos|AITA for cutting ...|           [removed]|2021-01-24 21:54:50|           1|           null|\n",
      "|AmItheAsshole|      ThatGirlMariaB|AITA for cutting ...|           [removed]|2021-01-24 21:54:55|          24|           null|\n",
      "|AmItheAsshole|      SHAGGYDOPE2094|WIBTA if I asked ...|           [removed]|2021-01-24 22:00:24|          11|           null|\n",
      "|AmItheAsshole|           [deleted]|AITA if I donâ€™t w...|           [removed]|2021-01-24 22:00:25|          43|           null|\n",
      "|AmItheAsshole|      OrangeMonkey42|AITA for getting ...|My husband has be...|2021-01-24 22:00:53|          48| Not the A-hole|\n",
      "|AmItheAsshole|        h3xadecimal2|AIs from AI Dunge...|           [removed]|2021-01-24 22:01:15|           1|           null|\n",
      "|AmItheAsshole|           [deleted]|AITA for my mum w...|Iâ€™m 29 and have j...|2021-01-24 22:03:09|           7| Not the A-hole|\n",
      "|AmItheAsshole|Designer-Kangaroo611|AITA for getting ...|           [removed]|2021-01-24 22:03:15|           1|           null|\n",
      "|AmItheAsshole|     throw_away_8567|UPDATE: AITA for ...|           [removed]|2021-01-24 22:04:33|           1|         UPDATE|\n",
      "|AmItheAsshole|          Aita_pagan|AITA for asking m...|So I (15f) am a w...|2021-01-24 22:06:39|          69| Not the A-hole|\n",
      "|AmItheAsshole|          Ocean_faux|AITA For Trying T...|I have a class th...|2021-01-24 22:06:54|          19|No A-holes here|\n",
      "|AmItheAsshole|     Upbeat_Surprise|AITA for kicking ...|           [removed]|2021-01-24 22:07:21|           1|           null|\n",
      "|AmItheAsshole|         Dr_Bodyshot|AITA for greeting...|My ex broke up wi...|2021-01-24 22:09:12|          12| Not the A-hole|\n",
      "|AmItheAsshole|           [deleted]|AITA for taking s...|           [deleted]|2021-01-24 22:09:27|          16|           null|\n",
      "+-------------+--------------------+--------------------+--------------------+-------------------+------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a subset of columns\n",
    "submissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"created_utc\", \"num_comments\", \"link_flair_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          subreddit|count|\n",
      "+-------------------+-----+\n",
      "|     TrueOffMyChest|12086|\n",
      "|   unpopularopinion|42196|\n",
      "|           antiwork| 2388|\n",
      "|       socialskills| 3575|\n",
      "|             AskMen|14127|\n",
      "|      AmItheAsshole|32084|\n",
      "|relationship_advice|60643|\n",
      "|  explainlikeimfive|14905|\n",
      "|       OutOfTheLoop| 5865|\n",
      "|               tifu| 5654|\n",
      "|  NoStupidQuestions|48014|\n",
      "|           AskWomen| 9955|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "submissions.groupby('subreddit').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/07 17:14:02 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://project17-bucket0/test-victor.txt.\n",
      "java.nio.file.AccessDeniedException: s3a://project17-bucket0/test-victor.txt: getFileStatus on s3a://project17-bucket0/test-victor.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2XKZ4KW1X4JBD2ZK; S3 Extended Request ID: lBM+xt9iEyDy+vbstMgJ4StN0C3a3t+QxvGXFv511/X4rgnQUy+V0tuCKUQL9/I150RnDmxHG5v1hNWaga1X8a3vFVNuZJ8U), S3 Extended Request ID: lBM+xt9iEyDy+vbstMgJ4StN0C3a3t+QxvGXFv511/X4rgnQUy+V0tuCKUQL9/I150RnDmxHG5v1hNWaga1X8a3vFVNuZJ8U:403 Forbidden\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2275)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160)\n",
      "\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1777)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:3044)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:645)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2XKZ4KW1X4JBD2ZK; S3 Extended Request ID: lBM+xt9iEyDy+vbstMgJ4StN0C3a3t+QxvGXFv511/X4rgnQUy+V0tuCKUQL9/I150RnDmxHG5v1hNWaga1X8a3vFVNuZJ8U), S3 Extended Request ID: lBM+xt9iEyDy+vbstMgJ4StN0C3a3t+QxvGXFv511/X4rgnQUy+V0tuCKUQL9/I150RnDmxHG5v1hNWaga1X8a3vFVNuZJ8U\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n",
      "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n",
      "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n",
      "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1320)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1307)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1304)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2264)\n",
      "\t... 23 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o146.text.\n: java.nio.file.AccessDeniedException: s3a://project17-bucket0/test-victor.txt: getFileStatus on s3a://project17-bucket0/test-victor.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2XKPXEPSMPW9NHVW; S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ), S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2275)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:3033)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2XKPXEPSMPW9NHVW; S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ), S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1320)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1307)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1304)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2264)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://project17-bucket0/test-victor.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:421\u001b[0m, in \u001b[0;36mDataFrameReader.text\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    419\u001b[0m     paths \u001b[38;5;241m=\u001b[39m [paths]\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o146.text.\n: java.nio.file.AccessDeniedException: s3a://project17-bucket0/test-victor.txt: getFileStatus on s3a://project17-bucket0/test-victor.txt: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2XKPXEPSMPW9NHVW; S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ), S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2275)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:3033)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:784)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:782)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 2XKPXEPSMPW9NHVW; S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ), S3 Extended Request ID: qaMuk0kiAmFfpYnUgLVrz8XOCQVPhWrCgWsqeSSuC77pf4mOMMBN8ejbar0x7gp6na7IziRX+QkvQ/WBizC03pTV4dXP5/iZ\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1320)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1307)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1304)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2264)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.text(\"s3a://project17-bucket0/test-victor.txt\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
