{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "    theme: default\n",
    "    code-copy: true\n",
    "    code-line-numbers: true\n",
    "    highlight-style: github\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1: Frame your analysis and EDA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory 1\n",
    "\n",
    "#### Business Goals\n",
    "\n",
    "Determine if NSFW posts post affects user interaction.\n",
    "\n",
    "#### Technical Proposals\n",
    "\n",
    "Check the means of the distribution of comments for each type of post in a box plot. Perform hypothesis tests. Perform hypothesis tests for statistical significance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory 2\n",
    "\n",
    "#### Business Goals\n",
    "\n",
    "Determine what is the correlation that exists between the number of comments and the score of a post.\n",
    "\n",
    "#### Technical Proposals\n",
    "\n",
    "Calculate correlations between the score in various selected subreddits and the number of comments in each. Perform hypothesis tests for statistical significance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory 3\n",
    "\n",
    "#### Business Goals\n",
    "\n",
    "Determine the times of the day when posts typically receive the most engagement.\n",
    "\n",
    "#### Technical Proposals\n",
    "\n",
    "Plot comments over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-29 23:43:16 sagemaker-studio-692960231031-wo7kgoszj2g\n",
      "2023-08-29 23:50:01 sagemaker-us-east-1-692960231031\n",
      "2023-08-30 00:34:21 vad49\n",
      "2023-09-16 16:02:10 vad49-labdata\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE comments/\n",
      "                           PRE submissions/\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 ls s3://vad49/project_lowercase_test/\n",
    "!aws s3 ls s3://project17-bucket-alex/project_jan2021/\n",
    "\n",
    "#!aws s3 cp s3://project17-bucket-alex/eda_ideas.txt -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup - Run only once per Kernel App\n",
    "%conda install openjdk -y\n",
    "\n",
    "# install PySpark\n",
    "%pip install pyspark==3.2.0 s3fs pyarrow\n",
    "\n",
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, length, isnan, when, count\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 50) \n",
    "#pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in submissions and comments data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3_path_submissions = f\"s3a://project17-bucket-alex/project_jan2021//submissions\"\n",
    "print(f\"reading submissions from {s3_path_submissions}\")\n",
    "\n",
    "submissions = spark.read.parquet(s3_path_submissions, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3_path_comments = f\"s3a://project17-bucket-alex/project_jan2021//comments\"\n",
    "print(f\"reading submissions from {s3_path_comments}\")\n",
    "\n",
    "comments = spark.read.parquet(s3_path_comments, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_small = submissions.sample(withReplacement=False, fraction=0.01, seed=42)\n",
    "comments_small = comments.sample(withReplacement=False, fraction=0.01, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small dfs\n",
    "\n",
    "use_small = True  # to easily swap between the small and small dfs\n",
    "submissions_active = submissions_small if use_small else submissions\n",
    "comments_active = comments_small if use_small else comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache\n",
    "submissions_active.cache()\n",
    "comments_active.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1 Report on the basic info about your dataset. What are the interesting columns? What is the schema? How many rows do you have? etc. etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"submissions shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n",
    "print(f\"submissions shape: ({comments_active.count()}, {len(comments_active.columns)})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_active.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_active.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conduct basic data quality checks! Make sure there are no missing values, check the length of the comments, and remove rows of data that might be corrupted. Even if you think all your data is perfect, you still need to demonstrate that with your analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def check_and_remove_missing(df: DataFrame, threshold: int = 100) -> DataFrame:\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "    # Show the missing values count for each column\n",
    "    missing_values_collected = missing_values.collect()[0].asDict()\n",
    "    print(\"Missing values in each column:\")\n",
    "    for column, missing_count in missing_values_collected.items():\n",
    "        print(f\"{column}: {missing_count}\")\n",
    "\n",
    "    # Identify columns with missing values above threshold\n",
    "    columns_to_drop = [column for column, missing_count in missing_values_collected.items() if missing_count > threshold]\n",
    "\n",
    "    # Drop the identified columns from the dataframe\n",
    "    df = df.drop(*columns_to_drop)\n",
    "    \n",
    "    # Recalculate missing values for the updated DataFrame\n",
    "    missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    missing_values_collected = missing_values.collect()[0].asDict()\n",
    "    \n",
    "    # Print updated missing values count\n",
    "    print(\"Missing values after column removal:\")\n",
    "    for column, missing_count in missing_values_collected.items():\n",
    "        print(f\"{column}: {missing_count}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions_active = check_and_remove_missing(submissions_active)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn to int for displaying\n",
    "\n",
    "columns_to_cast_submissions = [\n",
    "    \"archived\", \"contest_mode\", \"hidden\", \"hide_score\", \n",
    "    \"is_crosspostable\", \"is_reddit_media_domain\", \"is_self\",\n",
    "    \"is_video\", \"locked\", \"over_18\", \"pinned\", \"spoiler\", \"stickied\"\n",
    "]\n",
    "\n",
    "for column_name in columns_to_cast_submissions:\n",
    "    submissions_active = submissions_active.withColumn(column_name, col(column_name).cast(\"integer\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove submissions without a body should obviously go, but what about the submissions without a self text (deleted, removed or empty). We can keep where the author is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_submissions(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # Define a list of conditions that would indicate a row needs to be removed\n",
    "    conditions = (col('selftext') != \"[removed]\") & (col('selftext') != \"[deleted]\") & (col('selftext').isNotNull() & (col('selftext') != \"\"))\n",
    "\n",
    "    # Apply the filter\n",
    "    cleaned_df = df.filter(conditions)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions_active = clean_submissions(submissions_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(submissions_active.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments_active = check_and_remove_missing(comments_active)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for the body of the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_comments(df: DataFrame) -> DataFrame:\n",
    "    \n",
    "    # Define the filter conditions\n",
    "    conditions = (col('body') != \"[removed]\") & (col('body') != \"[deleted]\") & (col('body').isNotNull() & (col('body') != \"\"))\n",
    "\n",
    "    # Apply the filter\n",
    "    cleaned_df = df.filter(conditions)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_active = clean_comments(comments_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn to ints for displaying\n",
    "columns_to_cast_comments = [\n",
    "    \"can_gild\", \"stickied\", \"is_submitter\"\n",
    "]\n",
    "\n",
    "for column_name in columns_to_cast_comments:\n",
    "    comments_active = comments_active.withColumn(column_name, col(column_name).cast(\"integer\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(comments_active.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Final shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"submissions shape: ({submissions_active.count()}, {len(submissions_active.columns)})\")\n",
    "print(f\"submissions shape: ({comments_active.count()}, {len(comments_active.columns)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_active.groupby('subreddit').count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Use data transformations to make AT LEAST 3 new variables that are relevant to your business questions. We cannot be more specific because this depends on your project and what you want to explore!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the weights for num_comments and score\n",
    "weight_comments = 0.5\n",
    "weight_score = 0.5\n",
    "\n",
    "# Add a new column with the weighted average of num_comments and score\n",
    "submissions_active = submissions_active.withColumn(\n",
    "    'comments_and_score',\n",
    "    (col('num_comments') * weight_comments) + (col('score') * weight_score)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submissions_active.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract the week of year and hour from `created_utc`\n",
    "comments_active = comments_active.withColumn('week_of_year', F.weekofyear('created_utc'))\n",
    "comments_active = comments_active.withColumn('hour_of_day', F.hour('created_utc'))\n",
    "\n",
    "\n",
    "comments_active.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Produce at least 5 interesting graphs about your dataset. Think about the dimensions that are interesting for your Reddit data! There are millions of choices. Make sure your graphs are connected to your business questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_active.groupby('over_18').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the number of true cases for over_18\n",
    "true_count = submissions_active.filter(col('over_18') == 1).count()\n",
    "\n",
    "# Sample the same number of false cases\n",
    "false_count = submissions_active.filter(col('over_18') == 0).count()\n",
    "fraction = true_count / false_count\n",
    "\n",
    "# Use sampleBy if you need stratified sampling to maintain a proportion\n",
    "sampled_false = submissions_active.filter(col('over_18') == 0).sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "\n",
    "# Since you need exactly `true_count` number of samples, we need to take action in the sampled DataFrame\n",
    "# This may cause Spark to scan the DataFrame twice\n",
    "sampled_false = sampled_false.limit(true_count)\n",
    "\n",
    "# Combine the true cases and sampled false cases\n",
    "combined = submissions_active.filter(col('over_18') == 1).unionAll(sampled_false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now convert this combined Spark DataFrame to Pandas DataFrame for visualization (assuming the data is small enough to fit into memory)\n",
    "pandas_df = combined.toPandas()\n",
    "\n",
    "# Ensure 'over_18' is a string to be treated as categorical data\n",
    "pandas_df['over_18'] = pandas_df['over_18'].map({0: 'False', 1: 'True'})\n",
    "\n",
    "\n",
    "# Calculate the mean and standard deviation of num_comments\n",
    "#mean_comments = pandas_df['num_comments'].mean()\n",
    "#std_comments = pandas_df['num_comments'].std()\n",
    "\n",
    "# Define the upper bound as 3 standard deviations above the mean\n",
    "#upper_bound = mean_comments + 3 * std_comments\n",
    "\n",
    "\n",
    "# remove rows with more than 200 comments\n",
    "pandas_df = pandas_df[pandas_df['num_comments'] <= 200]\n",
    "\n",
    "pandas_df.to_csv('../../data/eda-plots/box-plot-nsfw-data.csv', index=False)\n",
    "\n",
    "\n",
    "# Filter the DataFrame to exclude any num_comments above the upper bound\n",
    "#pandas_df = pandas_df[pandas_df['num_comments'] <= upper_bound]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a box plot for the score by over_18 status\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = pandas_df.boxplot(by='over_18', column=['num_comments'], grid=False)\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Box Plot of Scores by Over 18 Status')\n",
    "ax.set_xlabel('Over 18 Status')\n",
    "ax.set_ylabel('Comments')\n",
    "plt.suptitle('')  # Suppress the automatic Pandas-generated title\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = pandas_df['over_18'].value_counts()\n",
    "print(\"Counts for 'over_18' in the DataFrame:\")\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Group by 'subreddit' and calculate the mean of 'num_comments' and 'score'\n",
    "subreddit_means = submissions_active.groupBy('subreddit') \\\n",
    "                                    .agg(F.mean('num_comments').alias('mean_num_comments'), \n",
    "                                         F.mean('score').alias('mean_score'))\n",
    "\n",
    "# Now convert this Spark DataFrame to a Pandas DataFrame\n",
    "pandas_df_means = subreddit_means.toPandas()\n",
    "\n",
    "# Ensure the 'subreddit' is treated as a category for better plotting\n",
    "pandas_df_means['subreddit'] = pandas_df_means['subreddit'].astype('category')\n",
    "\n",
    "pandas_df_means.to_csv('../../data/eda-plots/mean-comments-vs-mean-score-data.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter_plot = sns.scatterplot(data=pandas_df_means, \n",
    "                               x='mean_num_comments', \n",
    "                               y='mean_score', \n",
    "                               hue='subreddit')\n",
    "\n",
    "# Enhance the plot\n",
    "scatter_plot.set_title('Scatter Plot of Mean Comments vs. Mean Score, Hued by Subreddit')\n",
    "scatter_plot.set_xlabel('Mean Number of Comments')\n",
    "scatter_plot.set_ylabel('Mean Score')\n",
    "plt.legend(title='Subreddit', loc='upper right')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, hour, avg\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, unix_timestamp, lit\n",
    "\n",
    "# Assuming 'created_utc' is in Unix timestamp format, convert it to timestamp type\n",
    "#comments_active = comments_active.withColumn('created_utc', to_timestamp('created_utc'))\n",
    "\n",
    "\n",
    "\n",
    "# Filter out dates on or before January 3rd, 2021\n",
    "comments_active = comments_active.filter(to_date(col('created_utc')) > lit('2021-01-03'))\n",
    "\n",
    "\n",
    "# Group by week of year and hour of day, then calculate the average number of comments\n",
    "comments_grouped = comments_active.groupBy('week_of_year', 'hour_of_day').count()\n",
    "\n",
    "# Pivot the data to create a matrix of week_of_year (rows) by hour_of_day (columns) with the counts of comments\n",
    "comments_pivot = comments_grouped.groupBy('week_of_year').pivot('hour_of_day').avg('count').orderBy('week_of_year')\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "comments_pivot_df = comments_pivot.toPandas().set_index('week_of_year')\n",
    "\n",
    "comments_pivot_df.to_csv('../../data/eda-plots/average-comments-hour-and-week-data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the heatmap using seaborn\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(comments_pivot_df, cmap='viridis')\n",
    "plt.title('Average Number of Comments per Hour and Week of the Year')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Week of the Year')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Produce at least 3 interesting summary tables about your dataset. You can decide how to split up your data into categories, time slices, etc. There are infinite ways you can make summary statistics. Be unique, creative, and interesting!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, corr\n",
    "\n",
    "# This will calculate the correlation between 'num_comments' and 'score' for each subreddit within the original data.\n",
    "correlation_by_subreddit = submissions_active.groupBy('subreddit') \\\n",
    "                                             .agg(corr(col('num_comments'), col('score')).alias('correlation_coefficient'))\n",
    "\n",
    "# Now collect and show the data\n",
    "correlation_by_subreddit.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df_correlation_table = correlation_by_subreddit.toPandas()\n",
    "\n",
    "# Export to CSV\n",
    "pandas_df_correlation_table.to_csv('../../data/eda-plots/correlation_by_subreddit.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Implement regex searches for specific keywords of interest to produce dummy variables and then make statistics that are related to your business questions. Note, that you DO NOT have to do textual cleaning of the data at this point. The next assignment on NLP will focus on the textual cleaning and analysis aspect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "# Define the keywords\n",
    "keywords = ['fascinating', 'entertaining', 'boring']\n",
    "\n",
    "\n",
    "# Initialize the DataFrame with the original data\n",
    "comments_active_dummies = comments_active\n",
    "\n",
    "# Create dummy variables for each keyword\n",
    "for keyword in keywords:\n",
    "    comments_active_dummies = comments_active_dummies.withColumn(\n",
    "        keyword, \n",
    "        (regexp_extract(col('body'), f'\\\\b{keyword}\\\\b', 0) != '').cast('integer')\n",
    "    )\n",
    "\n",
    "# This will hold all the pandas dataframes with the counts\n",
    "combined_counts = {}\n",
    "\n",
    "# Go through the keywords, convert the counts to pandas dataframes and collect them in a dictionary\n",
    "for keyword in keywords:\n",
    "    count_df = comments_active_dummies.groupBy(keyword).count().toPandas()\n",
    "    count_df.set_index(keyword, inplace=True)  # Set the keyword column as the index\n",
    "    combined_counts[keyword] = count_df\n",
    "\n",
    "# Now, assuming that the index contains the same values for all dataframes, concatenate them\n",
    "# The 'axis=1' parameter is used to concatenate columns, not rows\n",
    "combined_df = pd.concat(combined_counts.values(), axis=1, keys=combined_counts.keys())\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(combined_df)\n",
    "\n",
    "combined_df.to_csv('../../data/eda-plots/dummies_keywords_count.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Find some type of external data to join onto your Reddit data. Donâ€™t know what to pick? Consider a time-related dataset. Stock prices, game details over time, active users on a platform, sports scores, covid cases, etc., etc. While you may not need to join this external data with your entire dataset, you must have at least one analysis that connects to external data. You do not have to join the external data and analyze it yet, just find it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are planning to make any custom datasets that are derived from your Reddit data, make them now. These datasets might be graph-focused, or maybe they are time series focused, it is completely up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
