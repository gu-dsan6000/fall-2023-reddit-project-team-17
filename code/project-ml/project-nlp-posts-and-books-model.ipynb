{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1405a471-c3fd-47cd-9b38-60b728041350",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091a8809-54c7-4e24-baff-9fb4ca5599f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-29 23:43:16 sagemaker-studio-692960231031-wo7kgoszj2g\n",
      "2023-08-29 23:50:01 sagemaker-us-east-1-692960231031\n",
      "2023-08-30 00:34:21 vad49\n",
      "2023-09-16 16:02:10 vad49-labdata\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bd8edb-def2-4f8d-95c2-14d9cbbeee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-18 18:55:58          0 _SUCCESS\n",
      "2023-11-18 18:55:54    2866925 part-00000-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:55    2564497 part-00001-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:56      26078 part-00035-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:56      12157 part-00051-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:57      28228 part-00053-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n",
      "2023-11-18 18:55:57      30806 part-00059-e5360052-4545-4fe2-9b12-7c9ce44bf3ec-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://project17-bucket-alex/stories-and-books-nlp/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e901466e-a7b6-43c2-9b26-3d57f34fe058",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True is False: # set to true only for the first un\n",
    "    # Setup - Run only once per Kernel App\n",
    "    %conda install openjdk -y\n",
    "\n",
    "    # install PySpark\n",
    "    %pip install pyspark==3.2.0 s3fs pyarrow torch\n",
    "\n",
    "    # restart kernel\n",
    "    from IPython.core.display import HTML\n",
    "    HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e9d083-5324-40ea-8aee-3cbdbfe0ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, length, isnan, when, count, regexp_extract, weekofyear, hour, avg, to_date, unix_timestamp, lit, corr\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150) \n",
    "#pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db918676-d370-4796-8e76-9e1f6cfae9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-97fc5c93-cd0f-4192-9454-122ddf1f555f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 423ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-97fc5c93-cd0f-4192-9454-122ddf1f555f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/11ms)\n",
      "23/11/18 19:34:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/18 19:34:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/11/18 19:34:33 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark and build Spark session\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .config(\n",
    "        \"fs.s3a.aws.credentials.provider\",\n",
    "        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n",
    "    )\n",
    "    #.config(\"spark-jars-packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.3\")\\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b93534d-2ad1-4b41-8f9a-f696f7f0a794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/18 19:34:37 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|       custom_tokens|         text_as_int|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|       Metamorphosis|     [metamorphosis]|[29, 21, 36, 17, ...|\n",
      "|      by Franz Kafka|[by,  , franz,  ,...|[18, 41, 1, 22, 3...|\n",
      "|Translated by Dav...|[translated,  , b...|[36, 34, 17, 30, ...|\n",
      "|                   I|                 [i]|                [25]|\n",
      "|One morning, when...|[one,  , morning,...|[31, 30, 21, 1, 2...|\n",
      "|“What’s happened ...|[whats,  , happen...|[39, 24, 17, 36, ...|\n",
      "|Gregor then turne...|[gregor,  , then,...|[23, 34, 21, 23, ...|\n",
      "|“Oh, God”, he tho...|[oh, ,,  , god, ,...|[31, 24, 3, 1, 23...|\n",
      "|He slid back into...|[he,  , slid,  , ...|[24, 21, 1, 35, 2...|\n",
      "|And he looked ove...|[and,  , he,  , l...|[17, 30, 20, 1, 2...|\n",
      "|He was still hurr...|[he,  , was,  , s...|[24, 21, 1, 39, 1...|\n",
      "|The first thing h...|[the,  , first,  ...|[36, 24, 21, 1, 2...|\n",
      "|It was a simple m...|[it,  , was,  , a...|[25, 36, 1, 39, 1...|\n",
      "|The first thing h...|[the,  , first,  ...|[36, 24, 21, 1, 2...|\n",
      "|So then he tried ...|[so,  , then,  , ...|[35, 31, 1, 36, 2...|\n",
      "|It took just as m...|[it,  , took,  , ...|[25, 36, 1, 36, 3...|\n",
      "|But then he said ...|[but,  , then,  ,...|[18, 37, 36, 1, 3...|\n",
      "|When Gregor was a...|[when,  , gregor,...|[39, 24, 21, 30, ...|\n",
      "|After a while he ...|[after,  , a,  , ...|[17, 22, 36, 21, ...|\n",
      "|“Something’s fall...|[somethings,  , f...|[35, 31, 29, 21, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "\n",
    "all_model_text = spark.read.parquet(\"s3a://project17-bucket-alex/stories-and-books-nlp/\")\n",
    "\n",
    "#display(f\"shape: ({all_model_text.count()}, {len(all_model_text.columns)})\")\n",
    "all_model_text.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ebb430-c476-424e-a1e0-cd333bfcfb90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run on a sample\n",
    "\n",
    "all_model_text = all_model_text.sample(False, 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd04ae0-9320-478a-b37d-a26a4125fc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783adab2-99f1-4e25-b8b8-9c36371e2522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b909c7-0d33-4d5e-bd14-fefea6c7317d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "#from queue import PriorityQueue\n",
    "#from Levenshtein import distance \n",
    "\n",
    "\n",
    "# Collect all sequences into one long sequence\n",
    "flattened_seq = all_model_text.select(\"text_as_int\").rdd.flatMap(lambda x: x[0]).collect()\n",
    "\n",
    "# Convert the flattened sequence into a PyTorch tensor\n",
    "char_dataset = torch.tensor(flattened_seq, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f25daa-2746-4c54-a60a-c63c338126a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "\n",
    "# Collect all unique tokens\n",
    "all_tokens = all_model_text.select(explode(col(\"custom_tokens\"))).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Create the vocabulary from these tokens\n",
    "vocab = sorted(set(all_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51369a9c-d875-45e8-a1ee-c9d51a835410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# #### Efficiently load and batch the dataset for training using a `DataLoader`. Make sure to reserve some of the data for validation and testing. Describe how you handle batching and sequence lengths.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2a4188-c2a2-4971-aa76-50b95bc25132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# define sequences\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(char_dataset) // (seq_length + 1)\n",
    "\n",
    "\n",
    "# create the dataset\n",
    "#char_dataset = torch.tensor(text_as_int, dtype=torch.long)\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(char_dataset) - seq_length, seq_length):\n",
    "    sequences.append(char_dataset[i:i+seq_length])\n",
    "    targets.append(char_dataset[i+1:i+seq_length+1])\n",
    "\n",
    "dataset = TensorDataset(torch.stack(sequences), torch.stack(targets))\n",
    "\n",
    "\n",
    "\n",
    "# split it\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "# turn into dataloader\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6bb910-767e-4bdf-83d3-5d891852fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Batching and Sequence Length\n",
    "# \n",
    "# - 64 sequences per batch\n",
    "# - 100 characters per sequence\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Define your RNN model. Discuss the number of layers, hidden units, and the type of RNN cells you use. What is the total number of parameters in your model? Explain the rationale behind your architectural choices.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f448df4-6ec9-4417-8e36-0160a441699c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        # output\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f4984bf-a4f2-4705-8c46-82a285c7651b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(9239, 128)\n",
       "  (rnn): LSTM(128, 256, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=9239, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters: 4478615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "\n",
    "# instantiate\n",
    "model = RNN(vocab_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "\n",
    "display(model)\n",
    "\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f'total number of parameters: {total_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d54674-8597-4c1e-b96b-ec2c43f5ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Choices\n",
    "# \n",
    "# - RNN Cells: using LSTM cells over basic RNN cells since they are more resistant to the vanishing gradient problem and can capture long-range dependencies better.\n",
    "# - Layers: I set this to 2, as it worked well for purposes of this model - more layers could improve the model but also be more expensive and make it more likely to overfit.\n",
    "# - Hidden Units: I set to 256 hidden units for the same reasons as above.\n",
    "# - Embedding Dimension: 128 for the same reason as above.\n",
    "# \n",
    "# Note: total number of parameters printed above\n",
    "\n",
    "# %% [markdown]\n",
    "# #### Write the training loop.\n",
    "# #### Monitor and report on the training progress by tracking the loss. After training, evaluate the model's performance using a suitable evaluation metric (e.g., perplexity) on a validation dataset or a held-out portion of the training data. Discuss the results.\n",
    "# #### Specify the hyperparameters (for example, model hyperparameters, as well as sampling size and beam width from below) used in your model. Find suitable settings using the validation split.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8f25aed-e158-4f17-b2b5-3ea99ddcc1be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1, train loss: 3.3890, train perplexity: 29.6359, val loss: 2.9433, val perplexity: 18.9779\n",
      "test loss: 2.9484, test perplexity: 19.0747\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# training params\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "# training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    # progress bar\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # loop through data loader\n",
    "    for batch_idx, (data, target) in loop:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.permute(0, 2, 1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # acummulate loss\n",
    "        running_train_loss += loss.item()\n",
    "        \n",
    "        # update tqdm bar\n",
    "        loop.set_description(f\"epoch [{epoch+1}/{num_epochs}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    # get avg loss for the epoch\n",
    "    train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_perplexity = torch.exp(torch.tensor(train_loss))\n",
    "    \n",
    "    \n",
    "    # validate\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # loop through validation\n",
    "        for batch_idx, (data, target) in val_loop:\n",
    "            \n",
    "            output = model(data)\n",
    "            output = output.permute(0, 2, 1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            # tqdm\n",
    "            val_loop.set_description(f\"validation epoch [{epoch+1}/{num_epochs}]\")\n",
    "            val_loop.set_postfix(val_loss=loss.item())\n",
    "    \n",
    "    \n",
    "    # get avg loss for the epoch\n",
    "    val_loss = running_val_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_perplexity = torch.exp(torch.tensor(val_loss))\n",
    "    \n",
    "    # report\n",
    "    print(f\"epoch {epoch+1}/{num_epochs}, train loss: {train_loss:.4f}, train perplexity: {train_perplexity:.4f}, val loss: {val_loss:.4f}, val perplexity: {val_perplexity:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# eval\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        \n",
    "        output = model(data)\n",
    "        output = output.permute(0, 2, 1)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        \n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "# perplexity\n",
    "test_perplexity = torch.exp(torch.tensor(test_loss))\n",
    "\n",
    "\n",
    "# report\n",
    "print(f\"test loss: {test_loss:.4f}, test perplexity: {test_perplexity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4decb2fc-e8bb-42c4-bbee-4495ba1d2dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# #### Implement a text generation function using the trained RNN model. Provide a prompt or seed text, and use the RNN to generate a sequence of characters. Experiment with different prompt texts and observe how the generated text changes. Discuss any interesting patterns or observations you make during the text generation process.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4930065-e13a-4b10-bb41-a081b24905fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: hello world\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'char2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# divider\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, length, temperature)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# convert characters in to indices and reshape\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m input_eval \u001b[38;5;241m=\u001b[39m [char2idx[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m start_string]\n\u001b[1;32m     11\u001b[0m input_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_eval, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# convert characters in to indices and reshape\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m input_eval \u001b[38;5;241m=\u001b[39m [\u001b[43mchar2idx\u001b[49m[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m start_string]\n\u001b[1;32m     11\u001b[0m input_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(input_eval, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char2idx' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# with reference to https://github.com/karpathy/char-rnn\n",
    "\n",
    "def generate_text(model, start_string, length, temperature=1.0):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    # convert characters in to indices and reshape\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0)\n",
    "    generated_text = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # iterate for the specified length\n",
    "        for i in range(length):\n",
    "            \n",
    "            output = model(input_eval)\n",
    "            output = output[:, -1, :] / temperature  \n",
    "            probabilities = nn.functional.softmax(output, dim=-1)\n",
    "            \n",
    "            # sample\n",
    "            predicted_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            \n",
    "            \n",
    "            # add the text\n",
    "            input_eval = torch.cat([input_eval, predicted_id], dim=1)\n",
    "\n",
    "            generated_text.append(idx2char[predicted_id.item()])\n",
    "\n",
    "\n",
    "    # return text\n",
    "    return start_string + ''.join(generated_text)\n",
    "\n",
    "\n",
    "# generate text\n",
    "\n",
    "# prompts/seeds\n",
    "prompts = [\"hello world\", \"random sentence be like\", \"today i wanted to\", \"please tell me more about\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    print(generate_text(model, prompt, 100))\n",
    "    \n",
    "    # divider\n",
    "    print('-' * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8d810-2a57-4fdc-b239-ff4cbad69194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071d6ab-950a-461f-a860-c656db8ec755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c355f-595c-43f7-a8f5-597fe963b09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e73f8f8-4d6b-47bf-842e-d86aad5de220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40784a97-eb71-4e8e-aee0-d0898a209767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
